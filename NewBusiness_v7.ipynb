{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\softwares\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\softwares\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\softwares\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: h5py in c:\\softwares\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\softwares\\lib\\site-packages (from keras) (1.16.5)\n",
      "Requirement already satisfied: pyyaml in c:\\softwares\\lib\\site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\softwares\\lib\\site-packages (from keras) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\softwares\\lib\\site-packages (from keras) (1.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\softwares\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\softwares\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\softwares\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\softwares\\lib\\site-packages (from tensorflow) (1.29.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\softwares\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\softwares\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\softwares\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (1.16.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\softwares\\lib\\site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\softwares\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\softwares\\lib\\site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\softwares\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\softwares\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\softwares\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\softwares\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\softwares\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\softwares\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\softwares\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (41.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\softwares\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\softwares\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in c:\\softwares\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\softwares\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.23)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\softwares\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\softwares\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\softwares\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\softwares\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\softwares\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: more-itertools in c:\\softwares\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\softwares\\lib\\site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\softwares\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\softwares\\lib\\site-packages (from scikit-learn->sklearn) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\softwares\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1321 sha256=5148e122ec3476bd54872de58018510b71368a3c43dc4f46bfca555478ed9bab\n",
      "  Stored in directory: C:\\Users\\zeena\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Softwares\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Softwares\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3ee10f6d7cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 69\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Softwares\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Softwares\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Softwares\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "numpy.random.seed(29)    \n",
    "python_random.seed(29)\n",
    "tf.random.set_random_seed(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-dc96f7bcf66e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pandas.read_csv(\"MonthsHistory_1.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "X = dataset[:,0:11]\n",
    "y = dataset[:,11]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(11, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(11, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(11, input_dim=11, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "\n",
    "mc = ModelCheckpoint('best.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82 samples, validate on 41 samples\n",
      "Epoch 1/10000\n",
      "82/82 [==============================] - 0s 304us/step - loss: 931915.2134 - val_loss: 760051.8765\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 760051.87652, saving model to best.h5\n",
      "Epoch 2/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 604622.9520 - val_loss: 489716.7195\n",
      "\n",
      "Epoch 00002: val_loss improved from 760051.87652 to 489716.71951, saving model to best.h5\n",
      "Epoch 3/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 380006.8338 - val_loss: 307606.0423\n",
      "\n",
      "Epoch 00003: val_loss improved from 489716.71951 to 307606.04230, saving model to best.h5\n",
      "Epoch 4/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 216801.3377 - val_loss: 159766.0888\n",
      "\n",
      "Epoch 00004: val_loss improved from 307606.04230 to 159766.08880, saving model to best.h5\n",
      "Epoch 5/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 97481.4624 - val_loss: 65206.5609\n",
      "\n",
      "Epoch 00005: val_loss improved from 159766.08880 to 65206.56088, saving model to best.h5\n",
      "Epoch 6/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 34626.9769 - val_loss: 31770.9818\n",
      "\n",
      "Epoch 00006: val_loss improved from 65206.56088 to 31770.98175, saving model to best.h5\n",
      "Epoch 7/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 31917.8365 - val_loss: 51358.9230\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 31770.98175\n",
      "Epoch 8/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 62428.8426 - val_loss: 59156.2194\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 31770.98175\n",
      "Epoch 9/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 61814.9118 - val_loss: 41704.9056\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 31770.98175\n",
      "Epoch 10/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 37234.6103 - val_loss: 31804.4260\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 31770.98175\n",
      "Epoch 11/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 24885.9403 - val_loss: 37052.5764\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 31770.98175\n",
      "Epoch 12/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 25583.6554 - val_loss: 46266.2429\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 31770.98175\n",
      "Epoch 13/10000\n",
      "82/82 [==============================] - 0s 42us/step - loss: 30279.1995 - val_loss: 49686.0063\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 31770.98175\n",
      "Epoch 14/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 31868.6240 - val_loss: 47043.9265\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 31770.98175\n",
      "Epoch 15/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 29034.3366 - val_loss: 39914.3224\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 31770.98175\n",
      "Epoch 16/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 24498.2854 - val_loss: 34380.1150\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 31770.98175\n",
      "Epoch 17/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23302.0965 - val_loss: 31957.5666\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 31770.98175\n",
      "Epoch 18/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 24773.0355 - val_loss: 31770.4718\n",
      "\n",
      "Epoch 00018: val_loss improved from 31770.98175 to 31770.47185, saving model to best.h5\n",
      "Epoch 19/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 26125.2712 - val_loss: 31761.6326\n",
      "\n",
      "Epoch 00019: val_loss improved from 31770.47185 to 31761.63257, saving model to best.h5\n",
      "Epoch 20/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 25345.8392 - val_loss: 31948.3286\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 31761.63257\n",
      "Epoch 21/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23856.0466 - val_loss: 33397.6430\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 31761.63257\n",
      "Epoch 22/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23500.6182 - val_loss: 35345.4775\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 31761.63257\n",
      "Epoch 23/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23915.1990 - val_loss: 36159.1130\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 31761.63257\n",
      "Epoch 24/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 24086.6957 - val_loss: 35547.6097\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 31761.63257\n",
      "Epoch 25/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23736.5835 - val_loss: 34456.2603\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 31761.63257\n",
      "Epoch 26/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23692.5424 - val_loss: 33092.9588\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 31761.63257\n",
      "Epoch 27/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23569.0685 - val_loss: 32660.6234\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 31761.63257\n",
      "Epoch 28/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23649.3129 - val_loss: 32743.9561\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 31761.63257\n",
      "Epoch 29/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23571.2060 - val_loss: 33352.2866\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 31761.63257\n",
      "Epoch 30/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23538.9504 - val_loss: 33888.7920\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 31761.63257\n",
      "Epoch 31/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23513.6660 - val_loss: 33940.4555\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 31761.63257\n",
      "Epoch 32/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23518.3241 - val_loss: 33868.0159\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 31761.63257\n",
      "Epoch 33/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23504.0837 - val_loss: 33523.1159\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 31761.63257\n",
      "Epoch 34/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23502.8716 - val_loss: 33283.2679\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 31761.63257\n",
      "Epoch 35/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23480.3995 - val_loss: 32943.2812\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 31761.63257\n",
      "Epoch 36/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23551.7999 - val_loss: 32940.4585\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 31761.63257\n",
      "Epoch 37/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23663.4368 - val_loss: 32793.1539\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 31761.63257\n",
      "Epoch 38/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23543.4940 - val_loss: 33126.9189\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 31761.63257\n",
      "Epoch 39/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23491.9597 - val_loss: 33942.7211\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 31761.63257\n",
      "Epoch 40/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23763.0225 - val_loss: 34614.7918\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 31761.63257\n",
      "Epoch 41/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23633.5290 - val_loss: 33936.3011\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 31761.63257\n",
      "Epoch 42/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23637.0141 - val_loss: 33540.2261\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 31761.63257\n",
      "Epoch 43/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23522.0213 - val_loss: 33748.7246\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 31761.63257\n",
      "Epoch 44/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23481.2954 - val_loss: 33480.5933\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 31761.63257\n",
      "Epoch 45/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23512.2490 - val_loss: 33236.4599\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 31761.63257\n",
      "Epoch 46/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23493.9288 - val_loss: 33362.8498\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 31761.63257\n",
      "Epoch 47/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23464.2749 - val_loss: 33733.8563\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 31761.63257\n",
      "Epoch 48/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23494.8797 - val_loss: 33963.4920\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 31761.63257\n",
      "Epoch 49/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23517.9435 - val_loss: 33878.1405\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 31761.63257\n",
      "Epoch 50/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23493.2057 - val_loss: 33504.6749\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 31761.63257\n",
      "Epoch 51/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23549.3697 - val_loss: 33310.3521\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 31761.63257\n",
      "Epoch 52/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23474.3868 - val_loss: 32864.5911\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 31761.63257\n",
      "Epoch 53/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23601.4859 - val_loss: 32727.0914\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 31761.63257\n",
      "Epoch 54/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23608.7653 - val_loss: 32911.2752\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 31761.63257\n",
      "Epoch 55/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23482.3207 - val_loss: 33912.5629\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 31761.63257\n",
      "Epoch 56/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23585.8685 - val_loss: 34754.8952\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 31761.63257\n",
      "Epoch 57/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23653.9632 - val_loss: 34651.3014\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 31761.63257\n",
      "Epoch 58/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23618.5724 - val_loss: 34351.6669\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 31761.63257\n",
      "Epoch 59/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23490.2588 - val_loss: 33469.8918\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 31761.63257\n",
      "Epoch 60/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23465.1302 - val_loss: 32871.6220\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 31761.63257\n",
      "Epoch 61/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23713.1647 - val_loss: 32691.3867\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 31761.63257\n",
      "Epoch 62/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23823.6230 - val_loss: 33324.7480\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 31761.63257\n",
      "Epoch 63/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23477.6929 - val_loss: 33420.0599\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 31761.63257\n",
      "Epoch 64/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23508.0088 - val_loss: 33196.4173\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 31761.63257\n",
      "Epoch 65/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23704.7596 - val_loss: 33545.2970\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 31761.63257\n",
      "Epoch 66/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23469.1084 - val_loss: 33236.8625\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 31761.63257\n",
      "Epoch 67/10000\n",
      "82/82 [==============================] - 0s 20us/step - loss: 23492.9316 - val_loss: 33275.7568\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 31761.63257\n",
      "Epoch 68/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23485.6129 - val_loss: 33247.1857\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 31761.63257\n",
      "Epoch 69/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23468.7726 - val_loss: 33044.1964\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 31761.63257\n",
      "Epoch 70/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23474.4418 - val_loss: 33195.4997\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 31761.63257\n",
      "Epoch 71/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23457.4130 - val_loss: 33573.7432\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 31761.63257\n",
      "Epoch 72/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23487.3684 - val_loss: 33829.6341\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 31761.63257\n",
      "Epoch 73/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23505.4557 - val_loss: 33899.9215\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 31761.63257\n",
      "Epoch 74/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23459.1999 - val_loss: 33394.3486\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 31761.63257\n",
      "Epoch 75/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23470.5604 - val_loss: 33068.5355\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 31761.63257\n",
      "Epoch 76/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23533.9789 - val_loss: 33129.6521\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 31761.63257\n",
      "Epoch 77/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23678.0218 - val_loss: 33442.7691\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 31761.63257\n",
      "Epoch 78/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23504.1040 - val_loss: 33015.6807\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 31761.63257\n",
      "Epoch 79/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23561.8889 - val_loss: 33137.5847\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 31761.63257\n",
      "Epoch 80/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23472.9551 - val_loss: 34131.6990\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 31761.63257\n",
      "Epoch 81/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23542.1403 - val_loss: 34390.3189\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 31761.63257\n",
      "Epoch 82/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23561.9085 - val_loss: 33790.4447\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 31761.63257\n",
      "Epoch 83/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23518.2774 - val_loss: 33207.8167\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 31761.63257\n",
      "Epoch 84/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23537.2510 - val_loss: 33123.2168\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 31761.63257\n",
      "Epoch 85/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23531.5537 - val_loss: 33690.8743\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 31761.63257\n",
      "Epoch 86/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23464.5107 - val_loss: 33605.0919\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 31761.63257\n",
      "Epoch 87/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23446.7072 - val_loss: 33351.4410\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 31761.63257\n",
      "Epoch 88/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23654.1513 - val_loss: 32807.3232\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 31761.63257\n",
      "Epoch 89/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23489.0367 - val_loss: 33032.5163\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 31761.63257\n",
      "Epoch 90/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23425.6676 - val_loss: 33643.0462\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 31761.63257\n",
      "Epoch 91/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23540.5417 - val_loss: 34208.4710\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 31761.63257\n",
      "Epoch 92/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23501.9529 - val_loss: 33758.8355\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 31761.63257\n",
      "Epoch 93/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23536.5858 - val_loss: 33246.3202\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 31761.63257\n",
      "Epoch 94/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23444.4405 - val_loss: 33451.6311\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 31761.63257\n",
      "Epoch 95/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23457.2270 - val_loss: 33400.0681\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 31761.63257\n",
      "Epoch 96/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23774.1200 - val_loss: 33960.8617\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 31761.63257\n",
      "Epoch 97/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 12us/step - loss: 23462.1803 - val_loss: 32866.4307\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 31761.63257\n",
      "Epoch 98/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23858.7623 - val_loss: 32242.2321\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 31761.63257\n",
      "Epoch 99/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23853.8333 - val_loss: 32972.5786\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 31761.63257\n",
      "Epoch 100/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23436.5388 - val_loss: 33273.1719\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 31761.63257\n",
      "Epoch 101/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23406.5120 - val_loss: 33659.5643\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 31761.63257\n",
      "Epoch 102/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23553.3187 - val_loss: 33819.9543\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 31761.63257\n",
      "Epoch 103/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23412.7126 - val_loss: 34600.7846\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 31761.63257\n",
      "Epoch 104/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23601.5345 - val_loss: 34586.7172\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 31761.63257\n",
      "Epoch 105/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23560.8792 - val_loss: 33390.3700\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 31761.63257\n",
      "Epoch 106/10000\n",
      "82/82 [==============================] - 0s 30us/step - loss: 23502.6630 - val_loss: 32761.2264\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 31761.63257\n",
      "Epoch 107/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23523.5326 - val_loss: 32755.9047\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 31761.63257\n",
      "Epoch 108/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23490.5223 - val_loss: 33104.5811\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 31761.63257\n",
      "Epoch 109/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23409.0484 - val_loss: 33408.2011\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 31761.63257\n",
      "Epoch 110/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23733.3664 - val_loss: 34125.3095\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 31761.63257\n",
      "Epoch 111/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23521.7772 - val_loss: 33132.9447\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 31761.63257\n",
      "Epoch 112/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23412.2941 - val_loss: 33070.2466\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 31761.63257\n",
      "Epoch 113/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23467.5244 - val_loss: 33147.1426\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 31761.63257\n",
      "Epoch 114/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23402.0572 - val_loss: 32953.7424\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 31761.63257\n",
      "Epoch 115/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23495.1662 - val_loss: 33077.9345\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 31761.63257\n",
      "Epoch 116/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23605.3228 - val_loss: 32701.6513\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 31761.63257\n",
      "Epoch 117/10000\n",
      "82/82 [==============================] - 0s 48us/step - loss: 23463.6689 - val_loss: 33172.0317\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 31761.63257\n",
      "Epoch 118/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23377.5106 - val_loss: 33968.6718\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 31761.63257\n",
      "Epoch 119/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23520.2700 - val_loss: 35449.4262\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 31761.63257\n",
      "Epoch 120/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23840.4298 - val_loss: 34583.5660\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 31761.63257\n",
      "Epoch 121/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23511.9628 - val_loss: 33786.1386\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 31761.63257\n",
      "Epoch 122/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23520.1314 - val_loss: 33034.8684\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 31761.63257\n",
      "Epoch 123/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23448.1374 - val_loss: 33162.6182\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 31761.63257\n",
      "Epoch 124/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23423.8063 - val_loss: 33428.7798\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 31761.63257\n",
      "Epoch 125/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23501.1030 - val_loss: 33457.9698\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 31761.63257\n",
      "Epoch 126/10000\n",
      "82/82 [==============================] - 0s 18us/step - loss: 23610.7652 - val_loss: 32843.5091\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 31761.63257\n",
      "Epoch 127/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23517.0821 - val_loss: 33100.8289\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 31761.63257\n",
      "Epoch 128/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23405.9586 - val_loss: 33297.4965\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 31761.63257\n",
      "Epoch 129/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23504.8683 - val_loss: 34113.1929\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 31761.63257\n",
      "Epoch 130/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23428.9339 - val_loss: 33673.6943\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 31761.63257\n",
      "Epoch 131/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23437.6625 - val_loss: 33046.2747\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 31761.63257\n",
      "Epoch 132/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23478.9789 - val_loss: 33036.1535\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 31761.63257\n",
      "Epoch 133/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23391.7808 - val_loss: 33009.4113\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 31761.63257\n",
      "Epoch 134/10000\n",
      "82/82 [==============================] - 0s 32us/step - loss: 23436.2784 - val_loss: 33110.4927\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 31761.63257\n",
      "Epoch 135/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23439.0941 - val_loss: 32877.7964\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 31761.63257\n",
      "Epoch 136/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23368.4633 - val_loss: 33446.3509\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 31761.63257\n",
      "Epoch 137/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23612.9060 - val_loss: 34344.2611\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 31761.63257\n",
      "Epoch 138/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23426.2919 - val_loss: 33514.4986\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 31761.63257\n",
      "Epoch 139/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23520.2077 - val_loss: 32731.0321\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 31761.63257\n",
      "Epoch 140/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23415.5617 - val_loss: 33262.6200\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 31761.63257\n",
      "Epoch 141/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23478.3459 - val_loss: 34265.1966\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 31761.63257\n",
      "Epoch 142/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23545.4427 - val_loss: 33602.0952\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 31761.63257\n",
      "Epoch 143/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23566.8509 - val_loss: 33848.7508\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 31761.63257\n",
      "Epoch 144/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23364.5148 - val_loss: 33283.4532\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 31761.63257\n",
      "Epoch 145/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23362.3953 - val_loss: 32801.3501\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 31761.63257\n",
      "Epoch 146/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 37us/step - loss: 23418.1140 - val_loss: 33049.8053\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 31761.63257\n",
      "Epoch 147/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23488.4628 - val_loss: 33263.7308\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 31761.63257\n",
      "Epoch 148/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23362.9514 - val_loss: 32810.8600\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 31761.63257\n",
      "Epoch 149/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23418.6854 - val_loss: 32964.7873\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 31761.63257\n",
      "Epoch 150/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23387.6164 - val_loss: 33356.9486\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 31761.63257\n",
      "Epoch 151/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23450.2472 - val_loss: 33977.1433\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 31761.63257\n",
      "Epoch 152/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23545.4085 - val_loss: 33645.4961\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 31761.63257\n",
      "Epoch 153/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23209.5064 - val_loss: 32425.5068\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 31761.63257\n",
      "Epoch 154/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23555.9727 - val_loss: 31992.4921\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 31761.63257\n",
      "Epoch 155/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23770.1245 - val_loss: 32499.8456\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 31761.63257\n",
      "Epoch 156/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23520.3164 - val_loss: 33918.3583\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 31761.63257\n",
      "Epoch 157/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23594.4506 - val_loss: 34451.0595\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 31761.63257\n",
      "Epoch 158/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23490.4910 - val_loss: 33359.3838\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 31761.63257\n",
      "Epoch 159/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23331.1879 - val_loss: 33007.9249\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 31761.63257\n",
      "Epoch 160/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23501.6574 - val_loss: 32873.7552\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 31761.63257\n",
      "Epoch 161/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23509.7799 - val_loss: 33401.4196\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 31761.63257\n",
      "Epoch 162/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23480.0367 - val_loss: 34909.7519\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 31761.63257\n",
      "Epoch 163/10000\n",
      "82/82 [==============================] - 0s 499us/step - loss: 23690.2179 - val_loss: 34712.9146\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 31761.63257\n",
      "Epoch 164/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23625.9067 - val_loss: 32784.4688\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 31761.63257\n",
      "Epoch 165/10000\n",
      "82/82 [==============================] - 0s 12us/step - loss: 23543.2231 - val_loss: 32297.9595\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 31761.63257\n",
      "Epoch 166/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23593.1275 - val_loss: 33445.4687\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 31761.63257\n",
      "Epoch 167/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23403.1326 - val_loss: 34436.4767\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 31761.63257\n",
      "Epoch 168/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23526.1080 - val_loss: 34340.6582\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 31761.63257\n",
      "Epoch 169/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23407.1991 - val_loss: 32882.3951\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 31761.63257\n",
      "Epoch 170/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23968.2107 - val_loss: 32430.0144\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 31761.63257\n",
      "Epoch 171/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23505.5204 - val_loss: 34568.0659\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 31761.63257\n",
      "Epoch 172/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23516.7011 - val_loss: 34821.8232\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 31761.63257\n",
      "Epoch 173/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23504.7210 - val_loss: 33396.0408\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 31761.63257\n",
      "Epoch 174/10000\n",
      "82/82 [==============================] - 0s 37us/step - loss: 23241.6452 - val_loss: 32457.3289\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 31761.63257\n",
      "Epoch 175/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23461.1058 - val_loss: 32378.7828\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 31761.63257\n",
      "Epoch 176/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23749.1012 - val_loss: 32679.5198\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 31761.63257\n",
      "Epoch 177/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23780.0289 - val_loss: 35028.6600\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 31761.63257\n",
      "Epoch 178/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23587.1833 - val_loss: 34384.9802\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 31761.63257\n",
      "Epoch 179/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23354.6623 - val_loss: 33250.7516\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 31761.63257\n",
      "Epoch 180/10000\n",
      "82/82 [==============================] - 0s 26us/step - loss: 23371.0341 - val_loss: 32129.5252\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 31761.63257\n",
      "Epoch 181/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23853.5181 - val_loss: 32503.3534\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 31761.63257\n",
      "Epoch 182/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23497.7551 - val_loss: 32412.3395\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 31761.63257\n",
      "Epoch 183/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23358.5464 - val_loss: 33455.0441\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 31761.63257\n",
      "Epoch 184/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23528.8366 - val_loss: 34524.1688\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 31761.63257\n",
      "Epoch 185/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23498.7201 - val_loss: 33313.8823\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 31761.63257\n",
      "Epoch 186/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23476.2460 - val_loss: 32852.0672\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 31761.63257\n",
      "Epoch 187/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23379.8782 - val_loss: 33810.4258\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 31761.63257\n",
      "Epoch 188/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23330.4446 - val_loss: 33662.2824\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 31761.63257\n",
      "Epoch 189/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23481.2207 - val_loss: 33534.4627\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 31761.63257\n",
      "Epoch 190/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23574.8455 - val_loss: 32219.7352\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 31761.63257\n",
      "Epoch 191/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23537.9934 - val_loss: 32357.9733\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 31761.63257\n",
      "Epoch 192/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23513.7542 - val_loss: 33622.1116\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 31761.63257\n",
      "Epoch 193/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23304.4430 - val_loss: 33743.7329\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 31761.63257\n",
      "Epoch 194/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23696.7125 - val_loss: 33989.4325\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 31761.63257\n",
      "Epoch 195/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 24us/step - loss: 23269.1666 - val_loss: 32587.5876\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 31761.63257\n",
      "Epoch 196/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23437.6498 - val_loss: 32415.0518\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 31761.63257\n",
      "Epoch 197/10000\n",
      "82/82 [==============================] - 0s 36us/step - loss: 23388.9478 - val_loss: 33354.4527\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 31761.63257\n",
      "Epoch 198/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23371.4702 - val_loss: 34175.1192\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 31761.63257\n",
      "Epoch 199/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23381.1574 - val_loss: 33636.8968\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 31761.63257\n",
      "Epoch 200/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23279.8187 - val_loss: 32485.0854\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 31761.63257\n",
      "Epoch 201/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23444.3318 - val_loss: 32426.9082\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 31761.63257\n",
      "Epoch 202/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23393.5299 - val_loss: 32924.1823\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 31761.63257\n",
      "Epoch 203/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23666.0069 - val_loss: 34308.4248\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 31761.63257\n",
      "Epoch 204/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23637.4554 - val_loss: 33010.7298\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 31761.63257\n",
      "Epoch 205/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23289.2144 - val_loss: 33097.4447\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 31761.63257\n",
      "Epoch 206/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23282.9280 - val_loss: 33566.5644\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 31761.63257\n",
      "Epoch 207/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23290.8128 - val_loss: 33982.0404\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 31761.63257\n",
      "Epoch 208/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23352.2792 - val_loss: 33555.6672\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 31761.63257\n",
      "Epoch 209/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23334.9317 - val_loss: 32772.7084\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 31761.63257\n",
      "Epoch 210/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23542.8797 - val_loss: 33131.4660\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 31761.63257\n",
      "Epoch 211/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23805.7495 - val_loss: 32332.7092\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 31761.63257\n",
      "Epoch 212/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23282.7237 - val_loss: 33850.0599\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 31761.63257\n",
      "Epoch 213/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23391.1938 - val_loss: 34755.4912\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 31761.63257\n",
      "Epoch 214/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23660.5226 - val_loss: 33624.6292\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 31761.63257\n",
      "Epoch 215/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23280.7540 - val_loss: 33173.5446\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 31761.63257\n",
      "Epoch 216/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23452.3394 - val_loss: 32390.5324\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 31761.63257\n",
      "Epoch 217/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23612.6860 - val_loss: 32059.3457\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 31761.63257\n",
      "Epoch 218/10000\n",
      "82/82 [==============================] - 0s 24us/step - loss: 23524.0021 - val_loss: 33605.5148\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 31761.63257\n",
      "Epoch 219/10000\n",
      "82/82 [==============================] - 0s 25us/step - loss: 23537.8295 - val_loss: 34579.1596\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 31761.63257\n",
      "Epoch 00219: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10000, verbose=1, callbacks=[es, mc])\n",
    "#history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XuQ5GV97/H3t+8zs7Mze4VlF9yV7EEBdcEVMVgeDQq7eI5gqRRJjFSKkzUWEnKMHiGnDMfkWEWqctBQJSQoRCxRRNQDOVlkASEmQS4LElkuZkdAdljYnb3vzrUv3/PH8/RMT++vexrYmR52Pq+qqel+frenn/51f/p5fr/+tbk7IiIirUi1uwIiIvLGodAQEZGWKTRERKRlCg0REWmZQkNERFqm0BARkZYpNEREpGUKDRERaZlCQ0REWpZpdwWOtMWLF/vKlSvbXQ0RkTeUxx57bJe7L5lqvqMuNFauXMnmzZvbXQ0RkTcUM/tNK/NpeEpERFqm0BARkZYpNEREpGVH3TENEZFXq1gs0t/fz8jISLurMu0KhQIrVqwgm82+puUVGiIy5/X399Pd3c3KlSsxs3ZXZ9q4O7t376a/v59Vq1a9pnVoeEpE5ryRkREWLVp0VAcGgJmxaNGi19WjUmiIiMBRHxhVr/dxKjSiHz3ezy0Pt3SasojInKXQiP7x37dz6yPb2l0NEZmD9u3bx3XXXfeqlzvvvPPYt2/fNNSoMYVGlEmnKJYr7a6GiMxBjUKjXC43XW7jxo309vZOV7US6eypKJs2ShVvdzVEZA664oor+PWvf82aNWvIZrPMmzePZcuW8cQTT/D0009zwQUXsG3bNkZGRrj88svZsGEDMHHZpEOHDrF+/Xre+9738uCDD7J8+XLuuOMOOjo6jnhdFRpRJpWipJ6GyJz35X98iqe3Hzii6zz5uPlc9V9PaTj96quvZsuWLTzxxBM88MADfPjDH2bLli3jp8XedNNNLFy4kOHhYd71rnfxsY99jEWLFk1ax9atW/ne977HN77xDS688EJ++MMf8slPfvKIPg5QaIzLpI1iWT0NEWm/M844Y9L3KK699lp+/OMfA7Bt2za2bt16WGisWrWKNWvWAPDOd76TF154YVrqptCIsqkUpYp6GiJzXbMewUzp6uoav/3AAw9w77338vOf/5zOzk7e//73J37PIp/Pj99Op9MMDw9PS910IDzKpI2Sehoi0gbd3d0cPHgwcdr+/ftZsGABnZ2dPPvsszz00EMzXLvJ1NOIsjp7SkTaZNGiRZx11lmceuqpdHR0cMwxx4xPW7duHX/3d3/H29/+dk466STOPPPMNtZUoTEuk9LZUyLSPt/97ncTy/P5PHfddVfitOpxi8WLF7Nly5bx8s9//vNHvH5VGp6KMumUhqdERKag0IiyaaOoA+EiIk0pNKJMKoU7lDVEJSLSkEIjyqTDlR91MFxEpDGFRpSNoaGD4SIijSk0okwqNIUuJSIi0phCI8qOD0+ppyEiM+u1Xhod4Gtf+xpDQ0NHuEaNKTSibDr2NHQGlYjMsDdSaOjLfVGmGhrqaYjIDKu9NPqHPvQhli5dym233cbo6Cgf/ehH+fKXv8zg4CAXXngh/f39lMtlvvSlL7Fjxw62b9/OBz7wARYvXsz9998/7XVVaERZnT0lIgB3XQGvPHlk13ns22D91Q0n114afdOmTdx+++088sgjuDsf+chH+NnPfsbAwADHHXcc//RP/wSEa1L19PRwzTXXcP/997N48eIjW+cGNDwVjR8I19lTItJGmzZtYtOmTZx22mmcfvrpPPvss2zdupW3ve1t3HvvvXzxi1/kX/7lX+jp6WlL/dTTiPQ9DREBmvYIZoK7c+WVV/LpT3/6sGmPPfYYGzdu5Morr+Scc87hL/7iL2a8fuppROPf09AxDRGZYbWXRj/33HO56aabOHToEAAvvfQSO3fuZPv27XR2dvLJT36Sz3/+8zz++OOHLTsT1NOIqsNT6mmIyEyrvTT6+vXr+b3f+z3e8573ADBv3jy+853v0NfXxxe+8AVSqRTZbJbrr78egA0bNrB+/XqWLVumA+EzKaPvaYhIG9VfGv3yyy+fdP/EE0/k3HPPPWy5yy67jMsuu2xa61ZLw1ORvqchIjK1lkLDzP67mT1lZlvM7HtmVjCzVWb2sJltNbPvm1kuzpuP9/vi9JU167kylv/KzM6tKV8Xy/rM7Iqa8sRtTIdMSsc0RESmMmVomNly4E+Ate5+KpAGLgL+Gviqu68G9gKXxEUuAfa6+28BX43zYWYnx+VOAdYB15lZ2szSwNeB9cDJwO/GeWmyjSOu2tPQMQ2Rucl9bnxgfL2Ps9XhqQzQYWYZoBN4Gfgd4PY4/Wbggnj7/HifOP1sM7NYfqu7j7r780AfcEb863P359x9DLgVOD8u02gbR1xGV7kVmbMKhQK7d+8+6oPD3dm9ezeFQuE1r2PKA+Hu/pKZ/Q3wIjAMbAIeA/a5eynO1g8sj7eXA9visiUz2w8siuUP1ay6dpltdeXvjss02sYRp7OnROauFStW0N/fz8DAQLurMu0KhQIrVqx4zctPGRpmtoDQS1gF7AN+QBhKqleNaGswrVF5Um+n2fxJddwAbAA44YQTkmaZkr6nITJ3ZbNZVq1a1e5qvCG0Mjz1QeB5dx9w9yLwI+C3gd44XAWwAtgeb/cDxwPE6T3AntryumUale9qso1J3P0Gd1/r7muXLFnSwkM6XEZnT4mITKmV0HgRONPMOuNxhrOBp4H7gY/HeS4G7oi374z3idN/6mGg8E7gonh21SpgNfAI8CiwOp4plSMcLL8zLtNoG0dcNqXvaYiITGXK0HD3hwkHox8HnozL3AB8EficmfURjj/cGBe5EVgUyz8HXBHX8xRwGyFwfgJc6u7leMzis8DdwDPAbXFemmzjiJu4NLp6GiIijbT0jXB3vwq4qq74OcKZT/XzjgCfaLCerwBfSSjfCGxMKE/cxnTQ2VMiIlPTN8Kj7PjZUwoNEZFGFBrReE9Dw1MiIg0pNKLM8G6WspeihqdERBpSaET2fz/DDblr1NMQEWlCoVGVzpGzsg6Ei4g0odCoSmfJU9RlREREmlBoVKVzZK2ky4iIiDSh0KhK58hR1mVERESaUGhUZXJkKel7GiIiTSg0qtIhNHT2lIhIYwqNqnQ29DR09pSISEMKjap0jox6GiIiTSk0qqrDUyWFhohIIwqNqnQWgEql2OaKiIjMXgqNqnQu/C+NtbceIiKzmEKjqhoaZfU0REQaUWhUxeEpyqPtrYeIyCym0KiKPQ1TT0NEpCGFRtX48JSOaYiINKLQqIrDU6azp0REGlJoVI0PT6mnISLSiEKjKp0H1NMQEWlGoVEVh6dSFfU0REQaUWhUVYen1NMQEWlIoVEVQyOl0BARaUihUTU+PKXQEBFpRKFRNd7TKLW5IiIis5dCo6ra03AdCBcRaUShURV7GllKlPXrfSIiiRQaVeOhUaaoX+8TEUmk0KiKw1M5ipTU0xARSaTQqKoZntLvhIuIJFNoVNWERrGsnoaISBKFRlV1eMpKlCrqaYiIJFFoVJlRTmXj8JR6GiIiSVoKDTPrNbPbzexZM3vGzN5jZgvN7B4z2xr/L4jzmplda2Z9ZvZLMzu9Zj0Xx/m3mtnFNeXvNLMn4zLXmpnF8sRtTBdPZXX2lIhIE632NP4W+Im7vwV4B/AMcAVwn7uvBu6L9wHWA6vj3wbgeggBAFwFvBs4A7iqJgSuj/NWl1sXyxttY1pUqj0NnT0lIpJoytAws/nA+4AbAdx9zN33AecDN8fZbgYuiLfPB77twUNAr5ktA84F7nH3Pe6+F7gHWBenzXf3n7u7A9+uW1fSNqaFp3LkKDFWUk9DRCRJKz2NNwMDwD+Y2S/M7Jtm1gUc4+4vA8T/S+P8y4FtNcv3x7Jm5f0J5TTZxiRmtsHMNpvZ5oGBgRYeUjKPPY0xDU+JiCRqJTQywOnA9e5+GjBI82EiSyjz11DeMne/wd3XuvvaJUuWvJpFJ68nnSVr6mmIiDTSSmj0A/3u/nC8fzshRHbEoSXi/5018x9fs/wKYPsU5SsSymmyjemRzoWehkJDRCTRlKHh7q8A28zspFh0NvA0cCdQPQPqYuCOePtO4FPxLKozgf1xaOlu4BwzWxAPgJ8D3B2nHTSzM+NZU5+qW1fSNqZHOhzTGFVoiIgkyrQ432XALWaWA54D/pAQOLeZ2SXAi8An4rwbgfOAPmAozou77zGzvwIejfP9pbvvibc/A3wL6ADuin8AVzfYxvRIh2MaQwoNEZFELYWGuz8BrE2YdHbCvA5c2mA9NwE3JZRvBk5NKN+dtI3pYukcOQbZVy7P1CZFRN5Q9I3wGpbJkbUSo0X1NEREkig0alg8pqFTbkVEkik0alhGZ0+JiDSj0KiRyhbI6uwpEZGGFBo1UrGnodAQEUmm0Khh6Rw5K2t4SkSkAYVGrXQ2frlPp9yKiCRRaNRK53TtKRGRJhQateI3whUaIiLJFBq10joQLiLSjEKjVvXLfUUd0xARSaLQqJXOAlAuj7W5IiIis5NCo1Y6B0C5ONrmioiIzE4KjVoxNLyknoaISBKFRq04PFUpqachIpJEoVEr9jQq6mmIiCRSaNTK5AHwUrHNFRERmZ0UGrXi8JSOaYiIJFNo1IrDU5R1TENEJIlCo1YcnkqVR9pcERGR2UmhUSvTAUBaoSEikkihUStbACBdGcXd21wZEZHZR6FRK/Y08j5GqaLQEBGpp9CoFXsaBcZ0eXQRkQQKjVqxp1Gwoi6PLiKSQKFRSz0NEZGmFBq1qj0NhYaISCKFRq1MHsfI2xijJf0Qk4hIPYVGLTMq6TwFxnRMQ0QkgUKjTiVdCMNTZYWGiEg9hUadSqZAgSKjRYWGiEg9hUYdzxQomHoaIiJJFBp1PNOhs6dERBpQaNTLFOKBcJ09JSJSr+XQMLO0mf3CzP5fvL/KzB42s61m9n0zy8XyfLzfF6evrFnHlbH8V2Z2bk35uljWZ2ZX1JQnbmNaZTvC8JR6GiIih3k1PY3LgWdq7v818FV3Xw3sBS6J5ZcAe939t4Cvxvkws5OBi4BTgHXAdTGI0sDXgfXAycDvxnmbbWPaWLZAXsNTIiKJWgoNM1sBfBj4ZrxvwO8At8dZbgYuiLfPj/eJ08+O858P3Oruo+7+PNAHnBH/+tz9OXcfA24Fzp9iG9PGsh3h7CmFhojIYVrtaXwN+B9A9Z10EbDP3Uvxfj+wPN5eDmwDiNP3x/nHy+uWaVTebBvTJpXTgXARkUamDA0z+y/ATnd/rLY4YVafYtqRKk+q4wYz22xmmwcGBpJmaVkq20GHjeqUWxGRBK30NM4CPmJmLxCGjn6H0PPoNbNMnGcFsD3e7geOB4jTe4A9teV1yzQq39VkG5O4+w3uvtbd1y5ZsqSFh9RYtaeh4SkRkcNNGRrufqW7r3D3lYQD2T91998H7gc+Hme7GLgj3r4z3idO/6mH3069E7gonl21ClgNPAI8CqyOZ0rl4jbujMs02sa0mTimoVNuRUTqvZ7vaXwR+JyZ9RGOP9wYy28EFsXyzwFXALj7U8BtwNPAT4BL3b0cj1l8FribcHbWbXHeZtuYPpkO8lZkrFiael4RkTkmM/UsE9z9AeCBePs5wplP9fOMAJ9osPxXgK8klG8ENiaUJ25jWsUfYiqPjczoZkVE3gj0jfB68YeYKqNDba6IiMjso9CoV+1pFIfbXBERkdlHoVGv2tMYU09DRKSeQqNe7GlUxtTTEBGpp9CoF3saaHhKROQwCo16safhJZ09JSJST6FRr9rTKKmnISJST6FRL/Y0rDja5oqIiMw+Co16saeRKmt4SkSknkKjXuxppMojhMtfiYhIlUKjXuxp5HyMYlmhISJSS6FRL/Y0CowxXNSVbkVEaik06sWeRoExhscUGiIitRQa9VIpyqksBVNPQ0SknkIjQSVdUE9DRCSBQiNBJdOhYxoiIgkUGgk800mnjTKi0BARmUShkcCznXQyquEpEZE6Co0kuU46GdHwlIhIHYVGAsuF4SmFhojIZAqNBKlcFx3omIaISD2FRoJUvisMT+mYhojIJAqNBOlCNx0anhIROYxCI4HlusLZUwoNEZFJFBpJsp102Bgjo6V210REZFZRaCTJdZLCKY0NtbsmIiKzikIjSbYLgMqoQkNEpJZCI0muEwAfPdTmioiIzC4KjSTZEBoUB9tbDxGRWUahkSQ3DwAfU2iIiNRSaCSJw1NWHG5zRUREZheFRpI4PJUq6UC4iEgthUaSXDh7KqWehojIJAqNJLGnkSmrpyEiUkuhkST2NNJl9TRERGpNGRpmdryZ3W9mz5jZU2Z2eSxfaGb3mNnW+H9BLDczu9bM+szsl2Z2es26Lo7zbzWzi2vK32lmT8ZlrjUza7aNaRd7GvnKCMVyZUY2KSLyRtBKT6ME/Jm7vxU4E7jUzE4GrgDuc/fVwH3xPsB6YHX82wBcDyEAgKuAdwNnAFfVhMD1cd7qcutieaNtTK9MngopOmyUQV1/SkRk3JSh4e4vu/vj8fZB4BlgOXA+cHOc7Wbggnj7fODbHjwE9JrZMuBc4B533+Pue4F7gHVx2nx3/7m7O/DtunUlbWN6mVHOdNLFCAdHFBoiIlWv6piGma0ETgMeBo5x95chBAuwNM62HNhWs1h/LGtW3p9QTpNt1Ndrg5ltNrPNAwMDr+YhNVTOdNDBKIfU0xARGddyaJjZPOCHwJ+6+4FmsyaU+Wsob5m73+Dua9197ZIlS17Noo3XmQ2/E67QEBGZ0FJomFmWEBi3uPuPYvGOOLRE/L8zlvcDx9csvgLYPkX5ioTyZtuYdp7tpJNRDml4SkRkXCtnTxlwI/CMu19TM+lOoHoG1MXAHTXln4pnUZ0J7I9DS3cD55jZgngA/Bzg7jjtoJmdGbf1qbp1JW1j2lmuiw5G1NMQEamRaWGes4A/AJ40sydi2Z8DVwO3mdklwIvAJ+K0jcB5QB8wBPwhgLvvMbO/Ah6N8/2lu++Jtz8DfAvoAO6KfzTZxrRL5brotH1sU2iIiIybMjTc/V9JPu4AcHbC/A5c2mBdNwE3JZRvBk5NKN+dtI2ZkMp3hQPhGp4SERmnb4Q3kCnMo5NRDqqnISIyTqHRgOW66NKX+0REJlFoNJLrCqfcanhKRGScQqOR3Dw6GWFwZLTdNRERmTUUGo109AJQHt7f5oqIiMweCo1GCiE0bGRfmysiIjJ7KDQaiT2NtEJDRGScQqORjnDV9sxYs8tsiYjMLQqNRuLwVKaoYxoiIlUKjUZiT6NQOkD4kruIiCg0GonHNLr9ECNF/eSriAgoNBrL5CmlCvTYIAdHi+2ujYjIrKDQaKKY66GXQX0rXEQkUmg0Uc730GOD+k0NEZFIodFEJd9DDwoNEZEqhUYzHQvotUManhIRiRQaTVhHL/NtkIMKDRERQKHRVL57Ib0cYvegrnQrIgKt/Ub4nJWdt4icjbJz32C7qyIiMiuop9GExW+FH9w30OaaiIjMDgqNZuL1p0YO7GpzRUREZgeFRjOxpzF2cE+bKyIiMjsoNJqJ158qDe3VRQtFRFBoNBd7Gt3l/ewfDtef+vdt+9j01Cts2zPUzpqJiLSFzp5qpmcFFcvyW6nt7DgwSlcuzaZv/E9+VnwLL+T+E5u/9EHymXS7aykiMmPU02gmk2d4wWpOsRfYcWCEvXf9b76Q+g7X9NzKwdEST7yon4IVkblFoTEFP+btnJJ6gdILD7L0sWvY4b2sHtnCStvBv/1698SMQ3vgX78G/34rFEfaV2ERkWmk0JhC/vg1LLYDvPnZv2c01cEfpb6MY2xYsJkH++KpuL/5OfztGrj3Kvjxp+Hr74L9L7W34iIi00ChMYXs8jUArNzzbzyYfhcLjj8ZW/U+ziv/lKe27WJw/x740R9B50L443+D3/8hDO+D73ws9D5ERI4iOhA+lWNPpYKRwvnB0Om84/heWHkZvbd8nD9J3cbg9/+BrgPb4ZJNcOyp4e+iW+A7H4dvfhAu+i4sfcvh662UYeBZ6H8Utv8CMh2wYCUsXAXL3gHdx07M6w7De0MIefzpWUtBthCWy3ZApgCpms8AlQoMDsCBfjg0AIX50LUECj1gaTCLf6lwvzwGY4dgbAhSmbju+JftgFTNAf/iCOx9AfY+D6XROE8B5h0LPcsh3z1R71J1qC5ub/x/nI5P/G9UZilIZSGdDdvzSqgThMe470UoDoXtdi0NjzOdhbFBGD0Qls3kINsF6UxY9+jB2Ka7Qht2LgxtUx6D4nD4K4+FdXUunPxclEbC8qMHQ5uNHgzb6loCS94Cuc7Jz8PwXhjaDV6GBatCWzVSfa53bYWXHguPo/cE6H0T9B4Pua6wzvIYVIrxueqYWLY8FupXGg3Pa8eCsF+4h3arlKFSCnUpjcK+34R19J4Q5i0X4cBLYZ/pXBi27xXoWBjat/rcVcpx34n3y8WJfTTbAd3LQptXlUuwuw8Gd4Z1LVwF2c6wvDsc2hFeC2NDMG8JnPDbE+00tAd2/Ud4jjsWwNJTwrqH9sC2h0Pbdx8Lx5w6fpo8EMr77g3tsHBVaPva5wagNBZeI/teDK+/nhNC+cGXQzuls+HxxDMpgfChcGR/aGuvwMI3h/mq66vuj/OWwrxjJu/ve54Lz8/ik0JZcTDUc+RAeLyFXsjPn3gtu4fnaFdfWM+Sk8I+Xtu2pdGwv+zeCiedB5l84/3rCFBoTCXfzUB2Bd1jO/lp+R18/PgeWP0hyqd+gs9s+QGV7YavuxpbsZZ/3bqLe5/ZwfBYL/9t/S2svm8DXPduOO70sDOnYnOPHoRXngxvOBB2lHIx7EBVuXnhz8thJ6208JOz6Xx4IVdK4Y8j+N2Sajh5Oezgzdad6w4vgJEDUJ6miz1aOtTBX+Xvt+e6Qzu/muUKPdB9XAigod01QdhAfj5g4bGXRjmiz4Olw3NQK9sVHk9p5PBtWfXNp4XHm5sX3uwazZvOhQ8IxeGawOoKy9Tvn6ksLDoxtN3YYHizLCacpp7piPtr8fDy+ctg9FAImkn1yIeAPvRK3M9rdB8X3lDLpfhcDU+e3vumEH7D+0LIFeuuK2ep8LjKY5PL5x0bwmxobwiZ+rp2HxvqcuClye1X6Al1rZRgcNfEa95STZ4TCx/yCj3hA1r94wfI90DXovCcDfxq4nX2mQfhmFMarPfIUGi0YP77P8uTz73E2amVvHvVIgDS6/+abQN7uWrbGt4zdg59t/+S72/eRkc2TTplfH9ziYvf9k2uPHYz+Rf/meLgfoZGRimVK6TzneTeeiFdbz4TVqyFhW/G3SkfGiCz97nQ89jfD2MHw87VsSB8uuhcNPGJ3yvhxVsaCS/G4kh4gbiHnT6VCZ905i8P/0cPhJ12ZH9YtvrJs/qXyYdPsdmO8CmyOBze8ErD4dNfcXCiF9K5EBaeGD8tdoQ6jA2FT4v7++HgK6Es313zCa2m9+CAVVu3vgcSJ9SWVcrhTaVcip+wLASvpcILcsHKUPfRg6EOg7vC/NnO8MKrlMJjGTsU3ixyXSHEC72hTUsjMLwntE06Hx5TtiO8SR7aAXueD588Cz1h/o4F4bHlu8OLNh8D/sBL4RPxoYFQ73QufkpdGJYzC+s67AOATb5bmB8e0/J3hvv7Xgx/e18Iz0s6F3pM6Vx4cxvcHfaLao8vUwjPZ7kUemIQpls6/K/eTuegZ0V4/ve9GJ67fHfo0XQtDW1S7VEM7Q69stLoRM+2NBr2vWxHCI9CT9g3SiPxk29f2O86F8HK94YPT93HTvQOq/tuKhPelJefHubd8zw8d39o80wh9N6WvCXU7dAr8NLjYR3dx8Lqc8PzsX9b+CC26z9CndPZEN5v+XB4vvc+D7t/DTufCdtd9o6wXKEXuo8JYbLnOTiwPUxf8Kaw7XIx7DevbAmPZclbw5ty1+Kwr3gZXv5lqI+lwnILVoVtHtoBO58O+5WlQ9ssPTm018CvQvvnOsO8+flhuyP7J3oyI/vDOlesDctVSuHxDe0O+/hQfD2vel9ou0Wrw980s6Ptm85r1671zZs3z8i2iuUK53z1Zzy/axAz+Mx/PpHLP7iaYtn5+3/+NV+/vw8Hejqy7Bs6vKfQlUtTyKYZLpYZLpZxh3TKyGdSpFOGAanqf7M4omT1bzFHPZtjD9hjrrrDaLHMWLlCLp0iG/eLquqtavtU94wj0V6v523BX0fP6ki8HaXMKLszVqpQLFdImZFJG9l0imzKGClVGBwtkc+k6MilyaZToc3dqXiof+1zQLxfZTUfeCbafmJa/fNghNctQKlSoVxxyhUnk06F5zVtNdua2H5te1Tfp6vzVdt4cj3hB3/8HlYt7npN7WZmj7n72qnmU0/jdcimU3z3j97NC7uGWLm4k2U9YWw5n4E/O+ckzj3lWO59Zgc7Dozw1mXzOe34BSzrLfDiniF+8eI+tu8bZrhYpjObpjOfIZMyxkoVRoplSpXqTjGxI1d8YueZK+bYw8V94nATGIVseGMZK1cYK1WojLfHxJvGpP81b9jVdb1Wr+fjyeva7utY1h0q7qRTKXIxKCoe3qyL5QpjJSefTTEvnxl/rY2VKxhGyibe9Gufg5Qx6bAETH6jrn0u6p+H+vkzqRBgKTNKlQqjxQrFisdgqQ2f2Pp1ITQxX01ZnNEsfBCdbrM+NMxsHfC3QBr4prtf3eYqTbKsp2M8LOqduryHU5f3HFa+eF6e009YkLCEiMjsNqtPuTWzNPB1YD1wMvC7ZnZye2slIjJ3zerQAM4A+tz9OXcfA24Fzm9znURE5qzZHhrLgW019/tjmYiItMFsD42kQ2KHHRo1sw1mttnMNg8M6KdZRUSmy2wPjX7g+Jr7K4Dt9TO5+w3uvtbd1y5ZsmTGKiciMtfM9tB4FFhtZqvMLAdcBNzZ5jqJiMxZs/qUW3cvmdlngbsJp9ze5O5PtblaIiJz1qwODQB33whsbHc9RETkKLyMiJkNAL95jYsvBnYdweocLdQuydQuh1ObJHsjtMub3H3Kg8JHXWi8Hma2uZVrr8w1apcEm0ueAAADDElEQVRkapfDqU2SHU3tMtsPhIuIyCyi0BARkZYpNCa7od0VmKXULsnULodTmyQ7atpFxzRERKRl6mmIiEjLFBqRma0zs1+ZWZ+ZXdHu+rSLmb1gZk+a2RNmtjmWLTSze8xsa/x/1P8YiJndZGY7zWxLTVliO1hwbdx3fmlmp7ev5tOrQbv8LzN7Ke4zT5jZeTXTrozt8iszO7c9tZ5+Zna8md1vZs+Y2VNmdnksP+r2GYUG+t2OBB9w9zU1pwheAdzn7quB++L9o923gHV1ZY3aYT2wOv5tAK6foTq2w7c4vF0Avhr3mTXxC7nE19BFwClxmevia+1oVAL+zN3fCpwJXBof/1G3zyg0Av1uR3PnAzfH2zcDF7SxLjPC3X8G7KkrbtQO5wPf9uAhoNfMls1MTWdWg3Zp5HzgVncfdffngT7Ca+2o4+4vu/vj8fZB4BnCzzgcdfuMQiPQ73ZMcGCTmT1mZhti2THu/jKEFwewtG21a69G7aD9Bz4bh1luqhm+nJPtYmYrgdOAhzkK9xmFRtDS73bMEWe5++mE7vOlZva+dlfoDWCu7z/XAycCa4CXgf8Ty+dcu5jZPOCHwJ+6+4FmsyaUvSHaRqERtPS7HXOBu2+P/3cCPyYMJ+yodp3j/53tq2FbNWqHOb3/uPsOdy+7ewX4BhNDUHOqXcwsSwiMW9z9R7H4qNtnFBqBfrcDMLMuM+uu3gbOAbYQ2uLiONvFwB3tqWHbNWqHO4FPxTNizgT2V4ck5oK6sfiPEvYZCO1ykZnlzWwV4aDvIzNdv5lgZgbcCDzj7tfUTDrq9plZf2n0maDf7Rh3DPDjsP+TAb7r7j8xs0eB28zsEuBF4BNtrOOMMLPvAe8HFptZP3AVcDXJ7bAROI9woHcI+MMZr/AMadAu7zezNYThlReATwO4+1NmdhvwNOHsokvdvdyOes+As4A/AJ40sydi2Z9zFO4z+ka4iIi0TMNTIiLSMoWGiIi0TKEhIiItU2iIiEjLFBoiItIyhYaIiLRMoSEiIi1TaIiISMv+P6PJC5iSW0tSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31761.63278287077"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_selection.cross_val_score(model, X, y, scoring='neg_mean_squared_error')\n",
    "#model.summary()\n",
    "\n",
    "#metrics.mean_squared_error(y,model.predict(X))\n",
    "\n",
    "saved_model = load_model('best.h5')\n",
    "\n",
    "metrics.mean_squared_error(y_test,saved_model.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2494948910.54545, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2494948910.54545\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2494948910.54545\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2494948910.54545\n",
      "\n",
      "Epoch 00005: val_loss improved from 2494948910.54545 to 1745300511.03030, saving model to best.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 1745300511.03030 to 262865386.66667, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 262865386.66667 to 90932369.69697, saving model to best.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 90932369.69697\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 90932369.69697\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 90932369.69697\n",
      "\n",
      "Epoch 00011: val_loss improved from 90932369.69697 to 7035982.77273, saving model to best.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 7035982.77273\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 7035982.77273\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 7035982.77273\n",
      "\n",
      "Epoch 00015: val_loss improved from 7035982.77273 to 3227312.78788, saving model to best.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3227312.78788\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3227312.78788\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3227312.78788\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3227312.78788\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3227312.78788\n",
      "\n",
      "Epoch 00021: val_loss improved from 3227312.78788 to 194398.09931, saving model to best.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 194398.09931\n",
      "\n",
      "Epoch 00030: val_loss improved from 194398.09931 to 141157.28631, saving model to best.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 141157.28631 to 103439.58179, saving model to best.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 103439.58179\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 103439.58179\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 103439.58179\n",
      "\n",
      "Epoch 00035: val_loss improved from 103439.58179 to 84779.48219, saving model to best.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 84779.48219\n",
      "\n",
      "Epoch 00049: val_loss improved from 84779.48219 to 82896.59223, saving model to best.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 82896.59223\n",
      "\n",
      "Epoch 00051: val_loss improved from 82896.59223 to 81172.99614, saving model to best.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 81172.99614\n",
      "\n",
      "Epoch 00064: val_loss improved from 81172.99614 to 78372.67971, saving model to best.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 78372.67971\n",
      "\n",
      "Epoch 00080: val_loss improved from 78372.67971 to 72853.51275, saving model to best.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 72853.51275\n",
      "\n",
      "Epoch 00104: val_loss improved from 72853.51275 to 69972.04688, saving model to best.h5\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 69972.04688\n",
      "\n",
      "Epoch 00124: val_loss improved from 69972.04688 to 64901.31954, saving model to best.h5\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 64901.31954\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 64901.31954\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 64901.31954\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 64901.31954\n",
      "\n",
      "Epoch 00129: val_loss improved from 64901.31954 to 64881.36867, saving model to best.h5\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 64881.36867\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 64881.36867\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 64881.36867\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 64881.36867\n",
      "\n",
      "Epoch 00134: val_loss improved from 64881.36867 to 61663.95928, saving model to best.h5\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 61663.95928\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 61663.95928\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 61663.95928\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 61663.95928\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 61663.95928\n",
      "\n",
      "Epoch 00140: val_loss improved from 61663.95928 to 60633.78554, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00141: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 60633.78554\n",
      "\n",
      "Epoch 00164: val_loss improved from 60633.78554 to 54536.62388, saving model to best.h5\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 54536.62388\n",
      "\n",
      "Epoch 00175: val_loss improved from 54536.62388 to 52486.84613, saving model to best.h5\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 52486.84613\n",
      "\n",
      "Epoch 00186: val_loss improved from 52486.84613 to 50915.16610, saving model to best.h5\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 50915.16610\n",
      "\n",
      "Epoch 00219: val_loss improved from 50915.16610 to 49191.83724, saving model to best.h5\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 49191.83724\n",
      "\n",
      "Epoch 00243: val_loss improved from 49191.83724 to 44691.47371, saving model to best.h5\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 44691.47371\n",
      "\n",
      "Epoch 00275: val_loss improved from 44691.47371 to 43800.11953, saving model to best.h5\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 43800.11953\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 43800.11953\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 43800.11953\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 43800.11953\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 43800.11953\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 43800.11953\n",
      "\n",
      "Epoch 00282: val_loss improved from 43800.11953 to 42072.91804, saving model to best.h5\n",
      "\n",
      "Epoch 00283: val_loss improved from 42072.91804 to 39926.27218, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00284: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 39926.27218\n",
      "\n",
      "Epoch 00400: val_loss improved from 39926.27218 to 33146.20005, saving model to best.h5\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 33146.20005\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 33146.20005\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 33146.20005\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 33146.20005\n",
      "\n",
      "Epoch 00405: val_loss improved from 33146.20005 to 32161.82718, saving model to best.h5\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 32161.82718\n",
      "\n",
      "Epoch 00427: val_loss improved from 32161.82718 to 31850.54480, saving model to best.h5\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 31850.54480\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 31850.54480\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 31850.54480\n",
      "\n",
      "Epoch 00431: val_loss improved from 31850.54480 to 29765.19401, saving model to best.h5\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 29765.19401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00440: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 29765.19401\n",
      "\n",
      "Epoch 00463: val_loss improved from 29765.19401 to 28608.87204, saving model to best.h5\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 28608.87204\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 28608.87204\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 28608.87204\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 28608.87204\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 28608.87204\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 28608.87204\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 28608.87204\n",
      "\n",
      "Epoch 00471: val_loss improved from 28608.87204 to 28511.38684, saving model to best.h5\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 28511.38684\n",
      "\n",
      "Epoch 00481: val_loss improved from 28511.38684 to 28255.31688, saving model to best.h5\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 28255.31688\n",
      "\n",
      "Epoch 00500: val_loss improved from 28255.31688 to 27869.96342, saving model to best.h5\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 27869.96342\n",
      "\n",
      "Epoch 00537: val_loss improved from 27869.96342 to 27829.81866, saving model to best.h5\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 27829.81866\n",
      "\n",
      "Epoch 00549: val_loss improved from 27829.81866 to 26921.80611, saving model to best.h5\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 26921.80611\n",
      "\n",
      "Epoch 00558: val_loss improved from 26921.80611 to 26869.28954, saving model to best.h5\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 26869.28954\n",
      "\n",
      "Epoch 00581: val_loss improved from 26869.28954 to 26643.61127, saving model to best.h5\n",
      "\n",
      "Epoch 00582: val_loss improved from 26643.61127 to 26621.98733, saving model to best.h5\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 26621.98733\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00591: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 26621.98733\n",
      "\n",
      "Epoch 00607: val_loss improved from 26621.98733 to 26468.75900, saving model to best.h5\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 26468.75900\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 26468.75900\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 26468.75900\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 26468.75900\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 26468.75900\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 26468.75900\n",
      "\n",
      "Epoch 00614: val_loss improved from 26468.75900 to 26162.49077, saving model to best.h5\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 26162.49077\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 26162.49077\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 26162.49077\n",
      "\n",
      "Epoch 00618: val_loss improved from 26162.49077 to 26161.61825, saving model to best.h5\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 26161.61825\n",
      "\n",
      "Epoch 00652: val_loss improved from 26161.61825 to 26125.20384, saving model to best.h5\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 26125.20384\n",
      "\n",
      "Epoch 00654: val_loss improved from 26125.20384 to 25919.98923, saving model to best.h5\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 25919.98923\n",
      "\n",
      "Epoch 00656: val_loss improved from 25919.98923 to 25853.42448, saving model to best.h5\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 25853.42448\n",
      "\n",
      "Epoch 00680: val_loss improved from 25853.42448 to 25749.84848, saving model to best.h5\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 25749.84848\n",
      "\n",
      "Epoch 00682: val_loss improved from 25749.84848 to 25721.89867, saving model to best.h5\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 25721.89867\n",
      "\n",
      "Epoch 00700: val_loss improved from 25721.89867 to 25545.21887, saving model to best.h5\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 25545.21887\n",
      "\n",
      "Epoch 00721: val_loss improved from 25545.21887 to 25330.76598, saving model to best.h5\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 25330.76598\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 25330.76598\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 25330.76598\n",
      "\n",
      "Epoch 00725: val_loss improved from 25330.76598 to 25137.18442, saving model to best.h5\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 25137.18442\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 25137.18442\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 25137.18442\n",
      "\n",
      "Epoch 00729: val_loss improved from 25137.18442 to 24995.04581, saving model to best.h5\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 24995.04581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00737: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 24995.04581\n",
      "\n",
      "Epoch 00741: val_loss improved from 24995.04581 to 24817.76527, saving model to best.h5\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 24817.76527\n",
      "\n",
      "Epoch 00743: val_loss improved from 24817.76527 to 24783.10440, saving model to best.h5\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 24783.10440\n",
      "\n",
      "Epoch 00769: val_loss improved from 24783.10440 to 24648.93987, saving model to best.h5\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 24648.93987\n",
      "\n",
      "Epoch 00791: val_loss improved from 24648.93987 to 24478.26255, saving model to best.h5\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 24478.26255\n",
      "\n",
      "Epoch 00831: val_loss improved from 24478.26255 to 24201.60452, saving model to best.h5\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 24201.60452\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00886: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 24201.60452\n",
      "\n",
      "Epoch 00947: val_loss improved from 24201.60452 to 23530.88707, saving model to best.h5\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 23530.88707\n",
      "\n",
      "Epoch 01019: val_loss improved from 23530.88707 to 23408.92483, saving model to best.h5\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 23408.92483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01041: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 23408.92483\n",
      "\n",
      "Epoch 01062: val_loss improved from 23408.92483 to 22461.55303, saving model to best.h5\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 22461.55303\n",
      "\n",
      "Epoch 01084: val_loss improved from 22461.55303 to 22394.84706, saving model to best.h5\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 22394.84706\n",
      "\n",
      "Epoch 01107: val_loss improved from 22394.84706 to 22363.54676, saving model to best.h5\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 22363.54676\n",
      "\n",
      "Epoch 01117: val_loss improved from 22363.54676 to 21968.94247, saving model to best.h5\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 21968.94247\n",
      "\n",
      "Epoch 01161: val_loss improved from 21968.94247 to 20904.61565, saving model to best.h5\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 20904.61565\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01189: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 20904.61565\n",
      "\n",
      "Epoch 01291: val_loss improved from 20904.61565 to 20305.31712, saving model to best.h5\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 20305.31712\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01346: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 20305.31712\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 20305.31712\n",
      "Epoch 01491: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 36112811845.81818, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 36112811845.81818 to 12997591195.15152, saving model to best.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 12997591195.15152\n",
      "\n",
      "Epoch 00004: val_loss improved from 12997591195.15152 to 852903806.06061, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 852903806.06061\n",
      "\n",
      "Epoch 00006: val_loss improved from 852903806.06061 to 553355818.66667, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 553355818.66667\n",
      "\n",
      "Epoch 00008: val_loss improved from 553355818.66667 to 81648210.18182, saving model to best.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 81648210.18182\n",
      "\n",
      "Epoch 00010: val_loss improved from 81648210.18182 to 228844.27717, saving model to best.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 228844.27717\n",
      "\n",
      "Epoch 00021: val_loss improved from 228844.27717 to 46047.71277, saving model to best.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 46047.71277\n",
      "\n",
      "Epoch 00033: val_loss improved from 46047.71277 to 41303.97100, saving model to best.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 41303.97100\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 41303.97100\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 41303.97100\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 41303.97100\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 41303.97100\n",
      "\n",
      "Epoch 00039: val_loss improved from 41303.97100 to 39038.09570, saving model to best.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 39038.09570\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 39038.09570\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 39038.09570\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 39038.09570\n",
      "\n",
      "Epoch 00044: val_loss improved from 39038.09570 to 37580.90970, saving model to best.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 37580.90970\n",
      "\n",
      "Epoch 00059: val_loss improved from 37580.90970 to 37418.34848, saving model to best.h5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 37418.34848\n",
      "\n",
      "Epoch 00070: val_loss improved from 37418.34848 to 36515.86852, saving model to best.h5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 36515.86852\n",
      "\n",
      "Epoch 00084: val_loss improved from 36515.86852 to 36343.04297, saving model to best.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 36343.04297\n",
      "\n",
      "Epoch 00131: val_loss improved from 36343.04297 to 34935.55445, saving model to best.h5\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 34935.55445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00153: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 34935.55445\n",
      "\n",
      "Epoch 00164: val_loss improved from 34935.55445 to 34018.03069, saving model to best.h5\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 34018.03069\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 34018.03069\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 34018.03069\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 34018.03069\n",
      "\n",
      "Epoch 00169: val_loss improved from 34018.03069 to 33982.73526, saving model to best.h5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 33982.73526\n",
      "\n",
      "Epoch 00202: val_loss improved from 33982.73526 to 32735.30031, saving model to best.h5\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 32735.30031\n",
      "\n",
      "Epoch 00234: val_loss improved from 32735.30031 to 32501.25478, saving model to best.h5\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 32501.25478\n",
      "\n",
      "Epoch 00247: val_loss improved from 32501.25478 to 32409.00897, saving model to best.h5\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 32409.00897\n",
      "\n",
      "Epoch 00249: val_loss improved from 32409.00897 to 32312.89021, saving model to best.h5\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 32312.89021\n",
      "\n",
      "Epoch 00299: val_loss improved from 32312.89021 to 31912.38521, saving model to best.h5\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 31912.38521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00301: val_loss did not improve from 31912.38521\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 31912.38521\n",
      "\n",
      "Epoch 00303: val_loss improved from 31912.38521 to 31579.10505, saving model to best.h5\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 31579.10505\n",
      "\n",
      "Epoch 00313: val_loss improved from 31579.10505 to 30772.48997, saving model to best.h5\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 30772.48997\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 30772.48997\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 30772.48997\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 30772.48997\n",
      "\n",
      "Epoch 00318: val_loss improved from 30772.48997 to 30620.11269, saving model to best.h5\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 30620.11269\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 30620.11269\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 30620.11269\n",
      "\n",
      "Epoch 00322: val_loss improved from 30620.11269 to 30360.38249, saving model to best.h5\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 30360.38249\n",
      "\n",
      "Epoch 00361: val_loss improved from 30360.38249 to 30195.68359, saving model to best.h5\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 30195.68359\n",
      "\n",
      "Epoch 00392: val_loss improved from 30195.68359 to 30057.62163, saving model to best.h5\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 30057.62163\n",
      "\n",
      "Epoch 00425: val_loss improved from 30057.62163 to 29492.89746, saving model to best.h5\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 29492.89746\n",
      "\n",
      "Epoch 00435: val_loss improved from 29492.89746 to 29261.62417, saving model to best.h5\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 29261.62417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00445: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 29261.62417\n",
      "\n",
      "Epoch 00461: val_loss improved from 29261.62417 to 29214.58745, saving model to best.h5\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 29214.58745\n",
      "\n",
      "Epoch 00512: val_loss improved from 29214.58745 to 28689.30575, saving model to best.h5\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 28689.30575\n",
      "\n",
      "Epoch 00526: val_loss improved from 28689.30575 to 27973.17788, saving model to best.h5\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 27973.17788\n",
      "\n",
      "Epoch 00577: val_loss improved from 27973.17788 to 27684.77178, saving model to best.h5\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 27684.77178\n",
      "\n",
      "Epoch 00593: val_loss improved from 27684.77178 to 27346.69818, saving model to best.h5\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 27346.69818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00599: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 27346.69818\n",
      "\n",
      "Epoch 00664: val_loss improved from 27346.69818 to 25852.71780, saving model to best.h5\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 25852.71780\n",
      "\n",
      "Epoch 00699: val_loss improved from 25852.71780 to 24703.00009, saving model to best.h5\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 24703.00009\n",
      "\n",
      "Epoch 00727: val_loss improved from 24703.00009 to 24162.24177, saving model to best.h5\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 24162.24177\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00753: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 24162.24177\n",
      "\n",
      "Epoch 00765: val_loss improved from 24162.24177 to 23939.40004, saving model to best.h5\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 23939.40004\n",
      "\n",
      "Epoch 00774: val_loss improved from 23939.40004 to 23692.52770, saving model to best.h5\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 23692.52770\n",
      "\n",
      "Epoch 00838: val_loss improved from 23692.52770 to 23129.66494, saving model to best.h5\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 23129.66494\n",
      "\n",
      "Epoch 00866: val_loss improved from 23129.66494 to 22135.23322, saving model to best.h5\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 22135.23322\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00906: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 22135.23322\n",
      "\n",
      "Epoch 00987: val_loss improved from 22135.23322 to 20057.34206, saving model to best.h5\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 20057.34206\n",
      "\n",
      "Epoch 01010: val_loss improved from 20057.34206 to 19406.67480, saving model to best.h5\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 19406.67480\n",
      "\n",
      "Epoch 01038: val_loss improved from 19406.67480 to 18930.95142, saving model to best.h5\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 18930.95142\n",
      "\n",
      "Epoch 01051: val_loss improved from 18930.95142 to 18837.70287, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01052: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 18837.70287\n",
      "\n",
      "Epoch 01108: val_loss improved from 18837.70287 to 18472.84450, saving model to best.h5\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 18472.84450\n",
      "\n",
      "Epoch 01128: val_loss improved from 18472.84450 to 18452.16147, saving model to best.h5\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 18452.16147\n",
      "\n",
      "Epoch 01148: val_loss improved from 18452.16147 to 18231.38306, saving model to best.h5\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 18231.38306\n",
      "\n",
      "Epoch 01173: val_loss improved from 18231.38306 to 17935.09956, saving model to best.h5\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 17935.09956\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01205: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 17935.09956\n",
      "\n",
      "Epoch 01239: val_loss improved from 17935.09956 to 17835.37363, saving model to best.h5\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 17835.37363\n",
      "\n",
      "Epoch 01269: val_loss improved from 17835.37363 to 17733.08900, saving model to best.h5\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 17733.08900\n",
      "\n",
      "Epoch 01289: val_loss improved from 17733.08900 to 17201.46214, saving model to best.h5\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 17201.46214\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01352: val_loss did not improve from 17201.46214\n",
      "\n",
      "Epoch 01353: val_loss improved from 17201.46214 to 16638.07126, saving model to best.h5\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 16638.07126\n",
      "\n",
      "Epoch 01398: val_loss improved from 16638.07126 to 16514.52630, saving model to best.h5\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 16514.52630\n",
      "\n",
      "Epoch 01487: val_loss improved from 16514.52630 to 14156.04478, saving model to best.h5\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 14156.04478\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01501: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01514: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01529: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01546: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01547: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01550: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01551: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01553: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01556: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01558: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01560: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01562: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01568: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01570: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01579: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01580: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01581: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01582: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01583: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01584: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01585: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01586: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01587: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01588: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01589: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01590: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01591: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01592: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01593: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01594: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01595: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01596: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01597: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01598: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01599: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01600: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01601: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01602: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01603: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01604: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01605: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01606: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01607: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01608: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01609: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01610: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01611: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01612: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01613: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01614: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01615: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01616: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01617: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01618: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01619: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01620: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01621: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01622: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01623: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01624: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01625: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01626: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01627: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01628: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01629: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01630: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01631: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01632: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01633: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01634: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01635: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01636: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01637: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01638: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01639: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01640: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01641: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01642: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01643: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01644: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01645: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01646: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01647: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01648: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01649: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01650: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01651: val_loss did not improve from 14156.04478\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01652: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01653: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01654: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01655: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01656: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01657: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01658: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01659: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01660: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01661: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01662: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01663: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01664: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01665: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01666: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01667: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01668: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01669: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01670: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01671: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01672: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01673: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01674: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01675: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01676: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01677: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01678: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01679: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01680: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01681: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01682: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01683: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01684: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01685: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01686: val_loss did not improve from 14156.04478\n",
      "\n",
      "Epoch 01687: val_loss did not improve from 14156.04478\n",
      "Epoch 01687: early stopping\n",
      "26\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 73421774599.75757, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 73421774599.75757\n",
      "\n",
      "Epoch 00003: val_loss improved from 73421774599.75757 to 28080295191.27273, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 28080295191.27273 to 9565343154.42424, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 9565343154.42424\n",
      "\n",
      "Epoch 00006: val_loss improved from 9565343154.42424 to 928224236.60606, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 928224236.60606\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 928224236.60606\n",
      "\n",
      "Epoch 00009: val_loss improved from 928224236.60606 to 125236427.63636, saving model to best.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 125236427.63636\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 125236427.63636\n",
      "\n",
      "Epoch 00012: val_loss improved from 125236427.63636 to 102490473.93939, saving model to best.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 102490473.93939\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 102490473.93939\n",
      "\n",
      "Epoch 00015: val_loss improved from 102490473.93939 to 25067199.15152, saving model to best.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 25067199.15152\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 25067199.15152\n",
      "\n",
      "Epoch 00018: val_loss improved from 25067199.15152 to 4922755.39394, saving model to best.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 4922755.39394\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4922755.39394\n",
      "\n",
      "Epoch 00021: val_loss improved from 4922755.39394 to 313786.49251, saving model to best.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 313786.49251\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 313786.49251\n",
      "\n",
      "Epoch 00024: val_loss improved from 313786.49251 to 178200.21023, saving model to best.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 178200.21023\n",
      "\n",
      "Epoch 00051: val_loss improved from 178200.21023 to 160127.46023, saving model to best.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 160127.46023\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 160127.46023\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 160127.46023\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 160127.46023\n",
      "\n",
      "Epoch 00056: val_loss improved from 160127.46023 to 151025.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 151025.62500\n",
      "\n",
      "Epoch 00058: val_loss improved from 151025.62500 to 150540.96307, saving model to best.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 150540.96307\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 150540.96307\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 150540.96307\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 150540.96307\n",
      "\n",
      "Epoch 00063: val_loss improved from 150540.96307 to 135810.70170, saving model to best.h5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 135810.70170\n",
      "\n",
      "Epoch 00065: val_loss improved from 135810.70170 to 133736.40862, saving model to best.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 133736.40862\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 133736.40862\n",
      "\n",
      "Epoch 00068: val_loss improved from 133736.40862 to 131303.34754, saving model to best.h5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 131303.34754\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 131303.34754\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 131303.34754\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 131303.34754\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 131303.34754\n",
      "\n",
      "Epoch 00074: val_loss improved from 131303.34754 to 129694.94555, saving model to best.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 129694.94555\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 129694.94555\n",
      "\n",
      "Epoch 00077: val_loss improved from 129694.94555 to 126782.43087, saving model to best.h5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 126782.43087\n",
      "\n",
      "Epoch 00079: val_loss improved from 126782.43087 to 126479.99574, saving model to best.h5\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 126479.99574\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 126479.99574\n",
      "\n",
      "Epoch 00082: val_loss improved from 126479.99574 to 124162.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00083: val_loss improved from 124162.50000 to 123751.85038, saving model to best.h5\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 123751.85038\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 123751.85038\n",
      "\n",
      "Epoch 00086: val_loss improved from 123751.85038 to 122327.08191, saving model to best.h5\n",
      "\n",
      "Epoch 00087: val_loss improved from 122327.08191 to 122195.76136, saving model to best.h5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 122195.76136\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 122195.76136\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 122195.76136\n",
      "\n",
      "Epoch 00091: val_loss improved from 122195.76136 to 121578.74384, saving model to best.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 121578.74384\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 121578.74384\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 121578.74384\n",
      "\n",
      "Epoch 00095: val_loss improved from 121578.74384 to 120064.36080, saving model to best.h5\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 120064.36080\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00097: val_loss did not improve from 120064.36080\n",
      "\n",
      "Epoch 00098: val_loss improved from 120064.36080 to 119727.33523, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 119727.33523 to 119106.17424, saving model to best.h5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 119106.17424\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 119106.17424\n",
      "\n",
      "Epoch 00102: val_loss improved from 119106.17424 to 118226.96638, saving model to best.h5\n",
      "\n",
      "Epoch 00103: val_loss improved from 118226.96638 to 118087.86648, saving model to best.h5\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 118087.86648\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 118087.86648\n",
      "\n",
      "Epoch 00106: val_loss improved from 118087.86648 to 118028.77415, saving model to best.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 118028.77415 to 117272.82244, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 117272.82244\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 117272.82244\n",
      "\n",
      "Epoch 00110: val_loss improved from 117272.82244 to 116609.98674, saving model to best.h5\n",
      "\n",
      "Epoch 00111: val_loss improved from 116609.98674 to 116241.08665, saving model to best.h5\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 116241.08665\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 116241.08665\n",
      "\n",
      "Epoch 00114: val_loss improved from 116241.08665 to 115841.36032, saving model to best.h5\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 115841.36032\n",
      "\n",
      "Epoch 00116: val_loss improved from 115841.36032 to 115568.88258, saving model to best.h5\n",
      "\n",
      "Epoch 00117: val_loss improved from 115568.88258 to 115112.62784, saving model to best.h5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 115112.62784\n",
      "\n",
      "Epoch 00119: val_loss improved from 115112.62784 to 114665.20644, saving model to best.h5\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 114665.20644\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 114665.20644\n",
      "\n",
      "Epoch 00122: val_loss improved from 114665.20644 to 114158.09943, saving model to best.h5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 114158.09943\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 114158.09943\n",
      "\n",
      "Epoch 00125: val_loss improved from 114158.09943 to 113373.20312, saving model to best.h5\n",
      "\n",
      "Epoch 00126: val_loss improved from 113373.20312 to 113182.09422, saving model to best.h5\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 113182.09422\n",
      "\n",
      "Epoch 00128: val_loss improved from 113182.09422 to 113046.68040, saving model to best.h5\n",
      "\n",
      "Epoch 00129: val_loss improved from 113046.68040 to 112894.19318, saving model to best.h5\n",
      "\n",
      "Epoch 00130: val_loss improved from 112894.19318 to 112375.00473, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 112375.00473\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 112375.00473\n",
      "\n",
      "Epoch 00133: val_loss improved from 112375.00473 to 111786.97159, saving model to best.h5\n",
      "\n",
      "Epoch 00134: val_loss improved from 111786.97159 to 111693.27415, saving model to best.h5\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 111693.27415\n",
      "\n",
      "Epoch 00136: val_loss improved from 111693.27415 to 111387.71686, saving model to best.h5\n",
      "\n",
      "Epoch 00137: val_loss improved from 111387.71686 to 111075.62642, saving model to best.h5\n",
      "\n",
      "Epoch 00138: val_loss improved from 111075.62642 to 110726.88826, saving model to best.h5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 110726.88826\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 110726.88826\n",
      "\n",
      "Epoch 00141: val_loss improved from 110726.88826 to 110344.02936, saving model to best.h5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 110344.02936\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 110344.02936\n",
      "\n",
      "Epoch 00144: val_loss improved from 110344.02936 to 109884.77036, saving model to best.h5\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 109884.77036\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 109884.77036\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 109884.77036\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 109884.77036\n",
      "\n",
      "Epoch 00149: val_loss improved from 109884.77036 to 108562.13447, saving model to best.h5\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 108562.13447\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 108562.13447\n",
      "\n",
      "Epoch 00152: val_loss improved from 108562.13447 to 108289.80729, saving model to best.h5\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 108289.80729\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 108289.80729\n",
      "\n",
      "Epoch 00155: val_loss improved from 108289.80729 to 108036.32150, saving model to best.h5\n",
      "\n",
      "Epoch 00156: val_loss improved from 108036.32150 to 107088.40483, saving model to best.h5\n",
      "\n",
      "Epoch 00157: val_loss improved from 107088.40483 to 106947.78977, saving model to best.h5\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 106947.78977\n",
      "\n",
      "Epoch 00159: val_loss improved from 106947.78977 to 106656.64773, saving model to best.h5\n",
      "\n",
      "Epoch 00160: val_loss improved from 106656.64773 to 106253.36837, saving model to best.h5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 106253.36837\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 106253.36837\n",
      "\n",
      "Epoch 00163: val_loss improved from 106253.36837 to 105639.90956, saving model to best.h5\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 105639.90956\n",
      "\n",
      "Epoch 00165: val_loss improved from 105639.90956 to 105380.46023, saving model to best.h5\n",
      "\n",
      "Epoch 00166: val_loss improved from 105380.46023 to 105069.46354, saving model to best.h5\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 105069.46354\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 105069.46354\n",
      "\n",
      "Epoch 00169: val_loss improved from 105069.46354 to 104487.06108, saving model to best.h5\n",
      "\n",
      "Epoch 00170: val_loss improved from 104487.06108 to 104228.78977, saving model to best.h5\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 104228.78977\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 104228.78977\n",
      "\n",
      "Epoch 00173: val_loss improved from 104228.78977 to 103635.59706, saving model to best.h5\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 103635.59706\n",
      "\n",
      "Epoch 00175: val_loss improved from 103635.59706 to 103260.04545, saving model to best.h5\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 103260.04545\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 103260.04545\n",
      "\n",
      "Epoch 00178: val_loss improved from 103260.04545 to 102678.79830, saving model to best.h5\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 102678.79830\n",
      "\n",
      "Epoch 00180: val_loss improved from 102678.79830 to 102434.48177, saving model to best.h5\n",
      "\n",
      "Epoch 00181: val_loss improved from 102434.48177 to 102013.28267, saving model to best.h5\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 102013.28267\n",
      "\n",
      "Epoch 00183: val_loss improved from 102013.28267 to 101621.49290, saving model to best.h5\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 101621.49290\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 101621.49290\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 101621.49290\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 101621.49290\n",
      "\n",
      "Epoch 00188: val_loss improved from 101621.49290 to 100629.39489, saving model to best.h5\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 100629.39489\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 100629.39489\n",
      "\n",
      "Epoch 00191: val_loss improved from 100629.39489 to 100321.51349, saving model to best.h5\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 100321.51349\n",
      "\n",
      "Epoch 00193: val_loss improved from 100321.51349 to 99624.17898, saving model to best.h5\n",
      "\n",
      "Epoch 00194: val_loss improved from 99624.17898 to 99609.79380, saving model to best.h5\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 99609.79380\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 99609.79380\n",
      "\n",
      "Epoch 00197: val_loss improved from 99609.79380 to 98795.93703, saving model to best.h5\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 98795.93703\n",
      "\n",
      "Epoch 00199: val_loss improved from 98795.93703 to 98537.63731, saving model to best.h5\n",
      "\n",
      "Epoch 00200: val_loss improved from 98537.63731 to 98220.76184, saving model to best.h5\n",
      "\n",
      "Epoch 00201: val_loss improved from 98220.76184 to 98056.56510, saving model to best.h5\n",
      "\n",
      "Epoch 00202: val_loss improved from 98056.56510 to 97996.31723, saving model to best.h5\n",
      "\n",
      "Epoch 00203: val_loss improved from 97996.31723 to 97769.51610, saving model to best.h5\n",
      "\n",
      "Epoch 00204: val_loss improved from 97769.51610 to 97449.26089, saving model to best.h5\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 97449.26089\n",
      "\n",
      "Epoch 00206: val_loss improved from 97449.26089 to 97011.93087, saving model to best.h5\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 97011.93087\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 97011.93087\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 97011.93087\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 97011.93087\n",
      "\n",
      "Epoch 00211: val_loss improved from 97011.93087 to 95996.78835, saving model to best.h5\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 95996.78835\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 95996.78835\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 95996.78835\n",
      "\n",
      "Epoch 00215: val_loss improved from 95996.78835 to 95228.55350, saving model to best.h5\n",
      "\n",
      "Epoch 00216: val_loss improved from 95228.55350 to 95141.43821, saving model to best.h5\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 95141.43821\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 95141.43821\n",
      "\n",
      "Epoch 00219: val_loss improved from 95141.43821 to 95062.06937, saving model to best.h5\n",
      "\n",
      "Epoch 00220: val_loss improved from 95062.06937 to 94878.04640, saving model to best.h5\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 94878.04640\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00222: val_loss improved from 94878.04640 to 93968.90341, saving model to best.h5\n",
      "\n",
      "Epoch 00223: val_loss improved from 93968.90341 to 93724.39299, saving model to best.h5\n",
      "\n",
      "Epoch 00224: val_loss improved from 93724.39299 to 93536.85227, saving model to best.h5\n",
      "\n",
      "Epoch 00225: val_loss improved from 93536.85227 to 93221.90199, saving model to best.h5\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 93221.90199\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 93221.90199\n",
      "\n",
      "Epoch 00228: val_loss improved from 93221.90199 to 93176.01326, saving model to best.h5\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 93176.01326\n",
      "\n",
      "Epoch 00230: val_loss improved from 93176.01326 to 92461.11671, saving model to best.h5\n",
      "\n",
      "Epoch 00231: val_loss improved from 92461.11671 to 92093.88447, saving model to best.h5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 92093.88447\n",
      "\n",
      "Epoch 00233: val_loss improved from 92093.88447 to 91721.69129, saving model to best.h5\n",
      "\n",
      "Epoch 00234: val_loss improved from 91721.69129 to 91471.02486, saving model to best.h5\n",
      "\n",
      "Epoch 00235: val_loss improved from 91471.02486 to 91272.58168, saving model to best.h5\n",
      "\n",
      "Epoch 00236: val_loss improved from 91272.58168 to 91228.52983, saving model to best.h5\n",
      "\n",
      "Epoch 00237: val_loss improved from 91228.52983 to 90881.83925, saving model to best.h5\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 90881.83925\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 90881.83925\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 90881.83925\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 90881.83925\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 90881.83925\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 90881.83925\n",
      "\n",
      "Epoch 00244: val_loss improved from 90881.83925 to 90030.20218, saving model to best.h5\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 90030.20218\n",
      "\n",
      "Epoch 00246: val_loss improved from 90030.20218 to 89396.67282, saving model to best.h5\n",
      "\n",
      "Epoch 00247: val_loss improved from 89396.67282 to 89294.18703, saving model to best.h5\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 89294.18703\n",
      "\n",
      "Epoch 00249: val_loss improved from 89294.18703 to 88637.19673, saving model to best.h5\n",
      "\n",
      "Epoch 00250: val_loss improved from 88637.19673 to 88510.67614, saving model to best.h5\n",
      "\n",
      "Epoch 00251: val_loss improved from 88510.67614 to 88239.65294, saving model to best.h5\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 88239.65294\n",
      "\n",
      "Epoch 00253: val_loss improved from 88239.65294 to 87979.47585, saving model to best.h5\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 87979.47585\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 87979.47585\n",
      "\n",
      "Epoch 00256: val_loss improved from 87979.47585 to 87380.88755, saving model to best.h5\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 87380.88755\n",
      "\n",
      "Epoch 00258: val_loss improved from 87380.88755 to 86950.56747, saving model to best.h5\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 86950.56747\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 86950.56747\n",
      "\n",
      "Epoch 00261: val_loss improved from 86950.56747 to 86405.97538, saving model to best.h5\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 86405.97538\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 86405.97538\n",
      "\n",
      "Epoch 00264: val_loss improved from 86405.97538 to 85808.82576, saving model to best.h5\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 85808.82576\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 85808.82576\n",
      "\n",
      "Epoch 00267: val_loss improved from 85808.82576 to 85219.01823, saving model to best.h5\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 85219.01823\n",
      "\n",
      "Epoch 00269: val_loss improved from 85219.01823 to 84857.56960, saving model to best.h5\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 84857.56960\n",
      "\n",
      "Epoch 00271: val_loss improved from 84857.56960 to 84502.21283, saving model to best.h5\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 84502.21283\n",
      "\n",
      "Epoch 00273: val_loss improved from 84502.21283 to 84122.40009, saving model to best.h5\n",
      "\n",
      "Epoch 00274: val_loss improved from 84122.40009 to 83984.79522, saving model to best.h5\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 83984.79522\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 83984.79522\n",
      "\n",
      "Epoch 00277: val_loss improved from 83984.79522 to 83781.35417, saving model to best.h5\n",
      "\n",
      "Epoch 00278: val_loss improved from 83781.35417 to 83240.59399, saving model to best.h5\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 83240.59399\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 83240.59399\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 83240.59399\n",
      "\n",
      "Epoch 00282: val_loss improved from 83240.59399 to 82532.32955, saving model to best.h5\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 82532.32955\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 82532.32955\n",
      "\n",
      "Epoch 00285: val_loss improved from 82532.32955 to 82089.74100, saving model to best.h5\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 82089.74100\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 82089.74100\n",
      "\n",
      "Epoch 00288: val_loss improved from 82089.74100 to 81890.09801, saving model to best.h5\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 81890.09801\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 81890.09801\n",
      "\n",
      "Epoch 00291: val_loss improved from 81890.09801 to 80919.96449, saving model to best.h5\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 80919.96449\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 80919.96449\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 80919.96449\n",
      "\n",
      "Epoch 00295: val_loss improved from 80919.96449 to 80541.21638, saving model to best.h5\n",
      "\n",
      "Epoch 00296: val_loss improved from 80541.21638 to 80079.66193, saving model to best.h5\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 80079.66193\n",
      "\n",
      "Epoch 00305: val_loss improved from 80079.66193 to 78780.72940, saving model to best.h5\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 78780.72940\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 78780.72940\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 78780.72940\n",
      "\n",
      "Epoch 00309: val_loss improved from 78780.72940 to 77810.79593, saving model to best.h5\n",
      "\n",
      "Epoch 00310: val_loss improved from 77810.79593 to 77655.68016, saving model to best.h5\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 77655.68016\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 77655.68016\n",
      "\n",
      "Epoch 00313: val_loss improved from 77655.68016 to 77199.60630, saving model to best.h5\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 77199.60630\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 77199.60630\n",
      "\n",
      "Epoch 00316: val_loss improved from 77199.60630 to 76730.49858, saving model to best.h5\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 76730.49858\n",
      "\n",
      "Epoch 00318: val_loss improved from 76730.49858 to 76507.08807, saving model to best.h5\n",
      "\n",
      "Epoch 00319: val_loss improved from 76507.08807 to 76262.33120, saving model to best.h5\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 76262.33120\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 76262.33120\n",
      "\n",
      "Epoch 00322: val_loss improved from 76262.33120 to 76016.24266, saving model to best.h5\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 76016.24266\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 76016.24266\n",
      "\n",
      "Epoch 00325: val_loss improved from 76016.24266 to 75234.03196, saving model to best.h5\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 75234.03196\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 75234.03196\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 75234.03196\n",
      "\n",
      "Epoch 00329: val_loss improved from 75234.03196 to 74613.03717, saving model to best.h5\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 74613.03717\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 74613.03717\n",
      "\n",
      "Epoch 00332: val_loss improved from 74613.03717 to 74575.06937, saving model to best.h5\n",
      "\n",
      "Epoch 00333: val_loss improved from 74575.06937 to 73924.28954, saving model to best.h5\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 73924.28954\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 73924.28954\n",
      "\n",
      "Epoch 00336: val_loss improved from 73924.28954 to 73415.69602, saving model to best.h5\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 73415.69602\n",
      "\n",
      "Epoch 00338: val_loss improved from 73415.69602 to 73191.63660, saving model to best.h5\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 73191.63660\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 73191.63660\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 73191.63660\n",
      "\n",
      "Epoch 00342: val_loss improved from 73191.63660 to 72528.95147, saving model to best.h5\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 72528.95147\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 72528.95147\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 72528.95147\n",
      "\n",
      "Epoch 00346: val_loss improved from 72528.95147 to 72336.17756, saving model to best.h5\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 72336.17756\n",
      "\n",
      "Epoch 00348: val_loss improved from 72336.17756 to 71668.99171, saving model to best.h5\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 71668.99171\n",
      "\n",
      "Epoch 00350: val_loss improved from 71668.99171 to 71301.47514, saving model to best.h5\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 71301.47514\n",
      "\n",
      "Epoch 00352: val_loss improved from 71301.47514 to 71250.59044, saving model to best.h5\n",
      "\n",
      "Epoch 00353: val_loss improved from 71250.59044 to 70991.56487, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00354: val_loss improved from 70991.56487 to 70822.57647, saving model to best.h5\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 70822.57647\n",
      "\n",
      "Epoch 00356: val_loss improved from 70822.57647 to 70449.51373, saving model to best.h5\n",
      "\n",
      "Epoch 00357: val_loss improved from 70449.51373 to 70232.92969, saving model to best.h5\n",
      "\n",
      "Epoch 00358: val_loss improved from 70232.92969 to 70100.66738, saving model to best.h5\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 70100.66738\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 70100.66738\n",
      "\n",
      "Epoch 00361: val_loss improved from 70100.66738 to 69668.79119, saving model to best.h5\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 69668.79119\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 69668.79119\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 69668.79119\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 69668.79119\n",
      "\n",
      "Epoch 00366: val_loss improved from 69668.79119 to 69423.25971, saving model to best.h5\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 69423.25971\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 69423.25971\n",
      "\n",
      "Epoch 00369: val_loss improved from 69423.25971 to 68720.31439, saving model to best.h5\n",
      "\n",
      "Epoch 00370: val_loss improved from 68720.31439 to 68397.65294, saving model to best.h5\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 68397.65294\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 68397.65294\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 68397.65294\n",
      "\n",
      "Epoch 00374: val_loss improved from 68397.65294 to 67766.70573, saving model to best.h5\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 67766.70573\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 67766.70573\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 67766.70573\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 67766.70573\n",
      "\n",
      "Epoch 00379: val_loss improved from 67766.70573 to 67558.16430, saving model to best.h5\n",
      "\n",
      "Epoch 00380: val_loss improved from 67558.16430 to 66961.30421, saving model to best.h5\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 66961.30421\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 66961.30421\n",
      "\n",
      "Epoch 00383: val_loss improved from 66961.30421 to 66644.58594, saving model to best.h5\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 66644.58594\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 66644.58594\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 66644.58594\n",
      "\n",
      "Epoch 00387: val_loss improved from 66644.58594 to 66323.60985, saving model to best.h5\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 66323.60985\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 66323.60985\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 66323.60985\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 66323.60985\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 66323.60985\n",
      "\n",
      "Epoch 00393: val_loss improved from 66323.60985 to 65208.39607, saving model to best.h5\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 65208.39607\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 65208.39607\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 65208.39607\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 65208.39607\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 65208.39607\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 65208.39607\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 65208.39607\n",
      "\n",
      "Epoch 00401: val_loss improved from 65208.39607 to 64617.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00402: val_loss improved from 64617.96875 to 64066.11056, saving model to best.h5\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 64066.11056\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 64066.11056\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 64066.11056\n",
      "\n",
      "Epoch 00406: val_loss improved from 64066.11056 to 63677.84801, saving model to best.h5\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 63677.84801\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 63677.84801\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 63677.84801\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 63677.84801\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 63677.84801\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 63677.84801\n",
      "\n",
      "Epoch 00413: val_loss improved from 63677.84801 to 62963.44223, saving model to best.h5\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 62963.44223\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 62963.44223\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 62963.44223\n",
      "\n",
      "Epoch 00417: val_loss improved from 62963.44223 to 62095.46709, saving model to best.h5\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 62095.46709\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 62095.46709\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 62095.46709\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 62095.46709\n",
      "\n",
      "Epoch 00422: val_loss improved from 62095.46709 to 61597.18561, saving model to best.h5\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 61597.18561\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 61597.18561\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 61597.18561\n",
      "\n",
      "Epoch 00426: val_loss improved from 61597.18561 to 61199.21733, saving model to best.h5\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 61199.21733\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 61199.21733\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 61199.21733\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 61199.21733\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 61199.21733\n",
      "\n",
      "Epoch 00432: val_loss improved from 61199.21733 to 60556.80232, saving model to best.h5\n",
      "\n",
      "Epoch 00433: val_loss improved from 60556.80232 to 60416.75047, saving model to best.h5\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 60416.75047\n",
      "\n",
      "Epoch 00435: val_loss improved from 60416.75047 to 60168.84872, saving model to best.h5\n",
      "\n",
      "Epoch 00436: val_loss improved from 60168.84872 to 59994.68868, saving model to best.h5\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 59994.68868\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 59994.68868\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 59994.68868\n",
      "\n",
      "Epoch 00440: val_loss improved from 59994.68868 to 59933.80800, saving model to best.h5\n",
      "\n",
      "Epoch 00441: val_loss improved from 59933.80800 to 59306.59375, saving model to best.h5\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 59306.59375\n",
      "\n",
      "Epoch 00443: val_loss improved from 59306.59375 to 59252.29380, saving model to best.h5\n",
      "\n",
      "Epoch 00444: val_loss improved from 59252.29380 to 58986.74242, saving model to best.h5\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 58986.74242\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 58986.74242\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 58986.74242\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 58986.74242\n",
      "\n",
      "Epoch 00449: val_loss improved from 58986.74242 to 58599.81037, saving model to best.h5\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 58599.81037\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 58599.81037\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 58599.81037\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 58599.81037\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 58599.81037\n",
      "\n",
      "Epoch 00455: val_loss improved from 58599.81037 to 57802.85062, saving model to best.h5\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 57802.85062\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 57802.85062\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 57802.85062\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 57802.85062\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 57802.85062\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 57802.85062\n",
      "\n",
      "Epoch 00462: val_loss improved from 57802.85062 to 57675.89891, saving model to best.h5\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 57675.89891\n",
      "\n",
      "Epoch 00464: val_loss improved from 57675.89891 to 57011.96567, saving model to best.h5\n",
      "\n",
      "Epoch 00465: val_loss improved from 57011.96567 to 56753.47206, saving model to best.h5\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 56753.47206\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 56753.47206\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 56753.47206\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 56753.47206\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 56753.47206\n",
      "\n",
      "Epoch 00471: val_loss improved from 56753.47206 to 56193.99242, saving model to best.h5\n",
      "\n",
      "Epoch 00472: val_loss improved from 56193.99242 to 56112.12050, saving model to best.h5\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 56112.12050\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 56112.12050\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 56112.12050\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 56112.12050\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 56112.12050\n",
      "\n",
      "Epoch 00478: val_loss improved from 56112.12050 to 55501.67519, saving model to best.h5\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 55501.67519\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 55501.67519\n",
      "\n",
      "Epoch 00481: val_loss improved from 55501.67519 to 55434.47964, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00482: val_loss did not improve from 55434.47964\n",
      "\n",
      "Epoch 00483: val_loss improved from 55434.47964 to 55415.12997, saving model to best.h5\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 55415.12997\n",
      "\n",
      "Epoch 00485: val_loss improved from 55415.12997 to 54798.96236, saving model to best.h5\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 54798.96236\n",
      "\n",
      "Epoch 00487: val_loss improved from 54798.96236 to 54562.91809, saving model to best.h5\n",
      "\n",
      "Epoch 00488: val_loss improved from 54562.91809 to 54475.27841, saving model to best.h5\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 54475.27841\n",
      "\n",
      "Epoch 00490: val_loss improved from 54475.27841 to 54347.68490, saving model to best.h5\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 54347.68490\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 54347.68490\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 54347.68490\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 54347.68490\n",
      "\n",
      "Epoch 00495: val_loss improved from 54347.68490 to 53821.69768, saving model to best.h5\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 53821.69768\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 53821.69768\n",
      "\n",
      "Epoch 00498: val_loss improved from 53821.69768 to 53574.29190, saving model to best.h5\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 53574.29190\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 53574.29190\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 53574.29190\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 53574.29190\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 53574.29190\n",
      "\n",
      "Epoch 00504: val_loss improved from 53574.29190 to 53176.78101, saving model to best.h5\n",
      "\n",
      "Epoch 00505: val_loss improved from 53176.78101 to 53161.16974, saving model to best.h5\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 53161.16974\n",
      "\n",
      "Epoch 00507: val_loss improved from 53161.16974 to 52844.42543, saving model to best.h5\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 52844.42543\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 52844.42543\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 52844.42543\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 52844.42543\n",
      "\n",
      "Epoch 00512: val_loss improved from 52844.42543 to 52413.29025, saving model to best.h5\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 52413.29025\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 52413.29025\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 52413.29025\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 52413.29025\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 52413.29025\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 52413.29025\n",
      "\n",
      "Epoch 00519: val_loss improved from 52413.29025 to 51783.66098, saving model to best.h5\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 51783.66098\n",
      "\n",
      "Epoch 00521: val_loss improved from 51783.66098 to 51725.79214, saving model to best.h5\n",
      "\n",
      "Epoch 00522: val_loss improved from 51725.79214 to 51581.41383, saving model to best.h5\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 51581.41383\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 51581.41383\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 51581.41383\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 51581.41383\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 51581.41383\n",
      "\n",
      "Epoch 00528: val_loss improved from 51581.41383 to 51355.17353, saving model to best.h5\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 51355.17353\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 51355.17353\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 51355.17353\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 51355.17353\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 51355.17353\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 51355.17353\n",
      "\n",
      "Epoch 00535: val_loss improved from 51355.17353 to 50702.06108, saving model to best.h5\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 50702.06108\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 50702.06108\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 50702.06108\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 50702.06108\n",
      "\n",
      "Epoch 00540: val_loss improved from 50702.06108 to 50133.83996, saving model to best.h5\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 50133.83996\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 50133.83996\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 50133.83996\n",
      "\n",
      "Epoch 00544: val_loss improved from 50133.83996 to 49970.25284, saving model to best.h5\n",
      "\n",
      "Epoch 00545: val_loss improved from 49970.25284 to 49703.39465, saving model to best.h5\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 49703.39465\n",
      "\n",
      "Epoch 00547: val_loss improved from 49703.39465 to 49592.57978, saving model to best.h5\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 49592.57978\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 49592.57978\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 49592.57978\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 49592.57978\n",
      "\n",
      "Epoch 00552: val_loss improved from 49592.57978 to 49505.37192, saving model to best.h5\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 49505.37192\n",
      "\n",
      "Epoch 00554: val_loss improved from 49505.37192 to 49042.73509, saving model to best.h5\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 49042.73509\n",
      "\n",
      "Epoch 00563: val_loss improved from 49042.73509 to 48654.23390, saving model to best.h5\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 48654.23390\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 48654.23390\n",
      "\n",
      "Epoch 00566: val_loss improved from 48654.23390 to 48282.04119, saving model to best.h5\n",
      "\n",
      "Epoch 00567: val_loss improved from 48282.04119 to 48169.42543, saving model to best.h5\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 48169.42543\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 48169.42543\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 48169.42543\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 48169.42543\n",
      "\n",
      "Epoch 00572: val_loss improved from 48169.42543 to 47906.43987, saving model to best.h5\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 47906.43987\n",
      "\n",
      "Epoch 00574: val_loss improved from 47906.43987 to 47775.13826, saving model to best.h5\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 47775.13826\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 47775.13826\n",
      "\n",
      "Epoch 00577: val_loss improved from 47775.13826 to 47635.74669, saving model to best.h5\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 47635.74669\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 47635.74669\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 47635.74669\n",
      "\n",
      "Epoch 00581: val_loss improved from 47635.74669 to 47160.63210, saving model to best.h5\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 47160.63210\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 47160.63210\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 47160.63210\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 47160.63210\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 47160.63210\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 47160.63210\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 47160.63210\n",
      "\n",
      "Epoch 00589: val_loss improved from 47160.63210 to 46667.41738, saving model to best.h5\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 46667.41738\n",
      "\n",
      "Epoch 00591: val_loss improved from 46667.41738 to 46620.69815, saving model to best.h5\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 46620.69815\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 46620.69815\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 46620.69815\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 46620.69815\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 46620.69815\n",
      "\n",
      "Epoch 00597: val_loss improved from 46620.69815 to 46240.32079, saving model to best.h5\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 46240.32079\n",
      "\n",
      "Epoch 00599: val_loss improved from 46240.32079 to 46069.89536, saving model to best.h5\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 46069.89536\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 46069.89536\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 46069.89536\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 46069.89536\n",
      "\n",
      "Epoch 00604: val_loss improved from 46069.89536 to 45785.23887, saving model to best.h5\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 45785.23887\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 45785.23887\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 45785.23887\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 45785.23887\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 45785.23887\n",
      "\n",
      "Epoch 00610: val_loss improved from 45785.23887 to 45634.00876, saving model to best.h5\n",
      "\n",
      "Epoch 00611: val_loss improved from 45634.00876 to 45521.91690, saving model to best.h5\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 45521.91690\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 45521.91690\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 45521.91690\n",
      "\n",
      "Epoch 00615: val_loss improved from 45521.91690 to 45204.11174, saving model to best.h5\n",
      "\n",
      "Epoch 00616: val_loss improved from 45204.11174 to 45067.68632, saving model to best.h5\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 45067.68632\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00618: val_loss improved from 45067.68632 to 44946.61742, saving model to best.h5\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 44946.61742\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 44946.61742\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 44946.61742\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 44946.61742\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 44946.61742\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 44946.61742\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 44946.61742\n",
      "\n",
      "Epoch 00626: val_loss improved from 44946.61742 to 44517.99929, saving model to best.h5\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 44517.99929\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 44517.99929\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 44517.99929\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 44517.99929\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 44517.99929\n",
      "\n",
      "Epoch 00632: val_loss improved from 44517.99929 to 44211.41264, saving model to best.h5\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 44211.41264\n",
      "\n",
      "Epoch 00643: val_loss improved from 44211.41264 to 43693.10322, saving model to best.h5\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 43693.10322\n",
      "\n",
      "Epoch 00645: val_loss improved from 43693.10322 to 43508.87831, saving model to best.h5\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 43508.87831\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 43508.87831\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 43508.87831\n",
      "\n",
      "Epoch 00649: val_loss improved from 43508.87831 to 43370.77012, saving model to best.h5\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 43370.77012\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 43370.77012\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 43370.77012\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 43370.77012\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 43370.77012\n",
      "\n",
      "Epoch 00655: val_loss improved from 43370.77012 to 43048.45336, saving model to best.h5\n",
      "\n",
      "Epoch 00656: val_loss improved from 43048.45336 to 42965.15175, saving model to best.h5\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 42965.15175\n",
      "\n",
      "Epoch 00658: val_loss improved from 42965.15175 to 42863.26160, saving model to best.h5\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 42863.26160\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 42863.26160\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 42863.26160\n",
      "\n",
      "Epoch 00662: val_loss improved from 42863.26160 to 42798.78812, saving model to best.h5\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 42798.78812\n",
      "\n",
      "Epoch 00664: val_loss improved from 42798.78812 to 42566.54522, saving model to best.h5\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 42566.54522\n",
      "\n",
      "Epoch 00675: val_loss improved from 42566.54522 to 42198.53965, saving model to best.h5\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 42198.53965\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 42198.53965\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 42198.53965\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 42198.53965\n",
      "\n",
      "Epoch 00680: val_loss improved from 42198.53965 to 41933.98722, saving model to best.h5\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 41933.98722\n",
      "\n",
      "Epoch 00693: val_loss improved from 41933.98722 to 41777.41264, saving model to best.h5\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 41777.41264\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 41777.41264\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 41777.41264\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 41777.41264\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 41777.41264\n",
      "\n",
      "Epoch 00699: val_loss improved from 41777.41264 to 41052.88589, saving model to best.h5\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 41052.88589\n",
      "\n",
      "Epoch 00732: val_loss improved from 41052.88589 to 40239.10062, saving model to best.h5\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 40239.10062\n",
      "\n",
      "Epoch 00754: val_loss improved from 40239.10062 to 39139.33902, saving model to best.h5\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 39139.33902\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 39139.33902\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 39139.33902\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 39139.33902\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 39139.33902\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 39139.33902\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 39139.33902\n",
      "\n",
      "Epoch 00762: val_loss improved from 39139.33902 to 38942.11316, saving model to best.h5\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 38942.11316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00766: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 38942.11316\n",
      "\n",
      "Epoch 00772: val_loss improved from 38942.11316 to 38801.19105, saving model to best.h5\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 38801.19105\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 38801.19105\n",
      "\n",
      "Epoch 00775: val_loss improved from 38801.19105 to 38512.90649, saving model to best.h5\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 38512.90649\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 38512.90649\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 38512.90649\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 38512.90649\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 38512.90649\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 38512.90649\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 38512.90649\n",
      "\n",
      "Epoch 00783: val_loss improved from 38512.90649 to 38352.68572, saving model to best.h5\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 38352.68572\n",
      "\n",
      "Epoch 00812: val_loss improved from 38352.68572 to 37843.50639, saving model to best.h5\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 37843.50639\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 37843.50639\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 37843.50639\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 37843.50639\n",
      "\n",
      "Epoch 00817: val_loss improved from 37843.50639 to 37691.29664, saving model to best.h5\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 37691.29664\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 37691.29664\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 37691.29664\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 37691.29664\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 37691.29664\n",
      "\n",
      "Epoch 00823: val_loss improved from 37691.29664 to 37245.85559, saving model to best.h5\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 37245.85559\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 37245.85559\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 37245.85559\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 37245.85559\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 37245.85559\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 37245.85559\n",
      "\n",
      "Epoch 00830: val_loss improved from 37245.85559 to 37077.48686, saving model to best.h5\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 37077.48686\n",
      "\n",
      "Epoch 00844: val_loss improved from 37077.48686 to 36710.97656, saving model to best.h5\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 36710.97656\n",
      "\n",
      "Epoch 00855: val_loss improved from 36710.97656 to 36517.04534, saving model to best.h5\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 36517.04534\n",
      "\n",
      "Epoch 00857: val_loss improved from 36517.04534 to 36428.03741, saving model to best.h5\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 36428.03741\n",
      "\n",
      "Epoch 00859: val_loss improved from 36428.03741 to 36403.94105, saving model to best.h5\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 36403.94105\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 36403.94105\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 36403.94105\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 36403.94105\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 36403.94105\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 36403.94105\n",
      "\n",
      "Epoch 00866: val_loss improved from 36403.94105 to 36257.79013, saving model to best.h5\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 36257.79013\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 36257.79013\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 36257.79013\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 36257.79013\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 36257.79013\n",
      "\n",
      "Epoch 00872: val_loss improved from 36257.79013 to 36075.36340, saving model to best.h5\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 36075.36340\n",
      "\n",
      "Epoch 00881: val_loss improved from 36075.36340 to 35979.24041, saving model to best.h5\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 35979.24041\n",
      "\n",
      "Epoch 00891: val_loss improved from 35979.24041 to 35805.90187, saving model to best.h5\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 35805.90187\n",
      "\n",
      "Epoch 00906: val_loss improved from 35805.90187 to 35534.25876, saving model to best.h5\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 35534.25876\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 35534.25876\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 35534.25876\n",
      "\n",
      "Epoch 00910: val_loss improved from 35534.25876 to 35356.61352, saving model to best.h5\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 35356.61352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00912: val_loss did not improve from 35356.61352\n",
      "\n",
      "Epoch 00913: val_loss improved from 35356.61352 to 35148.18478, saving model to best.h5\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 35148.18478\n",
      "\n",
      "Epoch 00929: val_loss improved from 35148.18478 to 34771.49053, saving model to best.h5\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 34771.49053\n",
      "\n",
      "Epoch 00946: val_loss improved from 34771.49053 to 34612.57102, saving model to best.h5\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 34612.57102\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 34612.57102\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 34612.57102\n",
      "\n",
      "Epoch 00950: val_loss improved from 34612.57102 to 34291.87701, saving model to best.h5\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 34291.87701\n",
      "\n",
      "Epoch 00965: val_loss improved from 34291.87701 to 33929.64844, saving model to best.h5\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 33929.64844\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 33929.64844\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 33929.64844\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 33929.64844\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 33929.64844\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 33929.64844\n",
      "\n",
      "Epoch 00972: val_loss improved from 33929.64844 to 33805.50071, saving model to best.h5\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 33805.50071\n",
      "\n",
      "Epoch 00996: val_loss improved from 33805.50071 to 33457.90365, saving model to best.h5\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 33457.90365\n",
      "\n",
      "Epoch 01021: val_loss improved from 33457.90365 to 33172.70762, saving model to best.h5\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 33172.70762\n",
      "\n",
      "Epoch 01039: val_loss improved from 33172.70762 to 32794.13707, saving model to best.h5\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 32794.13707\n",
      "\n",
      "Epoch 01051: val_loss improved from 32794.13707 to 32539.42507, saving model to best.h5\n",
      "\n",
      "Epoch 01052: val_loss improved from 32539.42507 to 32362.96248, saving model to best.h5\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 32362.96248\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01054: val_loss did not improve from 32362.96248\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 32362.96248\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 32362.96248\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 32362.96248\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 32362.96248\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 32362.96248\n",
      "\n",
      "Epoch 01060: val_loss improved from 32362.96248 to 32282.98378, saving model to best.h5\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 32282.98378\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 32282.98378\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 32282.98378\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 32282.98378\n",
      "\n",
      "Epoch 01065: val_loss improved from 32282.98378 to 32167.52308, saving model to best.h5\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 32167.52308\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 32167.52308\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 32167.52308\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 32167.52308\n",
      "\n",
      "Epoch 01070: val_loss improved from 32167.52308 to 32060.06061, saving model to best.h5\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 32060.06061\n",
      "\n",
      "Epoch 01085: val_loss improved from 32060.06061 to 31760.47313, saving model to best.h5\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 31760.47313\n",
      "\n",
      "Epoch 01103: val_loss improved from 31760.47313 to 31601.07375, saving model to best.h5\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 31601.07375\n",
      "\n",
      "Epoch 01127: val_loss improved from 31601.07375 to 31574.46046, saving model to best.h5\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 31574.46046\n",
      "\n",
      "Epoch 01141: val_loss improved from 31574.46046 to 31408.43170, saving model to best.h5\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 31408.43170\n",
      "\n",
      "Epoch 01154: val_loss improved from 31408.43170 to 31114.71662, saving model to best.h5\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 31114.71662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01197: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 31114.71662\n",
      "\n",
      "Epoch 01208: val_loss improved from 31114.71662 to 30465.40566, saving model to best.h5\n",
      "\n",
      "Epoch 01209: val_loss improved from 30465.40566 to 29961.85843, saving model to best.h5\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 29961.85843\n",
      "\n",
      "Epoch 01219: val_loss improved from 29961.85843 to 29811.30137, saving model to best.h5\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 29811.30137\n",
      "\n",
      "Epoch 01229: val_loss improved from 29811.30137 to 29718.87488, saving model to best.h5\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 29718.87488\n",
      "\n",
      "Epoch 01231: val_loss improved from 29718.87488 to 29706.04759, saving model to best.h5\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 29706.04759\n",
      "\n",
      "Epoch 01270: val_loss improved from 29706.04759 to 29118.48864, saving model to best.h5\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 29118.48864\n",
      "\n",
      "Epoch 01282: val_loss improved from 29118.48864 to 29104.56061, saving model to best.h5\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 29104.56061\n",
      "\n",
      "Epoch 01345: val_loss improved from 29104.56061 to 26682.45289, saving model to best.h5\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 26682.45289\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01356: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 26682.45289\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01508: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01514: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01529: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 26682.45289\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 26682.45289\n",
      "Epoch 01545: early stopping\n",
      "27\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 88014479360.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 88014479360.00000\n",
      "\n",
      "Epoch 00003: val_loss improved from 88014479360.00000 to 22677032960.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 22677032960.00000 to 15506464768.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 15506464768.00000 to 14178744320.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 14178744320.00000 to 2336097792.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2336097792.00000\n",
      "\n",
      "Epoch 00008: val_loss improved from 2336097792.00000 to 43768564.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 43768564.00000\n",
      "\n",
      "Epoch 00017: val_loss improved from 43768564.00000 to 13285052.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 13285052.00000\n",
      "\n",
      "Epoch 00027: val_loss improved from 13285052.00000 to 2030519.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2030519.75000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2030519.75000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2030519.75000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2030519.75000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2030519.75000\n",
      "\n",
      "Epoch 00033: val_loss improved from 2030519.75000 to 1172388.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1172388.25000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1172388.25000\n",
      "\n",
      "Epoch 00036: val_loss improved from 1172388.25000 to 437309.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 437309.56250\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 437309.56250\n",
      "\n",
      "Epoch 00039: val_loss improved from 437309.56250 to 245452.48438, saving model to best.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 245452.48438\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 245452.48438\n",
      "\n",
      "Epoch 00042: val_loss improved from 245452.48438 to 191805.70312, saving model to best.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 191805.70312\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 191805.70312\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 191805.70312\n",
      "\n",
      "Epoch 00046: val_loss improved from 191805.70312 to 69419.21875, saving model to best.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 69419.21875\n",
      "\n",
      "Epoch 00056: val_loss improved from 69419.21875 to 53823.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 53823.87500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 53823.87500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 53823.87500\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 53823.87500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 53823.87500\n",
      "\n",
      "Epoch 00062: val_loss improved from 53823.87500 to 51291.07812, saving model to best.h5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 51291.07812\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 51291.07812\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 51291.07812\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 51291.07812\n",
      "\n",
      "Epoch 00067: val_loss improved from 51291.07812 to 49038.73438, saving model to best.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 49038.73438 to 47397.66016, saving model to best.h5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 47397.66016\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 47397.66016\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 47397.66016\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 47397.66016\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 47397.66016\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 47397.66016\n",
      "\n",
      "Epoch 00075: val_loss improved from 47397.66016 to 46655.76172, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 46655.76172\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 46655.76172\n",
      "\n",
      "Epoch 00078: val_loss improved from 46655.76172 to 43074.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 43074.10156\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 43074.10156\n",
      "\n",
      "Epoch 00081: val_loss improved from 43074.10156 to 41349.58984, saving model to best.h5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 41349.58984\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 41349.58984\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 41349.58984\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 41349.58984\n",
      "\n",
      "Epoch 00086: val_loss improved from 41349.58984 to 39445.95703, saving model to best.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 39445.95703\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 39445.95703\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 39445.95703\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 39445.95703\n",
      "\n",
      "Epoch 00091: val_loss improved from 39445.95703 to 37926.39062, saving model to best.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 37926.39062\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 37926.39062\n",
      "\n",
      "Epoch 00094: val_loss improved from 37926.39062 to 36370.59766, saving model to best.h5\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 36370.59766\n",
      "\n",
      "Epoch 00096: val_loss improved from 36370.59766 to 35488.93359, saving model to best.h5\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 35488.93359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00098: val_loss did not improve from 35488.93359\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 35488.93359\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 35488.93359\n",
      "\n",
      "Epoch 00101: val_loss improved from 35488.93359 to 33948.45703, saving model to best.h5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 33948.45703\n",
      "\n",
      "Epoch 00103: val_loss improved from 33948.45703 to 33305.21875, saving model to best.h5\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 33305.21875\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 33305.21875\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 33305.21875\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 33305.21875\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 33305.21875\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 33305.21875\n",
      "\n",
      "Epoch 00110: val_loss improved from 33305.21875 to 31947.35742, saving model to best.h5\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 31947.35742\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 31947.35742\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 31947.35742\n",
      "\n",
      "Epoch 00114: val_loss improved from 31947.35742 to 31240.41602, saving model to best.h5\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 31240.41602\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 31240.41602\n",
      "\n",
      "Epoch 00117: val_loss improved from 31240.41602 to 29176.21484, saving model to best.h5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 29176.21484\n",
      "\n",
      "Epoch 00127: val_loss improved from 29176.21484 to 27067.46484, saving model to best.h5\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 27067.46484\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 27067.46484\n",
      "\n",
      "Epoch 00130: val_loss improved from 27067.46484 to 26954.18164, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 26954.18164\n",
      "\n",
      "Epoch 00132: val_loss improved from 26954.18164 to 26234.67188, saving model to best.h5\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 26234.67188\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 26234.67188\n",
      "\n",
      "Epoch 00135: val_loss improved from 26234.67188 to 25470.57812, saving model to best.h5\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 25470.57812\n",
      "\n",
      "Epoch 00145: val_loss improved from 25470.57812 to 24105.89648, saving model to best.h5\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 24105.89648\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 24105.89648\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 24105.89648\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 24105.89648\n",
      "\n",
      "Epoch 00150: val_loss improved from 24105.89648 to 23723.10742, saving model to best.h5\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 23723.10742\n",
      "\n",
      "Epoch 00160: val_loss improved from 23723.10742 to 22761.65820, saving model to best.h5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 22761.65820\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 22761.65820\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 22761.65820\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 22761.65820\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 22761.65820\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 22761.65820\n",
      "\n",
      "Epoch 00167: val_loss improved from 22761.65820 to 22387.43555, saving model to best.h5\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 22387.43555\n",
      "\n",
      "Epoch 00177: val_loss improved from 22387.43555 to 22106.71680, saving model to best.h5\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 22106.71680\n",
      "\n",
      "Epoch 00197: val_loss improved from 22106.71680 to 21837.34570, saving model to best.h5\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 21837.34570\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00278: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 21837.34570\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 21837.34570\n",
      "Epoch 00397: early stopping\n",
      "28\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 182039494656.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 182039494656.00000\n",
      "\n",
      "Epoch 00003: val_loss improved from 182039494656.00000 to 50500841472.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 50500841472.00000 to 36601671680.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 36601671680.00000 to 28024971264.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 28024971264.00000 to 4641900544.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4641900544.00000\n",
      "\n",
      "Epoch 00008: val_loss improved from 4641900544.00000 to 305966464.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 305966464.00000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 305966464.00000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 305966464.00000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 305966464.00000\n",
      "\n",
      "Epoch 00013: val_loss improved from 305966464.00000 to 26400518.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 26400518.00000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 26400518.00000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 26400518.00000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 26400518.00000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 26400518.00000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 26400518.00000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 26400518.00000\n",
      "\n",
      "Epoch 00021: val_loss improved from 26400518.00000 to 12296797.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 12296797.00000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 12296797.00000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 12296797.00000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 12296797.00000\n",
      "\n",
      "Epoch 00026: val_loss improved from 12296797.00000 to 2636126.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2636126.25000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2636126.25000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2636126.25000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2636126.25000\n",
      "\n",
      "Epoch 00031: val_loss improved from 2636126.25000 to 2233346.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2233346.00000\n",
      "\n",
      "Epoch 00033: val_loss improved from 2233346.00000 to 1205456.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1205456.37500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1205456.37500\n",
      "\n",
      "Epoch 00036: val_loss improved from 1205456.37500 to 965700.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 965700.00000\n",
      "\n",
      "Epoch 00038: val_loss improved from 965700.00000 to 578025.37500, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00039: val_loss did not improve from 578025.37500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 578025.37500\n",
      "\n",
      "Epoch 00041: val_loss improved from 578025.37500 to 252348.01562, saving model to best.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 252348.01562\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 252348.01562\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 252348.01562\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 252348.01562\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 252348.01562\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 252348.01562\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 252348.01562\n",
      "\n",
      "Epoch 00049: val_loss improved from 252348.01562 to 214637.46875, saving model to best.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 214637.46875\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 214637.46875\n",
      "\n",
      "Epoch 00052: val_loss improved from 214637.46875 to 192799.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 192799.37500\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 192799.37500\n",
      "\n",
      "Epoch 00055: val_loss improved from 192799.37500 to 189013.67188, saving model to best.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 189013.67188\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 189013.67188\n",
      "\n",
      "Epoch 00058: val_loss improved from 189013.67188 to 183969.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 183969.96875\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 183969.96875\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 183969.96875\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 183969.96875\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 183969.96875\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 183969.96875\n",
      "\n",
      "Epoch 00065: val_loss improved from 183969.96875 to 166390.07812, saving model to best.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 166390.07812\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 166390.07812\n",
      "\n",
      "Epoch 00068: val_loss improved from 166390.07812 to 161360.68750, saving model to best.h5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 161360.68750\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 161360.68750\n",
      "\n",
      "Epoch 00071: val_loss improved from 161360.68750 to 159590.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00072: val_loss improved from 159590.62500 to 154594.04688, saving model to best.h5\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 154594.04688\n",
      "\n",
      "Epoch 00074: val_loss improved from 154594.04688 to 148583.85938, saving model to best.h5\n",
      "\n",
      "Epoch 00075: val_loss improved from 148583.85938 to 147655.57812, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 147655.57812\n",
      "\n",
      "Epoch 00077: val_loss improved from 147655.57812 to 143607.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00078: val_loss improved from 143607.12500 to 142055.09375, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 142055.09375\n",
      "\n",
      "Epoch 00080: val_loss improved from 142055.09375 to 139131.95312, saving model to best.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 139131.95312\n",
      "\n",
      "Epoch 00082: val_loss improved from 139131.95312 to 138031.14062, saving model to best.h5\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 138031.14062\n",
      "\n",
      "Epoch 00084: val_loss improved from 138031.14062 to 132627.21875, saving model to best.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 132627.21875 to 130761.89062, saving model to best.h5\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 130761.89062\n",
      "\n",
      "Epoch 00087: val_loss improved from 130761.89062 to 127951.69531, saving model to best.h5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 127951.69531\n",
      "\n",
      "Epoch 00089: val_loss improved from 127951.69531 to 125029.39062, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 125029.39062\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 125029.39062\n",
      "\n",
      "Epoch 00092: val_loss improved from 125029.39062 to 121351.59375, saving model to best.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 121351.59375\n",
      "\n",
      "Epoch 00094: val_loss improved from 121351.59375 to 119259.74219, saving model to best.h5\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 119259.74219\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 119259.74219\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 119259.74219\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 119259.74219\n",
      "\n",
      "Epoch 00099: val_loss improved from 119259.74219 to 112621.85938, saving model to best.h5\n",
      "\n",
      "Epoch 00100: val_loss improved from 112621.85938 to 111753.24219, saving model to best.h5\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 111753.24219\n",
      "\n",
      "Epoch 00102: val_loss improved from 111753.24219 to 110084.57031, saving model to best.h5\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 110084.57031\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 110084.57031\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 110084.57031\n",
      "\n",
      "Epoch 00106: val_loss improved from 110084.57031 to 106002.83594, saving model to best.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 106002.83594 to 105147.92188, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 105147.92188\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 105147.92188\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 105147.92188\n",
      "\n",
      "Epoch 00111: val_loss improved from 105147.92188 to 103668.54688, saving model to best.h5\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 103668.54688\n",
      "\n",
      "Epoch 00113: val_loss improved from 103668.54688 to 101080.65625, saving model to best.h5\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 101080.65625\n",
      "\n",
      "Epoch 00115: val_loss improved from 101080.65625 to 98057.32812, saving model to best.h5\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 98057.32812\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 98057.32812\n",
      "\n",
      "Epoch 00118: val_loss improved from 98057.32812 to 96221.52344, saving model to best.h5\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 96221.52344\n",
      "\n",
      "Epoch 00120: val_loss improved from 96221.52344 to 94437.01562, saving model to best.h5\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 94437.01562\n",
      "\n",
      "Epoch 00122: val_loss improved from 94437.01562 to 94189.98438, saving model to best.h5\n",
      "\n",
      "Epoch 00123: val_loss improved from 94189.98438 to 93412.83594, saving model to best.h5\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 93412.83594\n",
      "\n",
      "Epoch 00125: val_loss improved from 93412.83594 to 91178.26562, saving model to best.h5\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 91178.26562\n",
      "\n",
      "Epoch 00127: val_loss improved from 91178.26562 to 90379.99219, saving model to best.h5\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 90379.99219\n",
      "\n",
      "Epoch 00129: val_loss improved from 90379.99219 to 90249.75781, saving model to best.h5\n",
      "\n",
      "Epoch 00130: val_loss improved from 90249.75781 to 88529.35156, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 88529.35156\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 88529.35156\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 88529.35156\n",
      "\n",
      "Epoch 00134: val_loss improved from 88529.35156 to 86564.01562, saving model to best.h5\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 86564.01562\n",
      "\n",
      "Epoch 00136: val_loss improved from 86564.01562 to 86254.57812, saving model to best.h5\n",
      "\n",
      "Epoch 00137: val_loss improved from 86254.57812 to 85301.65625, saving model to best.h5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 85301.65625\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 85301.65625\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 85301.65625\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 85301.65625\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 85301.65625\n",
      "\n",
      "Epoch 00143: val_loss improved from 85301.65625 to 84771.83594, saving model to best.h5\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 84771.83594\n",
      "\n",
      "Epoch 00145: val_loss improved from 84771.83594 to 81957.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 81957.25000\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 81957.25000\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 81957.25000\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 81957.25000\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 81957.25000\n",
      "\n",
      "Epoch 00151: val_loss improved from 81957.25000 to 79960.94531, saving model to best.h5\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 79960.94531\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 79960.94531\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 79960.94531\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 79960.94531\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 79960.94531\n",
      "\n",
      "Epoch 00157: val_loss improved from 79960.94531 to 77255.54688, saving model to best.h5\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 77255.54688\n",
      "\n",
      "Epoch 00159: val_loss improved from 77255.54688 to 76561.53906, saving model to best.h5\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 76561.53906\n",
      "\n",
      "Epoch 00161: val_loss improved from 76561.53906 to 75913.45312, saving model to best.h5\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 75913.45312\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 75913.45312\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 75913.45312\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 75913.45312\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 75913.45312\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 75913.45312\n",
      "\n",
      "Epoch 00168: val_loss improved from 75913.45312 to 74199.50781, saving model to best.h5\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 74199.50781\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 74199.50781\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 74199.50781\n",
      "\n",
      "Epoch 00172: val_loss improved from 74199.50781 to 73041.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 73041.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00177: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 73041.75000\n",
      "\n",
      "Epoch 00204: val_loss improved from 73041.75000 to 66844.26562, saving model to best.h5\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 66844.26562\n",
      "\n",
      "Epoch 00206: val_loss improved from 66844.26562 to 66486.82031, saving model to best.h5\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 66486.82031\n",
      "\n",
      "Epoch 00208: val_loss improved from 66486.82031 to 66083.34375, saving model to best.h5\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 66083.34375\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 66083.34375\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 66083.34375\n",
      "\n",
      "Epoch 00212: val_loss improved from 66083.34375 to 65754.66406, saving model to best.h5\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 65754.66406\n",
      "\n",
      "Epoch 00214: val_loss improved from 65754.66406 to 64674.41797, saving model to best.h5\n",
      "\n",
      "Epoch 00215: val_loss improved from 64674.41797 to 64614.58594, saving model to best.h5\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 64614.58594\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 64614.58594\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 64614.58594\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 64614.58594\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 64614.58594\n",
      "\n",
      "Epoch 00221: val_loss improved from 64614.58594 to 64043.83594, saving model to best.h5\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 64043.83594\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 64043.83594\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 64043.83594\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 64043.83594\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 64043.83594\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 64043.83594\n",
      "\n",
      "Epoch 00228: val_loss improved from 64043.83594 to 63358.98828, saving model to best.h5\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 63358.98828\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 63358.98828\n",
      "\n",
      "Epoch 00231: val_loss improved from 63358.98828 to 63250.47656, saving model to best.h5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 63250.47656\n",
      "\n",
      "Epoch 00252: val_loss improved from 63250.47656 to 62134.74609, saving model to best.h5\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 62134.74609\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 62134.74609\n",
      "\n",
      "Epoch 00255: val_loss improved from 62134.74609 to 60690.91406, saving model to best.h5\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 60690.91406\n",
      "\n",
      "Epoch 00257: val_loss improved from 60690.91406 to 60364.92969, saving model to best.h5\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 60364.92969\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 60364.92969\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 60364.92969\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 60364.92969\n",
      "\n",
      "Epoch 00262: val_loss improved from 60364.92969 to 60313.03125, saving model to best.h5\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 60313.03125\n",
      "\n",
      "Epoch 00285: val_loss improved from 60313.03125 to 60106.03125, saving model to best.h5\n",
      "\n",
      "Epoch 00286: val_loss improved from 60106.03125 to 59437.57031, saving model to best.h5\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 59437.57031\n",
      "\n",
      "Epoch 00296: val_loss improved from 59437.57031 to 58850.60156, saving model to best.h5\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 58850.60156\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 58850.60156\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 58850.60156\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 58850.60156\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 58850.60156\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 58850.60156\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 58850.60156\n",
      "\n",
      "Epoch 00304: val_loss improved from 58850.60156 to 58474.07422, saving model to best.h5\n",
      "\n",
      "Epoch 00305: val_loss improved from 58474.07422 to 58080.50781, saving model to best.h5\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 58080.50781\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 58080.50781\n",
      "\n",
      "Epoch 00308: val_loss improved from 58080.50781 to 57803.90234, saving model to best.h5\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 57803.90234\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 57803.90234\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 57803.90234\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 57803.90234\n",
      "\n",
      "Epoch 00313: val_loss improved from 57803.90234 to 57586.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 57586.18750\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00332: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 57586.18750\n",
      "\n",
      "Epoch 00362: val_loss improved from 57586.18750 to 56822.10938, saving model to best.h5\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 56822.10938\n",
      "\n",
      "Epoch 00407: val_loss improved from 56822.10938 to 55697.59375, saving model to best.h5\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 55697.59375\n",
      "\n",
      "Epoch 00467: val_loss improved from 55697.59375 to 55406.66406, saving model to best.h5\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 55406.66406\n",
      "\n",
      "Epoch 00469: val_loss improved from 55406.66406 to 54526.47656, saving model to best.h5\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 54526.47656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00479: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 54526.47656\n",
      "\n",
      "Epoch 00492: val_loss improved from 54526.47656 to 53963.91406, saving model to best.h5\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 53963.91406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00634: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 53963.91406\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 53963.91406\n",
      "Epoch 00692: early stopping\n",
      "29\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 72224686080.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 72224686080.00000\n",
      "\n",
      "Epoch 00003: val_loss improved from 72224686080.00000 to 24224292864.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 24224292864.00000\n",
      "\n",
      "Epoch 00005: val_loss improved from 24224292864.00000 to 18258925568.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 18258925568.00000 to 7676276736.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 7676276736.00000\n",
      "\n",
      "Epoch 00008: val_loss improved from 7676276736.00000 to 2463133440.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2463133440.00000\n",
      "\n",
      "Epoch 00010: val_loss improved from 2463133440.00000 to 464656384.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 464656384.00000\n",
      "\n",
      "Epoch 00012: val_loss improved from 464656384.00000 to 111603.46875, saving model to best.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 111603.46875\n",
      "\n",
      "Epoch 00055: val_loss improved from 111603.46875 to 46992.41797, saving model to best.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 46992.41797\n",
      "\n",
      "Epoch 00057: val_loss improved from 46992.41797 to 34654.21875, saving model to best.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 34654.21875\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 34654.21875\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 34654.21875\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 34654.21875\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 34654.21875\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 34654.21875\n",
      "\n",
      "Epoch 00064: val_loss improved from 34654.21875 to 31586.63281, saving model to best.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 31586.63281\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 31586.63281\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 31586.63281\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 31586.63281\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 31586.63281\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 31586.63281\n",
      "\n",
      "Epoch 00071: val_loss improved from 31586.63281 to 31492.37305, saving model to best.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 31492.37305\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 31492.37305\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 31492.37305\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 31492.37305\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 31492.37305\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 31492.37305\n",
      "\n",
      "Epoch 00078: val_loss improved from 31492.37305 to 31100.90625, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 31100.90625\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 31100.90625\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 31100.90625\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 31100.90625\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 31100.90625\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 31100.90625\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 31100.90625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00086: val_loss improved from 31100.90625 to 30841.41211, saving model to best.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 30841.41211\n",
      "\n",
      "Epoch 00088: val_loss improved from 30841.41211 to 30647.70312, saving model to best.h5\n",
      "\n",
      "Epoch 00089: val_loss improved from 30647.70312 to 30644.72070, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 30644.72070\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 30644.72070\n",
      "\n",
      "Epoch 00092: val_loss improved from 30644.72070 to 30529.02344, saving model to best.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 30529.02344\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 30529.02344\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 30529.02344\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 30529.02344\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 30529.02344\n",
      "\n",
      "Epoch 00098: val_loss improved from 30529.02344 to 30352.97656, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 30352.97656\n",
      "\n",
      "Epoch 00100: val_loss improved from 30352.97656 to 30311.17188, saving model to best.h5\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 30311.17188\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 30311.17188\n",
      "\n",
      "Epoch 00103: val_loss improved from 30311.17188 to 30304.99609, saving model to best.h5\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 30304.99609\n",
      "\n",
      "Epoch 00105: val_loss improved from 30304.99609 to 30233.72656, saving model to best.h5\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 30233.72656\n",
      "\n",
      "Epoch 00107: val_loss improved from 30233.72656 to 30147.51758, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 30147.51758\n",
      "\n",
      "Epoch 00109: val_loss improved from 30147.51758 to 30136.91602, saving model to best.h5\n",
      "\n",
      "Epoch 00110: val_loss improved from 30136.91602 to 30081.55664, saving model to best.h5\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 30081.55664\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 30081.55664\n",
      "\n",
      "Epoch 00113: val_loss improved from 30081.55664 to 29974.82031, saving model to best.h5\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 29974.82031\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 29974.82031\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 29974.82031\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 29974.82031\n",
      "\n",
      "Epoch 00118: val_loss improved from 29974.82031 to 29854.24219, saving model to best.h5\n",
      "\n",
      "Epoch 00119: val_loss improved from 29854.24219 to 29837.99805, saving model to best.h5\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 29837.99805\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 29837.99805\n",
      "\n",
      "Epoch 00122: val_loss improved from 29837.99805 to 29792.00977, saving model to best.h5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 29792.00977\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 29792.00977\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 29792.00977\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 29792.00977\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 29792.00977\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 29792.00977\n",
      "\n",
      "Epoch 00129: val_loss improved from 29792.00977 to 29687.15820, saving model to best.h5\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 29687.15820\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 29687.15820\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 29687.15820\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 29687.15820\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 29687.15820\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 29687.15820\n",
      "\n",
      "Epoch 00136: val_loss improved from 29687.15820 to 29640.89453, saving model to best.h5\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 29640.89453\n",
      "\n",
      "Epoch 00138: val_loss improved from 29640.89453 to 29589.64258, saving model to best.h5\n",
      "\n",
      "Epoch 00139: val_loss improved from 29589.64258 to 29568.74805, saving model to best.h5\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 29568.74805\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 29568.74805\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 29568.74805\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 29568.74805\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 29568.74805\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 29568.74805\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 29568.74805\n",
      "\n",
      "Epoch 00147: val_loss improved from 29568.74805 to 29483.05078, saving model to best.h5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 29483.05078\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 29483.05078\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 29483.05078\n",
      "\n",
      "Epoch 00151: val_loss improved from 29483.05078 to 29475.99805, saving model to best.h5\n",
      "\n",
      "Epoch 00152: val_loss improved from 29475.99805 to 29474.19531, saving model to best.h5\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 29474.19531\n",
      "\n",
      "Epoch 00164: val_loss improved from 29474.19531 to 29344.25195, saving model to best.h5\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 29344.25195\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 29344.25195\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 29344.25195\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 29344.25195\n",
      "\n",
      "Epoch 00169: val_loss improved from 29344.25195 to 29239.71289, saving model to best.h5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 29239.71289\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 29239.71289\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 29239.71289\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 29239.71289\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 29239.71289\n",
      "\n",
      "Epoch 00175: val_loss improved from 29239.71289 to 29209.84766, saving model to best.h5\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 29209.84766\n",
      "\n",
      "Epoch 00196: val_loss improved from 29209.84766 to 29110.21680, saving model to best.h5\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 29110.21680\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 29110.21680\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 29110.21680\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 29110.21680\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 29110.21680\n",
      "\n",
      "Epoch 00202: val_loss improved from 29110.21680 to 28955.49219, saving model to best.h5\n",
      "\n",
      "Epoch 00203: val_loss improved from 28955.49219 to 28894.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 28894.10156\n",
      "\n",
      "Epoch 00216: val_loss improved from 28894.10156 to 28735.90039, saving model to best.h5\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 28735.90039\n",
      "\n",
      "Epoch 00228: val_loss improved from 28735.90039 to 28733.26367, saving model to best.h5\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 28733.26367\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 28733.26367\n",
      "\n",
      "Epoch 00231: val_loss improved from 28733.26367 to 28651.84570, saving model to best.h5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 28651.84570\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00233: val_loss improved from 28651.84570 to 28600.18945, saving model to best.h5\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 28600.18945\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 28600.18945\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 28600.18945\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 28600.18945\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 28600.18945\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 28600.18945\n",
      "\n",
      "Epoch 00240: val_loss improved from 28600.18945 to 28505.52539, saving model to best.h5\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 28505.52539\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 28505.52539\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 28505.52539\n",
      "\n",
      "Epoch 00244: val_loss improved from 28505.52539 to 28414.07812, saving model to best.h5\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 28414.07812\n",
      "\n",
      "Epoch 00297: val_loss improved from 28414.07812 to 28089.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00298: val_loss improved from 28089.56250 to 27790.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 27790.81250\n",
      "\n",
      "Epoch 00307: val_loss improved from 27790.81250 to 27756.22656, saving model to best.h5\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 27756.22656\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 27756.22656\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 27756.22656\n",
      "\n",
      "Epoch 00311: val_loss improved from 27756.22656 to 27658.47461, saving model to best.h5\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 27658.47461\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 27658.47461\n",
      "\n",
      "Epoch 00314: val_loss improved from 27658.47461 to 27622.34180, saving model to best.h5\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 27622.34180\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 27622.34180\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 27622.34180\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 27622.34180\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 27622.34180\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 27622.34180\n",
      "\n",
      "Epoch 00321: val_loss improved from 27622.34180 to 27560.99023, saving model to best.h5\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 27560.99023\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 27560.99023\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 27560.99023\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 27560.99023\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 27560.99023\n",
      "\n",
      "Epoch 00327: val_loss improved from 27560.99023 to 27498.40039, saving model to best.h5\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 27498.40039\n",
      "\n",
      "Epoch 00354: val_loss improved from 27498.40039 to 27200.51367, saving model to best.h5\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 27200.51367\n",
      "\n",
      "Epoch 00376: val_loss improved from 27200.51367 to 27083.28125, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00377: val_loss did not improve from 27083.28125\n",
      "\n",
      "Epoch 00378: val_loss improved from 27083.28125 to 26944.11328, saving model to best.h5\n",
      "\n",
      "Epoch 00379: val_loss improved from 26944.11328 to 26928.37695, saving model to best.h5\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 26928.37695\n",
      "\n",
      "Epoch 00422: val_loss improved from 26928.37695 to 26597.04688, saving model to best.h5\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 26597.04688\n",
      "\n",
      "Epoch 00443: val_loss improved from 26597.04688 to 26425.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 26425.53125\n",
      "\n",
      "Epoch 00460: val_loss improved from 26425.53125 to 26339.11719, saving model to best.h5\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 26339.11719\n",
      "\n",
      "Epoch 00476: val_loss improved from 26339.11719 to 26060.76367, saving model to best.h5\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 26060.76367\n",
      "\n",
      "Epoch 00485: val_loss improved from 26060.76367 to 25900.97266, saving model to best.h5\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 25900.97266\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 25900.97266\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 25900.97266\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 25900.97266\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 25900.97266\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 25900.97266\n",
      "\n",
      "Epoch 00492: val_loss improved from 25900.97266 to 25887.55664, saving model to best.h5\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 25887.55664\n",
      "\n",
      "Epoch 00518: val_loss improved from 25887.55664 to 25710.99609, saving model to best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00519: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 25710.99609\n",
      "\n",
      "Epoch 00528: val_loss improved from 25710.99609 to 25417.41211, saving model to best.h5\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 25417.41211\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 25417.41211\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 25417.41211\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 25417.41211\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 25417.41211\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 25417.41211\n",
      "\n",
      "Epoch 00535: val_loss improved from 25417.41211 to 25360.22656, saving model to best.h5\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 25360.22656\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 25360.22656\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 25360.22656\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 25360.22656\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 25360.22656\n",
      "\n",
      "Epoch 00541: val_loss improved from 25360.22656 to 25326.28711, saving model to best.h5\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 25326.28711\n",
      "\n",
      "Epoch 00583: val_loss improved from 25326.28711 to 24881.75781, saving model to best.h5\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 24881.75781\n",
      "\n",
      "Epoch 00659: val_loss improved from 24881.75781 to 24265.26367, saving model to best.h5\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 24265.26367\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00671: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 24265.26367\n",
      "\n",
      "Epoch 00680: val_loss improved from 24265.26367 to 23993.97266, saving model to best.h5\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 23993.97266\n",
      "\n",
      "Epoch 00691: val_loss improved from 23993.97266 to 23943.57617, saving model to best.h5\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 23943.57617\n",
      "\n",
      "Epoch 00705: val_loss improved from 23943.57617 to 23755.54297, saving model to best.h5\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 23755.54297\n",
      "\n",
      "Epoch 00719: val_loss improved from 23755.54297 to 23625.14453, saving model to best.h5\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 23625.14453\n",
      "\n",
      "Epoch 00787: val_loss improved from 23625.14453 to 23301.65820, saving model to best.h5\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 23301.65820\n",
      "\n",
      "Epoch 00802: val_loss improved from 23301.65820 to 22933.40625, saving model to best.h5\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 22933.40625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00821: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 22933.40625\n",
      "\n",
      "Epoch 00850: val_loss improved from 22933.40625 to 22071.18945, saving model to best.h5\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 22071.18945\n",
      "\n",
      "Epoch 00899: val_loss improved from 22071.18945 to 20677.83984, saving model to best.h5\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 20677.83984\n",
      "\n",
      "Epoch 00930: val_loss improved from 20677.83984 to 20325.37109, saving model to best.h5\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 20325.37109\n",
      "\n",
      "Epoch 00949: val_loss improved from 20325.37109 to 19924.36328, saving model to best.h5\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 19924.36328\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00971: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 19924.36328\n",
      "\n",
      "Epoch 01011: val_loss improved from 19924.36328 to 19244.95703, saving model to best.h5\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 19244.95703\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01131: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 19244.95703\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 19244.95703\n",
      "Epoch 01211: early stopping\n",
      "30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 96419635200.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 96419635200.00000\n",
      "\n",
      "Epoch 00003: val_loss improved from 96419635200.00000 to 15483658240.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 15483658240.00000\n",
      "\n",
      "Epoch 00005: val_loss improved from 15483658240.00000 to 8735595520.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 8735595520.00000 to 4166356480.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4166356480.00000\n",
      "\n",
      "Epoch 00008: val_loss improved from 4166356480.00000 to 625987008.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 625987008.00000\n",
      "\n",
      "Epoch 00010: val_loss improved from 625987008.00000 to 1995114.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1995114.25000\n",
      "\n",
      "Epoch 00020: val_loss improved from 1995114.25000 to 1395277.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1395277.62500\n",
      "\n",
      "Epoch 00029: val_loss improved from 1395277.62500 to 986476.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 986476.18750 to 579617.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 579617.12500 to 252149.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 252149.81250 to 163028.89062, saving model to best.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 163028.89062 to 117018.73438, saving model to best.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 117018.73438\n",
      "\n",
      "Epoch 00035: val_loss improved from 117018.73438 to 89945.39062, saving model to best.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 89945.39062 to 80790.94531, saving model to best.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 80790.94531\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 80790.94531\n",
      "\n",
      "Epoch 00039: val_loss improved from 80790.94531 to 69791.89844, saving model to best.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 69791.89844\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 69791.89844\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 69791.89844\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 69791.89844\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 69791.89844\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 69791.89844\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 69791.89844\n",
      "\n",
      "Epoch 00047: val_loss improved from 69791.89844 to 62091.49219, saving model to best.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 62091.49219 to 54522.82031, saving model to best.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 54522.82031\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00061: val_loss did not improve from 54522.82031\n",
      "\n",
      "Epoch 00062: val_loss improved from 54522.82031 to 52385.60547, saving model to best.h5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 52385.60547\n",
      "\n",
      "Epoch 00074: val_loss improved from 52385.60547 to 51963.37891, saving model to best.h5\n",
      "\n",
      "Epoch 00075: val_loss improved from 51963.37891 to 50522.15625, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 50522.15625\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 50522.15625\n",
      "\n",
      "Epoch 00078: val_loss improved from 50522.15625 to 50027.17578, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 50027.17578\n",
      "\n",
      "Epoch 00100: val_loss improved from 50027.17578 to 47780.61328, saving model to best.h5\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 47780.61328\n",
      "\n",
      "Epoch 00109: val_loss improved from 47780.61328 to 47139.69922, saving model to best.h5\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 47139.69922\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 47139.69922\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 47139.69922\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 47139.69922\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 47139.69922\n",
      "\n",
      "Epoch 00115: val_loss improved from 47139.69922 to 45090.26953, saving model to best.h5\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 45090.26953\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 45090.26953\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 45090.26953\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 45090.26953\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 45090.26953\n",
      "\n",
      "Epoch 00121: val_loss improved from 45090.26953 to 43869.39062, saving model to best.h5\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 43869.39062\n",
      "\n",
      "Epoch 00130: val_loss improved from 43869.39062 to 42377.01172, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss improved from 42377.01172 to 41080.67969, saving model to best.h5\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 41080.67969\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 41080.67969\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 41080.67969\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 41080.67969\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 41080.67969\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 41080.67969\n",
      "\n",
      "Epoch 00138: val_loss improved from 41080.67969 to 38841.68359, saving model to best.h5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 38841.68359\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 38841.68359\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 38841.68359\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 38841.68359\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 38841.68359\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 38841.68359\n",
      "\n",
      "Epoch 00145: val_loss improved from 38841.68359 to 36084.81641, saving model to best.h5\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 36084.81641\n",
      "\n",
      "Epoch 00158: val_loss improved from 36084.81641 to 31427.85352, saving model to best.h5\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 31427.85352\n",
      "\n",
      "Epoch 00169: val_loss improved from 31427.85352 to 29937.84961, saving model to best.h5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 29937.84961\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 29937.84961\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 29937.84961\n",
      "\n",
      "Epoch 00173: val_loss improved from 29937.84961 to 29543.20898, saving model to best.h5\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 29543.20898\n",
      "\n",
      "Epoch 00187: val_loss improved from 29543.20898 to 28149.13086, saving model to best.h5\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 28149.13086\n",
      "\n",
      "Epoch 00199: val_loss improved from 28149.13086 to 28060.95508, saving model to best.h5\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 28060.95508\n",
      "\n",
      "Epoch 00208: val_loss improved from 28060.95508 to 27262.77344, saving model to best.h5\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 27262.77344\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00210: val_loss improved from 27262.77344 to 27253.67383, saving model to best.h5\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 27253.67383\n",
      "\n",
      "Epoch 00224: val_loss improved from 27253.67383 to 26761.70508, saving model to best.h5\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 26761.70508\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 26761.70508\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 26761.70508\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 26761.70508\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 26761.70508\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 26761.70508\n",
      "\n",
      "Epoch 00231: val_loss improved from 26761.70508 to 26537.00195, saving model to best.h5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 26537.00195\n",
      "\n",
      "Epoch 00248: val_loss improved from 26537.00195 to 26145.08008, saving model to best.h5\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 26145.08008\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 26145.08008\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 26145.08008\n",
      "\n",
      "Epoch 00252: val_loss improved from 26145.08008 to 25825.86133, saving model to best.h5\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 25825.86133\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 25825.86133\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 25825.86133\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 25825.86133\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 25825.86133\n",
      "\n",
      "Epoch 00258: val_loss improved from 25825.86133 to 25806.25195, saving model to best.h5\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 25806.25195\n",
      "\n",
      "Epoch 00300: val_loss improved from 25806.25195 to 25729.60938, saving model to best.h5\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 25729.60938\n",
      "\n",
      "Epoch 00311: val_loss improved from 25729.60938 to 25716.40430, saving model to best.h5\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 25716.40430\n",
      "\n",
      "Epoch 00326: val_loss improved from 25716.40430 to 25299.45508, saving model to best.h5\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 25299.45508\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00353: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 25299.45508\n",
      "\n",
      "Epoch 00373: val_loss improved from 25299.45508 to 25058.85352, saving model to best.h5\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 25058.85352\n",
      "\n",
      "Epoch 00406: val_loss improved from 25058.85352 to 25026.80078, saving model to best.h5\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 25026.80078\n",
      "\n",
      "Epoch 00418: val_loss improved from 25026.80078 to 24776.88086, saving model to best.h5\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 24776.88086\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 24776.88086\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 24776.88086\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 24776.88086\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 24776.88086\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 24776.88086\n",
      "\n",
      "Epoch 00425: val_loss improved from 24776.88086 to 24704.46680, saving model to best.h5\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 24704.46680\n",
      "\n",
      "Epoch 00453: val_loss improved from 24704.46680 to 24653.42773, saving model to best.h5\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 24653.42773\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00505: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 24653.42773\n",
      "\n",
      "Epoch 00559: val_loss improved from 24653.42773 to 23279.27734, saving model to best.h5\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 23279.27734\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00652: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 23279.27734\n",
      "\n",
      "Epoch 00730: val_loss improved from 23279.27734 to 22433.27539, saving model to best.h5\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 22433.27539\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00803: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 22433.27539\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 22433.27539\n",
      "Epoch 00930: early stopping\n",
      "31\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 179096059904.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 179096059904.00000\n",
      "\n",
      "Epoch 00003: val_loss improved from 179096059904.00000 to 51979591680.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 51979591680.00000\n",
      "\n",
      "Epoch 00005: val_loss improved from 51979591680.00000 to 50373197824.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 50373197824.00000 to 5814969344.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 5814969344.00000\n",
      "\n",
      "Epoch 00008: val_loss improved from 5814969344.00000 to 194404016.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 194404016.00000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 194404016.00000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 194404016.00000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 194404016.00000\n",
      "\n",
      "Epoch 00013: val_loss improved from 194404016.00000 to 13216829.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 13216829.00000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 13216829.00000\n",
      "\n",
      "Epoch 00030: val_loss improved from 13216829.00000 to 1856124.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1856124.87500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1856124.87500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1856124.87500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1856124.87500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1856124.87500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1856124.87500\n",
      "\n",
      "Epoch 00037: val_loss improved from 1856124.87500 to 166934.82812, saving model to best.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 166934.82812\n",
      "\n",
      "Epoch 00048: val_loss improved from 166934.82812 to 157634.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 157634.96875\n",
      "\n",
      "Epoch 00066: val_loss improved from 157634.96875 to 147894.82812, saving model to best.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 147894.82812\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 147894.82812\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 147894.82812\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 147894.82812\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 147894.82812\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 147894.82812\n",
      "\n",
      "Epoch 00073: val_loss improved from 147894.82812 to 144756.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00074: val_loss improved from 144756.53125 to 142701.29688, saving model to best.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 142701.29688\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 142701.29688\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 142701.29688\n",
      "\n",
      "Epoch 00078: val_loss improved from 142701.29688 to 135037.67188, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 135037.67188\n",
      "\n",
      "Epoch 00080: val_loss improved from 135037.67188 to 134062.95312, saving model to best.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 134062.95312\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 134062.95312\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 134062.95312\n",
      "\n",
      "Epoch 00084: val_loss improved from 134062.95312 to 131847.07812, saving model to best.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 131847.07812 to 131278.98438, saving model to best.h5\n",
      "\n",
      "Epoch 00086: val_loss improved from 131278.98438 to 130940.15625, saving model to best.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 130940.15625\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 130940.15625\n",
      "\n",
      "Epoch 00089: val_loss improved from 130940.15625 to 129365.49219, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 129365.49219\n",
      "\n",
      "Epoch 00091: val_loss improved from 129365.49219 to 128253.79688, saving model to best.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 128253.79688 to 127834.67969, saving model to best.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 127834.67969\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 127834.67969\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 127834.67969\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 127834.67969\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 127834.67969\n",
      "\n",
      "Epoch 00098: val_loss improved from 127834.67969 to 127567.03125, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 127567.03125 to 124177.86719, saving model to best.h5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 124177.86719\n",
      "\n",
      "Epoch 00101: val_loss improved from 124177.86719 to 123522.89062, saving model to best.h5\n",
      "\n",
      "Epoch 00102: val_loss improved from 123522.89062 to 123371.08594, saving model to best.h5\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 123371.08594\n",
      "\n",
      "Epoch 00104: val_loss improved from 123371.08594 to 122180.46094, saving model to best.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 122180.46094 to 121303.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00106: val_loss improved from 121303.31250 to 120802.59375, saving model to best.h5\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 120802.59375\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 120802.59375\n",
      "\n",
      "Epoch 00109: val_loss improved from 120802.59375 to 119681.61719, saving model to best.h5\n",
      "\n",
      "Epoch 00110: val_loss improved from 119681.61719 to 118909.85938, saving model to best.h5\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 118909.85938\n",
      "\n",
      "Epoch 00112: val_loss improved from 118909.85938 to 118032.05469, saving model to best.h5\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 118032.05469\n",
      "\n",
      "Epoch 00114: val_loss improved from 118032.05469 to 117172.47656, saving model to best.h5\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 117172.47656\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 117172.47656\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 117172.47656\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 117172.47656\n",
      "\n",
      "Epoch 00119: val_loss improved from 117172.47656 to 117060.79688, saving model to best.h5\n",
      "\n",
      "Epoch 00120: val_loss improved from 117060.79688 to 114489.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00121: val_loss improved from 114489.10156 to 113894.27344, saving model to best.h5\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 113894.27344\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 113894.27344\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 113894.27344\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 113894.27344\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 113894.27344\n",
      "\n",
      "Epoch 00127: val_loss improved from 113894.27344 to 112403.92188, saving model to best.h5\n",
      "\n",
      "Epoch 00128: val_loss improved from 112403.92188 to 111697.71875, saving model to best.h5\n",
      "\n",
      "Epoch 00129: val_loss improved from 111697.71875 to 111433.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 111433.75000\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 111433.75000\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 111433.75000\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 111433.75000\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 111433.75000\n",
      "\n",
      "Epoch 00135: val_loss improved from 111433.75000 to 110900.70312, saving model to best.h5\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 110900.70312\n",
      "\n",
      "Epoch 00137: val_loss improved from 110900.70312 to 109936.85156, saving model to best.h5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 109936.85156\n",
      "\n",
      "Epoch 00139: val_loss improved from 109936.85156 to 109658.39844, saving model to best.h5\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 109658.39844\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 109658.39844\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 109658.39844\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 109658.39844\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 109658.39844\n",
      "\n",
      "Epoch 00145: val_loss improved from 109658.39844 to 109021.28125, saving model to best.h5\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 109021.28125\n",
      "\n",
      "Epoch 00147: val_loss improved from 109021.28125 to 108737.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 108737.96875\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00150: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 108737.96875\n",
      "\n",
      "Epoch 00162: val_loss improved from 108737.96875 to 107307.26562, saving model to best.h5\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 107307.26562\n",
      "\n",
      "Epoch 00164: val_loss improved from 107307.26562 to 107129.73438, saving model to best.h5\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 107129.73438\n",
      "\n",
      "Epoch 00166: val_loss improved from 107129.73438 to 106890.39844, saving model to best.h5\n",
      "\n",
      "Epoch 00167: val_loss improved from 106890.39844 to 106809.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 106809.18750\n",
      "\n",
      "Epoch 00179: val_loss improved from 106809.18750 to 106357.32812, saving model to best.h5\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 106357.32812\n",
      "\n",
      "Epoch 00181: val_loss improved from 106357.32812 to 105741.92969, saving model to best.h5\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 105741.92969\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 105741.92969\n",
      "\n",
      "Epoch 00184: val_loss improved from 105741.92969 to 105189.35938, saving model to best.h5\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 105189.35938\n",
      "\n",
      "Epoch 00196: val_loss improved from 105189.35938 to 105020.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 105020.10156\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 105020.10156\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 105020.10156\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 105020.10156\n",
      "\n",
      "Epoch 00201: val_loss improved from 105020.10156 to 103881.32031, saving model to best.h5\n",
      "\n",
      "Epoch 00202: val_loss improved from 103881.32031 to 103787.57812, saving model to best.h5\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 103787.57812\n",
      "\n",
      "Epoch 00204: val_loss improved from 103787.57812 to 103505.57812, saving model to best.h5\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 103505.57812\n",
      "\n",
      "Epoch 00213: val_loss improved from 103505.57812 to 102924.71094, saving model to best.h5\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 102924.71094\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 102924.71094\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 102924.71094\n",
      "\n",
      "Epoch 00217: val_loss improved from 102924.71094 to 102648.49219, saving model to best.h5\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 102648.49219\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 102648.49219\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 102648.49219\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 102648.49219\n",
      "\n",
      "Epoch 00222: val_loss improved from 102648.49219 to 102086.60156, saving model to best.h5\n",
      "\n",
      "Epoch 00223: val_loss improved from 102086.60156 to 101777.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 101777.53125\n",
      "\n",
      "Epoch 00236: val_loss improved from 101777.53125 to 101068.40625, saving model to best.h5\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 101068.40625\n",
      "\n",
      "Epoch 00257: val_loss improved from 101068.40625 to 99127.30469, saving model to best.h5\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 99127.30469\n",
      "\n",
      "Epoch 00259: val_loss improved from 99127.30469 to 99069.42188, saving model to best.h5\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 99069.42188\n",
      "\n",
      "Epoch 00273: val_loss improved from 99069.42188 to 99003.85938, saving model to best.h5\n",
      "\n",
      "Epoch 00274: val_loss improved from 99003.85938 to 97899.49219, saving model to best.h5\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 97899.49219\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 97899.49219\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 97899.49219\n",
      "\n",
      "Epoch 00278: val_loss improved from 97899.49219 to 97741.63281, saving model to best.h5\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 97741.63281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00284: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 97741.63281\n",
      "\n",
      "Epoch 00292: val_loss improved from 97741.63281 to 96937.84375, saving model to best.h5\n",
      "\n",
      "Epoch 00293: val_loss improved from 96937.84375 to 96787.17188, saving model to best.h5\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 96787.17188\n",
      "\n",
      "Epoch 00295: val_loss improved from 96787.17188 to 96536.09375, saving model to best.h5\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 96536.09375\n",
      "\n",
      "Epoch 00306: val_loss improved from 96536.09375 to 96394.90625, saving model to best.h5\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 96394.90625\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 96394.90625\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 96394.90625\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 96394.90625\n",
      "\n",
      "Epoch 00311: val_loss improved from 96394.90625 to 96211.78906, saving model to best.h5\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 96211.78906\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 96211.78906\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 96211.78906\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 96211.78906\n",
      "\n",
      "Epoch 00316: val_loss improved from 96211.78906 to 95465.27344, saving model to best.h5\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 95465.27344\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 95465.27344\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 95465.27344\n",
      "\n",
      "Epoch 00320: val_loss improved from 95465.27344 to 95166.47656, saving model to best.h5\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 95166.47656\n",
      "\n",
      "Epoch 00335: val_loss improved from 95166.47656 to 94670.32031, saving model to best.h5\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 94670.32031\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 94670.32031\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 94670.32031\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 94670.32031\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 94670.32031\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 94670.32031\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 94670.32031\n",
      "\n",
      "Epoch 00343: val_loss improved from 94670.32031 to 94143.19531, saving model to best.h5\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 94143.19531\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 94143.19531\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 94143.19531\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 94143.19531\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 94143.19531\n",
      "\n",
      "Epoch 00349: val_loss improved from 94143.19531 to 93838.78125, saving model to best.h5\n",
      "\n",
      "Epoch 00350: val_loss improved from 93838.78125 to 93737.14844, saving model to best.h5\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 93737.14844\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 93737.14844\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 93737.14844\n",
      "\n",
      "Epoch 00354: val_loss improved from 93737.14844 to 93565.99219, saving model to best.h5\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 93565.99219\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 93565.99219\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 93565.99219\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 93565.99219\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 93565.99219\n",
      "\n",
      "Epoch 00360: val_loss improved from 93565.99219 to 93442.21094, saving model to best.h5\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 93442.21094\n",
      "\n",
      "Epoch 00374: val_loss improved from 93442.21094 to 93131.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 93131.25000\n",
      "\n",
      "Epoch 00376: val_loss improved from 93131.25000 to 92639.82031, saving model to best.h5\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 92639.82031\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 92639.82031\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 92639.82031\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 92639.82031\n",
      "\n",
      "Epoch 00381: val_loss improved from 92639.82031 to 92530.19531, saving model to best.h5\n",
      "\n",
      "Epoch 00382: val_loss improved from 92530.19531 to 92376.21094, saving model to best.h5\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 92376.21094\n",
      "\n",
      "Epoch 00384: val_loss improved from 92376.21094 to 92329.80469, saving model to best.h5\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 92329.80469\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 92329.80469\n",
      "\n",
      "Epoch 00387: val_loss improved from 92329.80469 to 92220.30469, saving model to best.h5\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 92220.30469\n",
      "\n",
      "Epoch 00404: val_loss improved from 92220.30469 to 91930.60938, saving model to best.h5\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 91930.60938\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 91930.60938\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 91930.60938\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 91930.60938\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 91930.60938\n",
      "\n",
      "Epoch 00410: val_loss improved from 91930.60938 to 91609.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 91609.43750\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 91609.43750\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 91609.43750\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 91609.43750\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 91609.43750\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 91609.43750\n",
      "\n",
      "Epoch 00417: val_loss improved from 91609.43750 to 91331.52344, saving model to best.h5\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 91331.52344\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 91331.52344\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 91331.52344\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 91331.52344\n",
      "\n",
      "Epoch 00422: val_loss improved from 91331.52344 to 91175.29688, saving model to best.h5\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 91175.29688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00431: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 91175.29688\n",
      "\n",
      "Epoch 00434: val_loss improved from 91175.29688 to 91026.96094, saving model to best.h5\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 91026.96094\n",
      "\n",
      "Epoch 00452: val_loss improved from 91026.96094 to 90500.22656, saving model to best.h5\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 90500.22656\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 90500.22656\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 90500.22656\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 90500.22656\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 90500.22656\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 90500.22656\n",
      "\n",
      "Epoch 00459: val_loss improved from 90500.22656 to 90413.01562, saving model to best.h5\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 90413.01562\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 90413.01562\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 90413.01562\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 90413.01562\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 90413.01562\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 90413.01562\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 90413.01562\n",
      "\n",
      "Epoch 00467: val_loss improved from 90413.01562 to 90258.07812, saving model to best.h5\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 90258.07812\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 90258.07812\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 90258.07812\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 90258.07812\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 90258.07812\n",
      "\n",
      "Epoch 00473: val_loss improved from 90258.07812 to 90126.35938, saving model to best.h5\n",
      "\n",
      "Epoch 00474: val_loss improved from 90126.35938 to 90106.01562, saving model to best.h5\n",
      "\n",
      "Epoch 00475: val_loss improved from 90106.01562 to 90101.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 90101.56250\n",
      "\n",
      "Epoch 00493: val_loss improved from 90101.56250 to 89953.21094, saving model to best.h5\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 89953.21094\n",
      "\n",
      "Epoch 00547: val_loss improved from 89953.21094 to 89360.23438, saving model to best.h5\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 89360.23438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00574: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 89360.23438\n",
      "\n",
      "Epoch 00583: val_loss improved from 89360.23438 to 89339.67188, saving model to best.h5\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 89339.67188\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 89339.67188\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 89339.67188\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 89339.67188\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 89339.67188\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 89339.67188\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 89339.67188\n",
      "\n",
      "Epoch 00591: val_loss improved from 89339.67188 to 88829.07031, saving model to best.h5\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 88829.07031\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 88829.07031\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 88829.07031\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 88829.07031\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 88829.07031\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 88829.07031\n",
      "\n",
      "Epoch 00598: val_loss improved from 88829.07031 to 88809.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 88809.53125\n",
      "\n",
      "Epoch 00662: val_loss improved from 88809.53125 to 88292.20312, saving model to best.h5\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 88292.20312\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00723: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 88292.20312\n",
      "\n",
      "Epoch 00749: val_loss improved from 88292.20312 to 87074.14062, saving model to best.h5\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 87074.14062\n",
      "\n",
      "Epoch 00803: val_loss improved from 87074.14062 to 85978.88281, saving model to best.h5\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 85978.88281\n",
      "\n",
      "Epoch 00818: val_loss improved from 85978.88281 to 85847.40625, saving model to best.h5\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 85847.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00877: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 85847.40625\n",
      "\n",
      "Epoch 00912: val_loss improved from 85847.40625 to 85793.48438, saving model to best.h5\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 85793.48438\n",
      "\n",
      "Epoch 00932: val_loss improved from 85793.48438 to 84725.90625, saving model to best.h5\n",
      "\n",
      "Epoch 00933: val_loss improved from 84725.90625 to 83029.35156, saving model to best.h5\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 83029.35156\n",
      "\n",
      "Epoch 01008: val_loss improved from 83029.35156 to 76283.94531, saving model to best.h5\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 76283.94531\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 76283.94531\n",
      "\n",
      "Epoch 01011: val_loss improved from 76283.94531 to 71755.07812, saving model to best.h5\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 71755.07812\n",
      "\n",
      "Epoch 01013: val_loss improved from 71755.07812 to 66358.47656, saving model to best.h5\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 66358.47656\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 66358.47656\n",
      "\n",
      "Epoch 01016: val_loss improved from 66358.47656 to 61020.29297, saving model to best.h5\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 61020.29297\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 61020.29297\n",
      "\n",
      "Epoch 01019: val_loss improved from 61020.29297 to 56407.68359, saving model to best.h5\n",
      "\n",
      "Epoch 01020: val_loss improved from 56407.68359 to 54359.05078, saving model to best.h5\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 54359.05078\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01022: val_loss improved from 54359.05078 to 52187.06641, saving model to best.h5\n",
      "\n",
      "Epoch 01023: val_loss improved from 52187.06641 to 51014.32422, saving model to best.h5\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 51014.32422\n",
      "\n",
      "Epoch 01025: val_loss improved from 51014.32422 to 49451.36719, saving model to best.h5\n",
      "\n",
      "Epoch 01026: val_loss improved from 49451.36719 to 48506.61719, saving model to best.h5\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 48506.61719\n",
      "\n",
      "Epoch 01028: val_loss improved from 48506.61719 to 48326.10156, saving model to best.h5\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 48326.10156\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 48326.10156\n",
      "\n",
      "Epoch 01031: val_loss improved from 48326.10156 to 44783.80078, saving model to best.h5\n",
      "\n",
      "Epoch 01032: val_loss improved from 44783.80078 to 44300.40234, saving model to best.h5\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 44300.40234\n",
      "\n",
      "Epoch 01034: val_loss improved from 44300.40234 to 43175.44141, saving model to best.h5\n",
      "\n",
      "Epoch 01035: val_loss improved from 43175.44141 to 42746.09766, saving model to best.h5\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 42746.09766\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 42746.09766\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 42746.09766\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 42746.09766\n",
      "\n",
      "Epoch 01040: val_loss improved from 42746.09766 to 40891.55078, saving model to best.h5\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 40891.55078\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 40891.55078\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 40891.55078\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 40891.55078\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 40891.55078\n",
      "\n",
      "Epoch 01046: val_loss improved from 40891.55078 to 40548.88672, saving model to best.h5\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 40548.88672\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 40548.88672\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 40548.88672\n",
      "\n",
      "Epoch 01050: val_loss improved from 40548.88672 to 40287.49219, saving model to best.h5\n",
      "\n",
      "Epoch 01051: val_loss improved from 40287.49219 to 37897.09375, saving model to best.h5\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 37897.09375\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 37897.09375\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 37897.09375\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 37897.09375\n",
      "\n",
      "Epoch 01056: val_loss improved from 37897.09375 to 37877.40234, saving model to best.h5\n",
      "\n",
      "Epoch 01057: val_loss improved from 37877.40234 to 37264.55859, saving model to best.h5\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 37264.55859\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 37264.55859\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 37264.55859\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 37264.55859\n",
      "\n",
      "Epoch 01062: val_loss improved from 37264.55859 to 36865.74609, saving model to best.h5\n",
      "\n",
      "Epoch 01063: val_loss improved from 36865.74609 to 36221.19141, saving model to best.h5\n",
      "\n",
      "Epoch 01064: val_loss improved from 36221.19141 to 35936.82812, saving model to best.h5\n",
      "\n",
      "Epoch 01065: val_loss improved from 35936.82812 to 35931.15625, saving model to best.h5\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 35931.15625\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 35931.15625\n",
      "\n",
      "Epoch 01068: val_loss improved from 35931.15625 to 35601.91797, saving model to best.h5\n",
      "\n",
      "Epoch 01069: val_loss improved from 35601.91797 to 35374.75391, saving model to best.h5\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 35374.75391\n",
      "\n",
      "Epoch 01071: val_loss improved from 35374.75391 to 35243.68359, saving model to best.h5\n",
      "\n",
      "Epoch 01072: val_loss improved from 35243.68359 to 34886.30078, saving model to best.h5\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 34886.30078\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 34886.30078\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 34886.30078\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 34886.30078\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 34886.30078\n",
      "\n",
      "Epoch 01078: val_loss improved from 34886.30078 to 34571.27344, saving model to best.h5\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 34571.27344\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 34571.27344\n",
      "\n",
      "Epoch 01081: val_loss improved from 34571.27344 to 33709.51562, saving model to best.h5\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 33709.51562\n",
      "\n",
      "Epoch 01091: val_loss improved from 33709.51562 to 32992.64844, saving model to best.h5\n",
      "\n",
      "Epoch 01092: val_loss improved from 32992.64844 to 32892.30078, saving model to best.h5\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 32892.30078\n",
      "\n",
      "Epoch 01103: val_loss improved from 32892.30078 to 32846.17578, saving model to best.h5\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 32846.17578\n",
      "\n",
      "Epoch 01114: val_loss improved from 32846.17578 to 32521.93750, saving model to best.h5\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 32521.93750\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 32521.93750\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 32521.93750\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 32521.93750\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 32521.93750\n",
      "\n",
      "Epoch 01120: val_loss improved from 32521.93750 to 32326.34570, saving model to best.h5\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 32326.34570\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 32326.34570\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 32326.34570\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 32326.34570\n",
      "\n",
      "Epoch 01125: val_loss improved from 32326.34570 to 32058.64062, saving model to best.h5\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 32058.64062\n",
      "\n",
      "Epoch 01127: val_loss improved from 32058.64062 to 32006.56641, saving model to best.h5\n",
      "\n",
      "Epoch 01128: val_loss improved from 32006.56641 to 31974.82031, saving model to best.h5\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 31974.82031\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 31974.82031\n",
      "\n",
      "Epoch 01131: val_loss improved from 31974.82031 to 31667.54883, saving model to best.h5\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 31667.54883\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 31667.54883\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 31667.54883\n",
      "\n",
      "Epoch 01135: val_loss improved from 31667.54883 to 31529.42969, saving model to best.h5\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 31529.42969\n",
      "\n",
      "Epoch 01137: val_loss improved from 31529.42969 to 31362.05273, saving model to best.h5\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 31362.05273\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 31362.05273\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 31362.05273\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 31362.05273\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 31362.05273\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 31362.05273\n",
      "\n",
      "Epoch 01144: val_loss improved from 31362.05273 to 31083.17578, saving model to best.h5\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 31083.17578\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 31083.17578\n",
      "\n",
      "Epoch 01147: val_loss improved from 31083.17578 to 30756.23047, saving model to best.h5\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 30756.23047\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 30756.23047\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 30756.23047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01151: val_loss did not improve from 30756.23047\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 30756.23047\n",
      "\n",
      "Epoch 01153: val_loss improved from 30756.23047 to 30670.06250, saving model to best.h5\n",
      "\n",
      "Epoch 01154: val_loss improved from 30670.06250 to 30344.97852, saving model to best.h5\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 30344.97852\n",
      "\n",
      "Epoch 01156: val_loss improved from 30344.97852 to 30218.52930, saving model to best.h5\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 30218.52930\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 30218.52930\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 30218.52930\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 30218.52930\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 30218.52930\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 30218.52930\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 30218.52930\n",
      "\n",
      "Epoch 01164: val_loss improved from 30218.52930 to 29911.74609, saving model to best.h5\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 29911.74609\n",
      "\n",
      "Epoch 01175: val_loss improved from 29911.74609 to 29673.36523, saving model to best.h5\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 29673.36523\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 29673.36523\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 29673.36523\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 29673.36523\n",
      "\n",
      "Epoch 01180: val_loss improved from 29673.36523 to 29504.71484, saving model to best.h5\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 29504.71484\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 29504.71484\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 29504.71484\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 29504.71484\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 29504.71484\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 29504.71484\n",
      "\n",
      "Epoch 01187: val_loss improved from 29504.71484 to 29394.23828, saving model to best.h5\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 29394.23828\n",
      "\n",
      "Epoch 01199: val_loss improved from 29394.23828 to 29099.37500, saving model to best.h5\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 29099.37500\n",
      "\n",
      "Epoch 01208: val_loss improved from 29099.37500 to 28869.75000, saving model to best.h5\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 28869.75000\n",
      "\n",
      "Epoch 01237: val_loss improved from 28869.75000 to 28716.67188, saving model to best.h5\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 28716.67188\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 28716.67188\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 28716.67188\n",
      "\n",
      "Epoch 01241: val_loss improved from 28716.67188 to 28421.68555, saving model to best.h5\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 28421.68555\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 28421.68555\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 28421.68555\n",
      "\n",
      "Epoch 01245: val_loss improved from 28421.68555 to 28185.49805, saving model to best.h5\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 28185.49805\n",
      "\n",
      "Epoch 01255: val_loss improved from 28185.49805 to 27930.75977, saving model to best.h5\n",
      "\n",
      "Epoch 01256: val_loss improved from 27930.75977 to 27822.62695, saving model to best.h5\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 27822.62695\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 27822.62695\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 27822.62695\n",
      "\n",
      "Epoch 01260: val_loss improved from 27822.62695 to 27670.81250, saving model to best.h5\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 27670.81250\n",
      "\n",
      "Epoch 01271: val_loss improved from 27670.81250 to 27524.09180, saving model to best.h5\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 27524.09180\n",
      "\n",
      "Epoch 01273: val_loss improved from 27524.09180 to 27291.92773, saving model to best.h5\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 27291.92773\n",
      "\n",
      "Epoch 01288: val_loss improved from 27291.92773 to 27124.67773, saving model to best.h5\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 27124.67773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01290: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 27124.67773\n",
      "\n",
      "Epoch 01298: val_loss improved from 27124.67773 to 27032.88672, saving model to best.h5\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 27032.88672\n",
      "\n",
      "Epoch 01310: val_loss improved from 27032.88672 to 27018.04688, saving model to best.h5\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 27018.04688\n",
      "\n",
      "Epoch 01328: val_loss improved from 27018.04688 to 26545.37305, saving model to best.h5\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 26545.37305\n",
      "\n",
      "Epoch 01355: val_loss improved from 26545.37305 to 26410.20508, saving model to best.h5\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 26410.20508\n",
      "\n",
      "Epoch 01367: val_loss improved from 26410.20508 to 26318.32812, saving model to best.h5\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 26318.32812\n",
      "\n",
      "Epoch 01381: val_loss improved from 26318.32812 to 25909.52539, saving model to best.h5\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 25909.52539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01443: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 25909.52539\n",
      "\n",
      "Epoch 01514: val_loss improved from 25909.52539 to 25884.73633, saving model to best.h5\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 25884.73633\n",
      "\n",
      "Epoch 01524: val_loss improved from 25884.73633 to 25737.44141, saving model to best.h5\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01529: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 25737.44141\n",
      "\n",
      "Epoch 01536: val_loss improved from 25737.44141 to 25630.62891, saving model to best.h5\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01546: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01547: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01550: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01551: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01553: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 25630.62891\n",
      "\n",
      "Epoch 01556: val_loss improved from 25630.62891 to 25595.65820, saving model to best.h5\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01558: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01560: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01562: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01568: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01570: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01579: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01580: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01581: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01582: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01583: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01584: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01585: val_loss did not improve from 25595.65820\n",
      "\n",
      "Epoch 01586: val_loss improved from 25595.65820 to 25567.20898, saving model to best.h5\n",
      "\n",
      "Epoch 01587: val_loss did not improve from 25567.20898\n",
      "\n",
      "Epoch 01588: val_loss did not improve from 25567.20898\n",
      "\n",
      "Epoch 01589: val_loss did not improve from 25567.20898\n",
      "\n",
      "Epoch 01590: val_loss did not improve from 25567.20898\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01591: val_loss did not improve from 25567.20898\n",
      "\n",
      "Epoch 01592: val_loss did not improve from 25567.20898\n",
      "\n",
      "Epoch 01593: val_loss improved from 25567.20898 to 25382.61719, saving model to best.h5\n",
      "\n",
      "Epoch 01594: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01595: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01596: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01597: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01598: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01599: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01600: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01601: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01602: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01603: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01604: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01605: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01606: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01607: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01608: val_loss did not improve from 25382.61719\n",
      "\n",
      "Epoch 01609: val_loss improved from 25382.61719 to 25099.46680, saving model to best.h5\n",
      "\n",
      "Epoch 01610: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01611: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01612: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01613: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01614: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01615: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01616: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01617: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01618: val_loss did not improve from 25099.46680\n",
      "\n",
      "Epoch 01619: val_loss improved from 25099.46680 to 24609.28516, saving model to best.h5\n",
      "\n",
      "Epoch 01620: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01621: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01622: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01623: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01624: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01625: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01626: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01627: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01628: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01629: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01630: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01631: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01632: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01633: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01634: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01635: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01636: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01637: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01638: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01639: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01640: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01641: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01642: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01643: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01644: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01645: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01646: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01647: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01648: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01649: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01650: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01651: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01652: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01653: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01654: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01655: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01656: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01657: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01658: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01659: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01660: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01661: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01662: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01663: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01664: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01665: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01666: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01667: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01668: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01669: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01670: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01671: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01672: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01673: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01674: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01675: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01676: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01677: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01678: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01679: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01680: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01681: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01682: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01683: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01684: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01685: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01686: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01687: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01688: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01689: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01690: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01691: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01692: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01693: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01694: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01695: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01696: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01697: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01698: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01699: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01700: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01701: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01702: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01703: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01704: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01705: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01706: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01707: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01708: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01709: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01710: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01711: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01712: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01713: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01714: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01715: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01716: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01717: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01718: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01719: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01720: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01721: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01722: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01723: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01724: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01725: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01726: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01727: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01728: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01729: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01730: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01731: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01732: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01733: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01734: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01735: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01736: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01737: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01738: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01739: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01740: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01741: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01742: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01743: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01744: val_loss did not improve from 24609.28516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01745: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01746: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01747: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01748: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01749: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01750: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01751: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01752: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01753: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01754: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01755: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01756: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01757: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01758: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01759: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01760: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01761: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01762: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01763: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01764: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01765: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01766: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01767: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01768: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01769: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01770: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01771: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01772: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01773: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01774: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01775: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01776: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01777: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01778: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01779: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01780: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01781: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01782: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01783: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01784: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01785: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01786: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01787: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01788: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01789: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01790: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01791: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01792: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01793: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01794: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01795: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01796: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01797: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01798: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01799: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01800: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01801: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01802: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01803: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01804: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01805: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01806: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01807: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01808: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01809: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01810: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01811: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01812: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01813: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01814: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01815: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01816: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01817: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01818: val_loss did not improve from 24609.28516\n",
      "\n",
      "Epoch 01819: val_loss did not improve from 24609.28516\n",
      "Epoch 01819: early stopping\n",
      "32\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 192857391104.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 192857391104.00000\n",
      "\n",
      "Epoch 00003: val_loss improved from 192857391104.00000 to 98675302400.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 98675302400.00000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 98675302400.00000\n",
      "\n",
      "Epoch 00006: val_loss improved from 98675302400.00000 to 330372960.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 330372960.00000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 330372960.00000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 330372960.00000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 330372960.00000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 330372960.00000\n",
      "\n",
      "Epoch 00012: val_loss improved from 330372960.00000 to 8049809.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 8049809.00000\n",
      "\n",
      "Epoch 00028: val_loss improved from 8049809.00000 to 1324172.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1324172.50000\n",
      "\n",
      "Epoch 00043: val_loss improved from 1324172.50000 to 692205.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 692205.12500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 692205.12500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 692205.12500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 692205.12500\n",
      "\n",
      "Epoch 00048: val_loss improved from 692205.12500 to 642199.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 642199.18750\n",
      "\n",
      "Epoch 00050: val_loss improved from 642199.18750 to 572012.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 572012.75000\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 572012.75000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 572012.75000\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 572012.75000\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 572012.75000\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 572012.75000\n",
      "\n",
      "Epoch 00057: val_loss improved from 572012.75000 to 526073.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 526073.62500\n",
      "\n",
      "Epoch 00059: val_loss improved from 526073.62500 to 516400.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 516400.96875\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 516400.96875\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 516400.96875\n",
      "\n",
      "Epoch 00063: val_loss improved from 516400.96875 to 494056.71875, saving model to best.h5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 494056.71875\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 494056.71875\n",
      "\n",
      "Epoch 00066: val_loss improved from 494056.71875 to 489371.93750, saving model to best.h5\n",
      "\n",
      "Epoch 00067: val_loss improved from 489371.93750 to 479851.65625, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00068: val_loss did not improve from 479851.65625\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 479851.65625\n",
      "\n",
      "Epoch 00070: val_loss improved from 479851.65625 to 466988.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00071: val_loss improved from 466988.53125 to 455103.78125, saving model to best.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 455103.78125\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 455103.78125\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 455103.78125\n",
      "\n",
      "Epoch 00075: val_loss improved from 455103.78125 to 446902.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 446902.25000\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 446902.25000\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 446902.25000\n",
      "\n",
      "Epoch 00079: val_loss improved from 446902.25000 to 445170.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 445170.31250 to 427660.46875, saving model to best.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 427660.46875\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 427660.46875\n",
      "\n",
      "Epoch 00083: val_loss improved from 427660.46875 to 324257.71875, saving model to best.h5\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 324257.71875\n",
      "\n",
      "Epoch 00085: val_loss improved from 324257.71875 to 244043.67188, saving model to best.h5\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 244043.67188\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 244043.67188\n",
      "\n",
      "Epoch 00088: val_loss improved from 244043.67188 to 234654.04688, saving model to best.h5\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 234654.04688\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 234654.04688\n",
      "\n",
      "Epoch 00091: val_loss improved from 234654.04688 to 228333.79688, saving model to best.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 228333.79688 to 227607.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 227607.53125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 227607.53125\n",
      "\n",
      "Epoch 00095: val_loss improved from 227607.53125 to 222268.42188, saving model to best.h5\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 222268.42188\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 222268.42188\n",
      "\n",
      "Epoch 00098: val_loss improved from 222268.42188 to 213778.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 213778.81250\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 213778.81250\n",
      "\n",
      "Epoch 00101: val_loss improved from 213778.81250 to 209644.21875, saving model to best.h5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 209644.21875\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 209644.21875\n",
      "\n",
      "Epoch 00104: val_loss improved from 209644.21875 to 204828.70312, saving model to best.h5\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 204828.70312\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 204828.70312\n",
      "\n",
      "Epoch 00107: val_loss improved from 204828.70312 to 202258.84375, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 202258.84375\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 202258.84375\n",
      "\n",
      "Epoch 00110: val_loss improved from 202258.84375 to 197430.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 197430.87500\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 197430.87500\n",
      "\n",
      "Epoch 00113: val_loss improved from 197430.87500 to 194815.15625, saving model to best.h5\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 194815.15625\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 194815.15625\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 194815.15625\n",
      "\n",
      "Epoch 00117: val_loss improved from 194815.15625 to 192973.65625, saving model to best.h5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 192973.65625\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 192973.65625\n",
      "\n",
      "Epoch 00120: val_loss improved from 192973.65625 to 186266.79688, saving model to best.h5\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 186266.79688\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 186266.79688\n",
      "\n",
      "Epoch 00123: val_loss improved from 186266.79688 to 183550.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 183550.31250\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 183550.31250\n",
      "\n",
      "Epoch 00126: val_loss improved from 183550.31250 to 181125.51562, saving model to best.h5\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 181125.51562\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 181125.51562\n",
      "\n",
      "Epoch 00129: val_loss improved from 181125.51562 to 176098.03125, saving model to best.h5\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 176098.03125\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 176098.03125\n",
      "\n",
      "Epoch 00132: val_loss improved from 176098.03125 to 174777.76562, saving model to best.h5\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 174777.76562\n",
      "\n",
      "Epoch 00134: val_loss improved from 174777.76562 to 171705.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 171705.31250\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 171705.31250\n",
      "\n",
      "Epoch 00137: val_loss improved from 171705.31250 to 169780.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 169780.50000\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 169780.50000\n",
      "\n",
      "Epoch 00140: val_loss improved from 169780.50000 to 165451.28125, saving model to best.h5\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 165451.28125\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 165451.28125\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 165451.28125\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 165451.28125\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 165451.28125\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 165451.28125\n",
      "\n",
      "Epoch 00147: val_loss improved from 165451.28125 to 160730.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 160730.37500\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 160730.37500\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 160730.37500\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 160730.37500\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 160730.37500\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 160730.37500\n",
      "\n",
      "Epoch 00154: val_loss improved from 160730.37500 to 157885.32812, saving model to best.h5\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 157885.32812\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 157885.32812\n",
      "\n",
      "Epoch 00157: val_loss improved from 157885.32812 to 152217.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 152217.81250\n",
      "\n",
      "Epoch 00159: val_loss improved from 152217.81250 to 150559.28125, saving model to best.h5\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 150559.28125\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 150559.28125\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 150559.28125\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 150559.28125\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 150559.28125\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 150559.28125\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 150559.28125\n",
      "\n",
      "Epoch 00167: val_loss improved from 150559.28125 to 149653.60938, saving model to best.h5\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 149653.60938\n",
      "\n",
      "Epoch 00169: val_loss improved from 149653.60938 to 143885.46875, saving model to best.h5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 143885.46875\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 143885.46875\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 143885.46875\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 143885.46875\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 143885.46875\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 143885.46875\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 143885.46875\n",
      "\n",
      "Epoch 00177: val_loss improved from 143885.46875 to 140143.89062, saving model to best.h5\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 140143.89062\n",
      "\n",
      "Epoch 00187: val_loss improved from 140143.89062 to 138395.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00188: val_loss improved from 138395.18750 to 137024.92188, saving model to best.h5\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 137024.92188\n",
      "\n",
      "Epoch 00197: val_loss improved from 137024.92188 to 136991.09375, saving model to best.h5\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 136991.09375\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 136991.09375\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 136991.09375\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 136991.09375\n",
      "\n",
      "Epoch 00202: val_loss improved from 136991.09375 to 132802.85938, saving model to best.h5\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 132802.85938\n",
      "\n",
      "Epoch 00204: val_loss improved from 132802.85938 to 132465.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 132465.25000\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 132465.25000\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 132465.25000\n",
      "\n",
      "Epoch 00208: val_loss improved from 132465.25000 to 129358.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 129358.25000\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 129358.25000\n",
      "\n",
      "Epoch 00211: val_loss improved from 129358.25000 to 129340.64844, saving model to best.h5\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 129340.64844\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 129340.64844\n",
      "\n",
      "Epoch 00214: val_loss improved from 129340.64844 to 125757.07812, saving model to best.h5\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 125757.07812\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 125757.07812\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 125757.07812\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 125757.07812\n",
      "\n",
      "Epoch 00219: val_loss improved from 125757.07812 to 124918.94531, saving model to best.h5\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 124918.94531\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 124918.94531\n",
      "\n",
      "Epoch 00222: val_loss improved from 124918.94531 to 124870.59375, saving model to best.h5\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 124870.59375\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 124870.59375\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 124870.59375\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 124870.59375\n",
      "\n",
      "Epoch 00227: val_loss improved from 124870.59375 to 121551.02344, saving model to best.h5\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 121551.02344\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 121551.02344\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 121551.02344\n",
      "\n",
      "Epoch 00231: val_loss improved from 121551.02344 to 120460.74219, saving model to best.h5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 120460.74219\n",
      "\n",
      "Epoch 00246: val_loss improved from 120460.74219 to 116843.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 116843.18750\n",
      "\n",
      "Epoch 00248: val_loss improved from 116843.18750 to 115710.38281, saving model to best.h5\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 115710.38281\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 115710.38281\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 115710.38281\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 115710.38281\n",
      "\n",
      "Epoch 00253: val_loss improved from 115710.38281 to 114960.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 114960.18750\n",
      "\n",
      "Epoch 00262: val_loss improved from 114960.18750 to 112129.60156, saving model to best.h5\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 112129.60156\n",
      "\n",
      "Epoch 00264: val_loss improved from 112129.60156 to 111642.78906, saving model to best.h5\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 111642.78906\n",
      "\n",
      "Epoch 00273: val_loss improved from 111642.78906 to 109475.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 109475.10156\n",
      "\n",
      "Epoch 00282: val_loss improved from 109475.10156 to 107394.82812, saving model to best.h5\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 107394.82812\n",
      "\n",
      "Epoch 00293: val_loss improved from 107394.82812 to 106136.74219, saving model to best.h5\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 106136.74219\n",
      "\n",
      "Epoch 00303: val_loss improved from 106136.74219 to 103406.39844, saving model to best.h5\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 103406.39844\n",
      "\n",
      "Epoch 00305: val_loss improved from 103406.39844 to 103229.89844, saving model to best.h5\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 103229.89844\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 103229.89844\n",
      "\n",
      "Epoch 00308: val_loss improved from 103229.89844 to 102249.22656, saving model to best.h5\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 102249.22656\n",
      "\n",
      "Epoch 00321: val_loss improved from 102249.22656 to 100731.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 100731.43750\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 100731.43750\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 100731.43750\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 100731.43750\n",
      "\n",
      "Epoch 00326: val_loss improved from 100731.43750 to 98818.68750, saving model to best.h5\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 98818.68750\n",
      "\n",
      "Epoch 00343: val_loss improved from 98818.68750 to 97813.67188, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00344: val_loss did not improve from 97813.67188\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 97813.67188\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 97813.67188\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 97813.67188\n",
      "\n",
      "Epoch 00348: val_loss improved from 97813.67188 to 96594.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 96594.10156\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 96594.10156\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 96594.10156\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 96594.10156\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 96594.10156\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 96594.10156\n",
      "\n",
      "Epoch 00355: val_loss improved from 96594.10156 to 95469.82812, saving model to best.h5\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 95469.82812\n",
      "\n",
      "Epoch 00368: val_loss improved from 95469.82812 to 94569.66406, saving model to best.h5\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 94569.66406\n",
      "\n",
      "Epoch 00370: val_loss improved from 94569.66406 to 94190.57031, saving model to best.h5\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 94190.57031\n",
      "\n",
      "Epoch 00379: val_loss improved from 94190.57031 to 93547.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 93547.10156\n",
      "\n",
      "Epoch 00381: val_loss improved from 93547.10156 to 92645.24219, saving model to best.h5\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 92645.24219\n",
      "\n",
      "Epoch 00383: val_loss improved from 92645.24219 to 91156.55469, saving model to best.h5\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 91156.55469\n",
      "\n",
      "Epoch 00393: val_loss improved from 91156.55469 to 90926.69531, saving model to best.h5\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 90926.69531\n",
      "\n",
      "Epoch 00430: val_loss improved from 90926.69531 to 89917.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 89917.25000\n",
      "\n",
      "Epoch 00432: val_loss improved from 89917.25000 to 88990.64062, saving model to best.h5\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 88990.64062\n",
      "\n",
      "Epoch 00444: val_loss improved from 88990.64062 to 88871.42188, saving model to best.h5\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 88871.42188\n",
      "\n",
      "Epoch 00453: val_loss improved from 88871.42188 to 88074.73438, saving model to best.h5\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 88074.73438\n",
      "\n",
      "Epoch 00465: val_loss improved from 88074.73438 to 84280.92188, saving model to best.h5\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 84280.92188\n",
      "\n",
      "Epoch 00467: val_loss improved from 84280.92188 to 83668.30469, saving model to best.h5\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 83668.30469\n",
      "\n",
      "Epoch 00484: val_loss improved from 83668.30469 to 83335.51562, saving model to best.h5\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 83335.51562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00486: val_loss did not improve from 83335.51562\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 83335.51562\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 83335.51562\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 83335.51562\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 83335.51562\n",
      "\n",
      "Epoch 00491: val_loss improved from 83335.51562 to 81862.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 81862.75000\n",
      "\n",
      "Epoch 00526: val_loss improved from 81862.75000 to 81578.08594, saving model to best.h5\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 81578.08594\n",
      "\n",
      "Epoch 00540: val_loss improved from 81578.08594 to 81091.17969, saving model to best.h5\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 81091.17969\n",
      "\n",
      "Epoch 00549: val_loss improved from 81091.17969 to 80334.40625, saving model to best.h5\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 80334.40625\n",
      "\n",
      "Epoch 00567: val_loss improved from 80334.40625 to 79932.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 79932.43750\n",
      "\n",
      "Epoch 00582: val_loss improved from 79932.43750 to 78727.38281, saving model to best.h5\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 78727.38281\n",
      "\n",
      "Epoch 00599: val_loss improved from 78727.38281 to 77683.51562, saving model to best.h5\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 77683.51562\n",
      "\n",
      "Epoch 00621: val_loss improved from 77683.51562 to 77334.57812, saving model to best.h5\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 77334.57812\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 77334.57812\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 77334.57812\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 77334.57812\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 77334.57812\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 77334.57812\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 77334.57812\n",
      "\n",
      "Epoch 00629: val_loss improved from 77334.57812 to 75448.93750, saving model to best.h5\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 75448.93750\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 75448.93750\n",
      "\n",
      "Epoch 00632: val_loss improved from 75448.93750 to 75137.30469, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00633: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 75137.30469\n",
      "\n",
      "Epoch 00716: val_loss improved from 75137.30469 to 72523.67969, saving model to best.h5\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 72523.67969\n",
      "\n",
      "Epoch 00743: val_loss improved from 72523.67969 to 69500.88281, saving model to best.h5\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 69500.88281\n",
      "\n",
      "Epoch 00759: val_loss improved from 69500.88281 to 69314.14062, saving model to best.h5\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 69314.14062\n",
      "\n",
      "Epoch 00768: val_loss improved from 69314.14062 to 69253.42969, saving model to best.h5\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 69253.42969\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00785: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 69253.42969\n",
      "\n",
      "Epoch 00806: val_loss improved from 69253.42969 to 67464.89062, saving model to best.h5\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 67464.89062\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00939: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 67464.89062\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 67464.89062\n",
      "Epoch 01006: early stopping\n",
      "33\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 366105919488.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 366105919488.00000\n",
      "\n",
      "Epoch 00003: val_loss improved from 366105919488.00000 to 157549084672.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 157549084672.00000 to 108501450752.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 108501450752.00000\n",
      "\n",
      "Epoch 00006: val_loss improved from 108501450752.00000 to 3714589184.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3714589184.00000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3714589184.00000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3714589184.00000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3714589184.00000\n",
      "\n",
      "Epoch 00011: val_loss improved from 3714589184.00000 to 1279606016.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1279606016.00000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1279606016.00000\n",
      "\n",
      "Epoch 00014: val_loss improved from 1279606016.00000 to 147037504.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 147037504.00000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 147037504.00000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 147037504.00000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 147037504.00000\n",
      "\n",
      "Epoch 00019: val_loss improved from 147037504.00000 to 6526394.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6526394.00000\n",
      "\n",
      "Epoch 00028: val_loss improved from 6526394.00000 to 4248853.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 4248853.00000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 4248853.00000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 4248853.00000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 4248853.00000\n",
      "\n",
      "Epoch 00033: val_loss improved from 4248853.00000 to 3836955.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 3836955.25000\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3836955.25000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 3836955.25000\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 3836955.25000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 3836955.25000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3836955.25000\n",
      "\n",
      "Epoch 00040: val_loss improved from 3836955.25000 to 2110586.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2110586.75000\n",
      "\n",
      "Epoch 00049: val_loss improved from 2110586.75000 to 1826524.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1826524.25000\n",
      "\n",
      "Epoch 00051: val_loss improved from 1826524.25000 to 1742954.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1742954.50000\n",
      "\n",
      "Epoch 00065: val_loss improved from 1742954.50000 to 1741633.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00066: val_loss improved from 1741633.50000 to 1687377.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1687377.37500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: val_loss improved from 1687377.37500 to 1674659.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 1674659.25000 to 1670376.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1670376.12500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1670376.12500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1670376.12500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1670376.12500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1670376.12500\n",
      "\n",
      "Epoch 00075: val_loss improved from 1670376.12500 to 1657856.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss improved from 1657856.75000 to 1644904.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1644904.37500\n",
      "\n",
      "Epoch 00078: val_loss improved from 1644904.37500 to 1632959.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1632959.12500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1632959.12500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1632959.12500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1632959.12500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1632959.12500\n",
      "\n",
      "Epoch 00084: val_loss improved from 1632959.12500 to 1606621.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1606621.50000\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1606621.50000\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1606621.50000\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1606621.50000\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1606621.50000\n",
      "\n",
      "Epoch 00090: val_loss improved from 1606621.50000 to 1595850.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 1595850.50000 to 1575953.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 1575953.50000 to 1571046.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1571046.37500\n",
      "\n",
      "Epoch 00094: val_loss improved from 1571046.37500 to 1566568.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00095: val_loss improved from 1566568.62500 to 1562231.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1562231.50000\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1562231.50000\n",
      "\n",
      "Epoch 00098: val_loss improved from 1562231.50000 to 1544554.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 1544554.87500 to 1539857.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1539857.12500\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1539857.12500\n",
      "\n",
      "Epoch 00102: val_loss improved from 1539857.12500 to 1525960.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00103: val_loss improved from 1525960.75000 to 1520612.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00104: val_loss improved from 1520612.12500 to 1516313.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1516313.75000\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1516313.75000\n",
      "\n",
      "Epoch 00107: val_loss improved from 1516313.75000 to 1504524.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1504524.87500\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1504524.87500\n",
      "\n",
      "Epoch 00110: val_loss improved from 1504524.87500 to 1500874.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1500874.37500\n",
      "\n",
      "Epoch 00112: val_loss improved from 1500874.37500 to 1489732.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1489732.87500\n",
      "\n",
      "Epoch 00114: val_loss improved from 1489732.87500 to 1475814.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00115: val_loss improved from 1475814.00000 to 1468461.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1468461.37500\n",
      "\n",
      "Epoch 00117: val_loss improved from 1468461.37500 to 1454765.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1454765.87500\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1454765.87500\n",
      "\n",
      "Epoch 00120: val_loss improved from 1454765.87500 to 1449681.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00121: val_loss improved from 1449681.87500 to 1437064.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1437064.62500\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1437064.62500\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1437064.62500\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1437064.62500\n",
      "\n",
      "Epoch 00126: val_loss improved from 1437064.62500 to 1425303.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1425303.62500\n",
      "\n",
      "Epoch 00128: val_loss improved from 1425303.62500 to 1405794.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00129: val_loss improved from 1405794.25000 to 1403868.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00130: val_loss improved from 1403868.87500 to 1401680.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1401680.12500\n",
      "\n",
      "Epoch 00132: val_loss improved from 1401680.12500 to 1391600.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1391600.00000\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1391600.00000\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1391600.00000\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1391600.00000\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1391600.00000\n",
      "\n",
      "Epoch 00138: val_loss improved from 1391600.00000 to 1360481.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1360481.12500\n",
      "\n",
      "Epoch 00140: val_loss improved from 1360481.12500 to 1344041.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00141: val_loss improved from 1344041.37500 to 1341440.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1341440.12500\n",
      "\n",
      "Epoch 00143: val_loss improved from 1341440.12500 to 1336589.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00144: val_loss improved from 1336589.87500 to 1331847.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00145: val_loss improved from 1331847.62500 to 1319853.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1319853.25000\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1319853.25000\n",
      "\n",
      "Epoch 00148: val_loss improved from 1319853.25000 to 1308735.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00149: val_loss improved from 1308735.87500 to 1307424.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1307424.87500\n",
      "\n",
      "Epoch 00151: val_loss improved from 1307424.87500 to 1293413.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1293413.37500\n",
      "\n",
      "Epoch 00153: val_loss improved from 1293413.37500 to 1281707.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00154: val_loss improved from 1281707.50000 to 1276479.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1276479.12500\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1276479.12500\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1276479.12500\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1276479.12500\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1276479.12500\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1276479.12500\n",
      "\n",
      "Epoch 00161: val_loss improved from 1276479.12500 to 1273942.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00162: val_loss improved from 1273942.12500 to 1241573.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1241573.75000\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1241573.75000\n",
      "\n",
      "Epoch 00165: val_loss improved from 1241573.75000 to 1224299.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00166: val_loss improved from 1224299.75000 to 1221837.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1221837.75000\n",
      "\n",
      "Epoch 00168: val_loss improved from 1221837.75000 to 1208716.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1208716.87500\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1208716.87500\n",
      "\n",
      "Epoch 00171: val_loss improved from 1208716.87500 to 1195547.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1195547.12500\n",
      "\n",
      "Epoch 00173: val_loss improved from 1195547.12500 to 1185656.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00174: val_loss improved from 1185656.50000 to 1181958.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00175: val_loss improved from 1181958.12500 to 1175908.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00176: val_loss improved from 1175908.25000 to 1170252.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1170252.12500\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1170252.12500\n",
      "\n",
      "Epoch 00179: val_loss improved from 1170252.12500 to 1160687.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1160687.75000\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1160687.75000\n",
      "\n",
      "Epoch 00182: val_loss improved from 1160687.75000 to 1154461.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00183: val_loss improved from 1154461.12500 to 1144490.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00184: val_loss improved from 1144490.00000 to 1134592.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00185: val_loss improved from 1134592.87500 to 1127243.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1127243.75000\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1127243.75000\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1127243.75000\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1127243.75000\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1127243.75000\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1127243.75000\n",
      "\n",
      "Epoch 00192: val_loss improved from 1127243.75000 to 1094084.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1094084.50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00194: val_loss did not improve from 1094084.50000\n",
      "\n",
      "Epoch 00195: val_loss improved from 1094084.50000 to 1080315.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1080315.62500\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1080315.62500\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1080315.62500\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1080315.62500\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1080315.62500\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1080315.62500\n",
      "\n",
      "Epoch 00202: val_loss improved from 1080315.62500 to 1067381.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1067381.87500\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1067381.87500\n",
      "\n",
      "Epoch 00205: val_loss improved from 1067381.87500 to 1049788.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1049788.37500\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1049788.37500\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1049788.37500\n",
      "\n",
      "Epoch 00209: val_loss improved from 1049788.37500 to 1014291.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00210: val_loss improved from 1014291.31250 to 1009952.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1009952.75000\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1009952.75000\n",
      "\n",
      "Epoch 00213: val_loss improved from 1009952.75000 to 996225.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00214: val_loss improved from 996225.62500 to 994470.06250, saving model to best.h5\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 994470.06250\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 994470.06250\n",
      "\n",
      "Epoch 00217: val_loss improved from 994470.06250 to 985969.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00218: val_loss improved from 985969.81250 to 977475.06250, saving model to best.h5\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 977475.06250\n",
      "\n",
      "Epoch 00220: val_loss improved from 977475.06250 to 965191.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 965191.18750\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 965191.18750\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 965191.18750\n",
      "\n",
      "Epoch 00224: val_loss improved from 965191.18750 to 964951.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00225: val_loss improved from 964951.75000 to 961629.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 961629.87500\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 961629.87500\n",
      "\n",
      "Epoch 00228: val_loss improved from 961629.87500 to 952665.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00229: val_loss improved from 952665.75000 to 924508.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 924508.87500\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 924508.87500\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 924508.87500\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 924508.87500\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 924508.87500\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 924508.87500\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 924508.87500\n",
      "\n",
      "Epoch 00237: val_loss improved from 924508.87500 to 888430.93750, saving model to best.h5\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 888430.93750\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 888430.93750\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 888430.93750\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 888430.93750\n",
      "\n",
      "Epoch 00242: val_loss improved from 888430.93750 to 869555.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00243: val_loss improved from 869555.37500 to 863643.93750, saving model to best.h5\n",
      "\n",
      "Epoch 00244: val_loss improved from 863643.93750 to 861409.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00245: val_loss improved from 861409.56250 to 855586.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 855586.81250\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 855586.81250\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 855586.81250\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 855586.81250\n",
      "\n",
      "Epoch 00250: val_loss improved from 855586.81250 to 833848.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 833848.43750\n",
      "\n",
      "Epoch 00252: val_loss improved from 833848.43750 to 824053.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 824053.56250\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 824053.56250\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 824053.56250\n",
      "\n",
      "Epoch 00256: val_loss improved from 824053.56250 to 808364.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 808364.81250\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 808364.81250\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 808364.81250\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 808364.81250\n",
      "\n",
      "Epoch 00261: val_loss improved from 808364.81250 to 788109.06250, saving model to best.h5\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 788109.06250\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 788109.06250\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 788109.06250\n",
      "\n",
      "Epoch 00265: val_loss improved from 788109.06250 to 772454.06250, saving model to best.h5\n",
      "\n",
      "Epoch 00266: val_loss improved from 772454.06250 to 766213.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 766213.43750\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 766213.43750\n",
      "\n",
      "Epoch 00269: val_loss improved from 766213.43750 to 764201.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 764201.81250\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 764201.81250\n",
      "\n",
      "Epoch 00272: val_loss improved from 764201.81250 to 743595.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 743595.18750\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 743595.18750\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 743595.18750\n",
      "\n",
      "Epoch 00276: val_loss improved from 743595.18750 to 727029.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00277: val_loss improved from 727029.56250 to 722685.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 722685.00000\n",
      "\n",
      "Epoch 00279: val_loss improved from 722685.00000 to 717135.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 717135.12500\n",
      "\n",
      "Epoch 00281: val_loss improved from 717135.12500 to 714095.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00282: val_loss improved from 714095.43750 to 703794.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 703794.00000\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 703794.00000\n",
      "\n",
      "Epoch 00285: val_loss improved from 703794.00000 to 695476.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 695476.56250\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 695476.56250\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 695476.56250\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 695476.56250\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 695476.56250\n",
      "\n",
      "Epoch 00291: val_loss improved from 695476.56250 to 672708.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 672708.87500\n",
      "\n",
      "Epoch 00302: val_loss improved from 672708.87500 to 645974.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00303: val_loss improved from 645974.12500 to 635985.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 635985.43750\n",
      "\n",
      "Epoch 00305: val_loss improved from 635985.43750 to 623506.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 623506.31250\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 623506.31250\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 623506.31250\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 623506.31250\n",
      "\n",
      "Epoch 00310: val_loss improved from 623506.31250 to 601377.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 601377.81250\n",
      "\n",
      "Epoch 00322: val_loss improved from 601377.81250 to 564947.06250, saving model to best.h5\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 564947.06250\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 564947.06250\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 564947.06250\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 564947.06250\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 564947.06250\n",
      "\n",
      "Epoch 00328: val_loss improved from 564947.06250 to 540425.25000, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00329: val_loss did not improve from 540425.25000\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 540425.25000\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 540425.25000\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 540425.25000\n",
      "\n",
      "Epoch 00333: val_loss improved from 540425.25000 to 525312.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00334: val_loss improved from 525312.31250 to 521728.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 521728.81250\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 521728.81250\n",
      "\n",
      "Epoch 00337: val_loss improved from 521728.81250 to 512565.28125, saving model to best.h5\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 512565.28125\n",
      "\n",
      "Epoch 00339: val_loss improved from 512565.28125 to 510860.43750, saving model to best.h5\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 510860.43750\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 510860.43750\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 510860.43750\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 510860.43750\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 510860.43750\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 510860.43750\n",
      "\n",
      "Epoch 00346: val_loss improved from 510860.43750 to 484795.40625, saving model to best.h5\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 484795.40625\n",
      "\n",
      "Epoch 00348: val_loss improved from 484795.40625 to 478828.03125, saving model to best.h5\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 478828.03125\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 478828.03125\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 478828.03125\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 478828.03125\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 478828.03125\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 478828.03125\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 478828.03125\n",
      "\n",
      "Epoch 00356: val_loss improved from 478828.03125 to 461306.84375, saving model to best.h5\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 461306.84375\n",
      "\n",
      "Epoch 00358: val_loss improved from 461306.84375 to 453308.37500, saving model to best.h5\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 453308.37500\n",
      "\n",
      "Epoch 00360: val_loss improved from 453308.37500 to 450876.71875, saving model to best.h5\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 450876.71875\n",
      "\n",
      "Epoch 00362: val_loss improved from 450876.71875 to 450670.09375, saving model to best.h5\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 450670.09375\n",
      "\n",
      "Epoch 00364: val_loss improved from 450670.09375 to 445176.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00365: val_loss improved from 445176.53125 to 439211.87500, saving model to best.h5\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 439211.87500\n",
      "\n",
      "Epoch 00367: val_loss improved from 439211.87500 to 424192.71875, saving model to best.h5\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 424192.71875\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 424192.71875\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 424192.71875\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 424192.71875\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 424192.71875\n",
      "\n",
      "Epoch 00373: val_loss improved from 424192.71875 to 411785.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 411785.96875\n",
      "\n",
      "Epoch 00375: val_loss improved from 411785.96875 to 408576.46875, saving model to best.h5\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 408576.46875\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 408576.46875\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 408576.46875\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 408576.46875\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 408576.46875\n",
      "\n",
      "Epoch 00381: val_loss improved from 408576.46875 to 387615.78125, saving model to best.h5\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 387615.78125\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 387615.78125\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 387615.78125\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 387615.78125\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 387615.78125\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 387615.78125\n",
      "\n",
      "Epoch 00388: val_loss improved from 387615.78125 to 374055.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00389: val_loss improved from 374055.62500 to 371941.78125, saving model to best.h5\n",
      "\n",
      "Epoch 00390: val_loss improved from 371941.78125 to 365267.21875, saving model to best.h5\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 365267.21875\n",
      "\n",
      "Epoch 00392: val_loss improved from 365267.21875 to 361008.34375, saving model to best.h5\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 361008.34375\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 361008.34375\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 361008.34375\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 361008.34375\n",
      "\n",
      "Epoch 00397: val_loss improved from 361008.34375 to 358756.93750, saving model to best.h5\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 358756.93750\n",
      "\n",
      "Epoch 00399: val_loss improved from 358756.93750 to 345283.53125, saving model to best.h5\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 345283.53125\n",
      "\n",
      "Epoch 00401: val_loss improved from 345283.53125 to 342382.34375, saving model to best.h5\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 342382.34375\n",
      "\n",
      "Epoch 00411: val_loss improved from 342382.34375 to 337797.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 337797.81250\n",
      "\n",
      "Epoch 00413: val_loss improved from 337797.81250 to 314449.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 314449.56250\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 314449.56250\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 314449.56250\n",
      "\n",
      "Epoch 00417: val_loss improved from 314449.56250 to 309850.03125, saving model to best.h5\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 309850.03125\n",
      "\n",
      "Epoch 00419: val_loss improved from 309850.03125 to 301695.68750, saving model to best.h5\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 301695.68750\n",
      "\n",
      "Epoch 00421: val_loss improved from 301695.68750 to 299850.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 299850.18750\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 299850.18750\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 299850.18750\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 299850.18750\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 299850.18750\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 299850.18750\n",
      "\n",
      "Epoch 00428: val_loss improved from 299850.18750 to 294608.40625, saving model to best.h5\n",
      "\n",
      "Epoch 00429: val_loss improved from 294608.40625 to 290423.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 290423.00000\n",
      "\n",
      "Epoch 00431: val_loss improved from 290423.00000 to 289012.34375, saving model to best.h5\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 289012.34375\n",
      "\n",
      "Epoch 00433: val_loss improved from 289012.34375 to 278651.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 278651.96875\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 278651.96875\n",
      "\n",
      "Epoch 00436: val_loss improved from 278651.96875 to 275414.59375, saving model to best.h5\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 275414.59375\n",
      "\n",
      "Epoch 00438: val_loss improved from 275414.59375 to 271646.56250, saving model to best.h5\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 271646.56250\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 271646.56250\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 271646.56250\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 271646.56250\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 271646.56250\n",
      "\n",
      "Epoch 00444: val_loss improved from 271646.56250 to 255750.82812, saving model to best.h5\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 255750.82812\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 255750.82812\n",
      "\n",
      "Epoch 00447: val_loss improved from 255750.82812 to 248933.25000, saving model to best.h5\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 248933.25000\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 248933.25000\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 248933.25000\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 248933.25000\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 248933.25000\n",
      "\n",
      "Epoch 00453: val_loss improved from 248933.25000 to 244203.26562, saving model to best.h5\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 244203.26562\n",
      "\n",
      "Epoch 00455: val_loss improved from 244203.26562 to 238718.60938, saving model to best.h5\n",
      "\n",
      "Epoch 00456: val_loss improved from 238718.60938 to 235195.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 235195.96875\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00458: val_loss did not improve from 235195.96875\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 235195.96875\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 235195.96875\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 235195.96875\n",
      "\n",
      "Epoch 00462: val_loss improved from 235195.96875 to 227266.40625, saving model to best.h5\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 227266.40625\n",
      "\n",
      "Epoch 00464: val_loss improved from 227266.40625 to 225752.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00465: val_loss improved from 225752.75000 to 221925.34375, saving model to best.h5\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 221925.34375\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 221925.34375\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 221925.34375\n",
      "\n",
      "Epoch 00469: val_loss improved from 221925.34375 to 217319.70312, saving model to best.h5\n",
      "\n",
      "Epoch 00470: val_loss improved from 217319.70312 to 212953.45312, saving model to best.h5\n",
      "\n",
      "Epoch 00471: val_loss improved from 212953.45312 to 210745.28125, saving model to best.h5\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 210745.28125\n",
      "\n",
      "Epoch 00473: val_loss improved from 210745.28125 to 209615.76562, saving model to best.h5\n",
      "\n",
      "Epoch 00474: val_loss improved from 209615.76562 to 207022.50000, saving model to best.h5\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 207022.50000\n",
      "\n",
      "Epoch 00476: val_loss improved from 207022.50000 to 203805.39062, saving model to best.h5\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 203805.39062\n",
      "\n",
      "Epoch 00478: val_loss improved from 203805.39062 to 201915.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00479: val_loss improved from 201915.96875 to 199099.04688, saving model to best.h5\n",
      "\n",
      "Epoch 00480: val_loss improved from 199099.04688 to 197953.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00481: val_loss improved from 197953.18750 to 196793.06250, saving model to best.h5\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 196793.06250\n",
      "\n",
      "Epoch 00483: val_loss improved from 196793.06250 to 193640.73438, saving model to best.h5\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 193640.73438\n",
      "\n",
      "Epoch 00492: val_loss improved from 193640.73438 to 185808.79688, saving model to best.h5\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 185808.79688\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 185808.79688\n",
      "\n",
      "Epoch 00495: val_loss improved from 185808.79688 to 179976.73438, saving model to best.h5\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 179976.73438\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 179976.73438\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 179976.73438\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 179976.73438\n",
      "\n",
      "Epoch 00500: val_loss improved from 179976.73438 to 172817.35938, saving model to best.h5\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 172817.35938\n",
      "\n",
      "Epoch 00511: val_loss improved from 172817.35938 to 171610.76562, saving model to best.h5\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 171610.76562\n",
      "\n",
      "Epoch 00523: val_loss improved from 171610.76562 to 149224.29688, saving model to best.h5\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 149224.29688\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 149224.29688\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 149224.29688\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 149224.29688\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 149224.29688\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 149224.29688\n",
      "\n",
      "Epoch 00530: val_loss improved from 149224.29688 to 147159.18750, saving model to best.h5\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 147159.18750\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 147159.18750\n",
      "\n",
      "Epoch 00533: val_loss improved from 147159.18750 to 138550.42188, saving model to best.h5\n",
      "\n",
      "Epoch 00534: val_loss improved from 138550.42188 to 138401.42188, saving model to best.h5\n",
      "\n",
      "Epoch 00535: val_loss improved from 138401.42188 to 138212.60938, saving model to best.h5\n",
      "\n",
      "Epoch 00536: val_loss improved from 138212.60938 to 135547.64062, saving model to best.h5\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 135547.64062\n",
      "\n",
      "Epoch 00538: val_loss improved from 135547.64062 to 134116.85938, saving model to best.h5\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 134116.85938\n",
      "\n",
      "Epoch 00550: val_loss improved from 134116.85938 to 126945.27344, saving model to best.h5\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 126945.27344\n",
      "\n",
      "Epoch 00552: val_loss improved from 126945.27344 to 123262.90625, saving model to best.h5\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 123262.90625\n",
      "\n",
      "Epoch 00561: val_loss improved from 123262.90625 to 116402.96875, saving model to best.h5\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 116402.96875\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 116402.96875\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 116402.96875\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 116402.96875\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 116402.96875\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 116402.96875\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 116402.96875\n",
      "\n",
      "Epoch 00569: val_loss improved from 116402.96875 to 113449.89062, saving model to best.h5\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 113449.89062\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 113449.89062\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 113449.89062\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 113449.89062\n",
      "\n",
      "Epoch 00574: val_loss improved from 113449.89062 to 107939.53906, saving model to best.h5\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 107939.53906\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 107939.53906\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 107939.53906\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 107939.53906\n",
      "\n",
      "Epoch 00579: val_loss improved from 107939.53906 to 104940.54688, saving model to best.h5\n",
      "\n",
      "Epoch 00580: val_loss improved from 104940.54688 to 103869.20312, saving model to best.h5\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 103869.20312\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 103869.20312\n",
      "\n",
      "Epoch 00583: val_loss improved from 103869.20312 to 103460.51562, saving model to best.h5\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 103460.51562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00592: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 103460.51562\n",
      "\n",
      "Epoch 00598: val_loss improved from 103460.51562 to 94807.75781, saving model to best.h5\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 94807.75781\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 94807.75781\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 94807.75781\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 94807.75781\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 94807.75781\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 94807.75781\n",
      "\n",
      "Epoch 00605: val_loss improved from 94807.75781 to 90844.46094, saving model to best.h5\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 90844.46094\n",
      "\n",
      "Epoch 00607: val_loss improved from 90844.46094 to 89660.02344, saving model to best.h5\n",
      "\n",
      "Epoch 00608: val_loss improved from 89660.02344 to 88650.30469, saving model to best.h5\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 88650.30469\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 88650.30469\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 88650.30469\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 88650.30469\n",
      "\n",
      "Epoch 00613: val_loss improved from 88650.30469 to 86512.14062, saving model to best.h5\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 86512.14062\n",
      "\n",
      "Epoch 00637: val_loss improved from 86512.14062 to 77001.51562, saving model to best.h5\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 77001.51562\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 77001.51562\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 77001.51562\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 77001.51562\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 77001.51562\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 77001.51562\n",
      "\n",
      "Epoch 00644: val_loss improved from 77001.51562 to 75191.83594, saving model to best.h5\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 75191.83594\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 75191.83594\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 75191.83594\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 75191.83594\n",
      "\n",
      "Epoch 00649: val_loss improved from 75191.83594 to 73023.30469, saving model to best.h5\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 73023.30469\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 73023.30469\n",
      "\n",
      "Epoch 00652: val_loss improved from 73023.30469 to 71666.89062, saving model to best.h5\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 71666.89062\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 71666.89062\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 71666.89062\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 71666.89062\n",
      "\n",
      "Epoch 00657: val_loss improved from 71666.89062 to 70405.70312, saving model to best.h5\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 70405.70312\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 70405.70312\n",
      "\n",
      "Epoch 00660: val_loss improved from 70405.70312 to 69785.69531, saving model to best.h5\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 69785.69531\n",
      "\n",
      "Epoch 00680: val_loss improved from 69785.69531 to 64403.04688, saving model to best.h5\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 64403.04688\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 64403.04688\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 64403.04688\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 64403.04688\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 64403.04688\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 64403.04688\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 64403.04688\n",
      "\n",
      "Epoch 00688: val_loss improved from 64403.04688 to 63318.86328, saving model to best.h5\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 63318.86328\n",
      "\n",
      "Epoch 00698: val_loss improved from 63318.86328 to 60360.47266, saving model to best.h5\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 60360.47266\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 60360.47266\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 60360.47266\n",
      "\n",
      "Epoch 00702: val_loss improved from 60360.47266 to 59542.31250, saving model to best.h5\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 59542.31250\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 59542.31250\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 59542.31250\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 59542.31250\n",
      "\n",
      "Epoch 00707: val_loss improved from 59542.31250 to 59045.11328, saving model to best.h5\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 59045.11328\n",
      "\n",
      "Epoch 00721: val_loss improved from 59045.11328 to 56206.71094, saving model to best.h5\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 56206.71094\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00734: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 56206.71094\n",
      "\n",
      "Epoch 00740: val_loss improved from 56206.71094 to 55274.61719, saving model to best.h5\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 55274.61719\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 55274.61719\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 55274.61719\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 55274.61719\n",
      "\n",
      "Epoch 00745: val_loss improved from 55274.61719 to 54556.24219, saving model to best.h5\n",
      "\n",
      "Epoch 00746: val_loss improved from 54556.24219 to 53947.53906, saving model to best.h5\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 53947.53906\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 53947.53906\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 53947.53906\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 53947.53906\n",
      "\n",
      "Epoch 00751: val_loss improved from 53947.53906 to 52729.10156, saving model to best.h5\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 52729.10156\n",
      "\n",
      "Epoch 00767: val_loss improved from 52729.10156 to 50986.03906, saving model to best.h5\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 50986.03906\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 50986.03906\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 50986.03906\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 50986.03906\n",
      "\n",
      "Epoch 00772: val_loss improved from 50986.03906 to 50702.14062, saving model to best.h5\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 50702.14062\n",
      "\n",
      "Epoch 00786: val_loss improved from 50702.14062 to 50449.88281, saving model to best.h5\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 50449.88281\n",
      "\n",
      "Epoch 00803: val_loss improved from 50449.88281 to 48299.32031, saving model to best.h5\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 48299.32031\n",
      "\n",
      "Epoch 00841: val_loss improved from 48299.32031 to 46814.46484, saving model to best.h5\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 46814.46484\n",
      "\n",
      "Epoch 00861: val_loss improved from 46814.46484 to 46423.07031, saving model to best.h5\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 46423.07031\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00876: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 46423.07031\n",
      "\n",
      "Epoch 00913: val_loss improved from 46423.07031 to 45875.58984, saving model to best.h5\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 45875.58984\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01028: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 45875.58984\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 45875.58984\n",
      "Epoch 01113: early stopping\n",
      "34\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss=[0]\n",
    "val_loss=[0]\n",
    "tot_loss=[0]\n",
    "\n",
    "for x in range(25,35):\n",
    "    dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_\"+str(x)+\".csv\", header=None)\n",
    "    dataset = dataframe.values\n",
    "\n",
    "    X = dataset[:,0:x*11]\n",
    "    #X = preprocessing.normalize(dataset[:,0:x*23])\n",
    "    y = dataset[:,x*11]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=29)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(x*11, input_dim=x*11, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(x*11, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(x*11, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "    mc = ModelCheckpoint('best.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)    \n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10000, verbose=0, callbacks=[es,mc])\n",
    "\n",
    "    loss = numpy.append(loss, min(history.history['loss']))\n",
    "    val_loss = numpy.append(val_loss, min(history.history['val_loss']))\n",
    "    saved_model = load_model('best.h5')\n",
    "    tot_loss = numpy.append(tot_loss,metrics.mean_squared_error(y,saved_model.predict(X)))\n",
    "\n",
    "    print(x)\n",
    "\n",
    "    \n",
    "loss = numpy.delete(loss,0)\n",
    "val_loss = numpy.delete(val_loss,0)\n",
    "tot_loss = numpy.delete(tot_loss,0)\n",
    "plt.plot(loss, label='train')\n",
    "plt.plot(val_loss, label='test')\n",
    "plt.plot(tot_loss, label='tot')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(numpy.delete(loss,range(45,50)), label='train')\n",
    "#plt.plot(numpy.delete(val_loss,range(45,50)), label='test')\n",
    "\n",
    "temp = numpy.delete(tot_loss,range(23,50))\n",
    "temp = numpy.delete(temp,range(0,15))\n",
    "\n",
    "#plt.plot(numpy.delete(loss,range(0,10)), label='train')\n",
    "#plt.plot(numpy.delete(val_loss,range(0,10)), label='test')\n",
    "#plt.plot(numpy.delete(tot_loss,range(0,10)), label='tot')\n",
    "\n",
    "plt.plot(temp, label='tot')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17165.027111481755"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_loss[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 156131764486.56412, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 156131764486.56412 to 56319296013.12820, saving model to best.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 56319296013.12820\n",
      "\n",
      "Epoch 00004: val_loss improved from 56319296013.12820 to 18794722277.74359, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 18794722277.74359 to 6919031.14103, saving model to best.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6919031.14103\n",
      "\n",
      "Epoch 00015: val_loss improved from 6919031.14103 to 1537395.68269, saving model to best.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1537395.68269\n",
      "\n",
      "Epoch 00031: val_loss improved from 1537395.68269 to 1502781.27564, saving model to best.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1502781.27564\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1502781.27564\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1502781.27564\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1502781.27564\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1502781.27564\n",
      "\n",
      "Epoch 00037: val_loss improved from 1502781.27564 to 1420392.20192, saving model to best.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1420392.20192\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1420392.20192\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1420392.20192\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1420392.20192\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1420392.20192\n",
      "\n",
      "Epoch 00043: val_loss improved from 1420392.20192 to 1349919.12500, saving model to best.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1349919.12500\n",
      "\n",
      "Epoch 00045: val_loss improved from 1349919.12500 to 1290114.51282, saving model to best.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1290114.51282\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1290114.51282\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1290114.51282\n",
      "\n",
      "Epoch 00049: val_loss improved from 1290114.51282 to 1283543.59936, saving model to best.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1283543.59936\n",
      "\n",
      "Epoch 00059: val_loss improved from 1283543.59936 to 1139818.64103, saving model to best.h5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1139818.64103\n",
      "\n",
      "Epoch 00061: val_loss improved from 1139818.64103 to 1116873.63141, saving model to best.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1116873.63141\n",
      "\n",
      "Epoch 00063: val_loss improved from 1116873.63141 to 1096909.96474, saving model to best.h5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1096909.96474\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1096909.96474\n",
      "\n",
      "Epoch 00066: val_loss improved from 1096909.96474 to 1069320.27885, saving model to best.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1069320.27885\n",
      "\n",
      "Epoch 00075: val_loss improved from 1069320.27885 to 981455.89103, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 981455.89103\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 981455.89103\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 981455.89103\n",
      "\n",
      "Epoch 00079: val_loss improved from 981455.89103 to 946039.08333, saving model to best.h5\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 946039.08333\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 946039.08333\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 946039.08333\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 946039.08333\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 946039.08333\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 946039.08333\n",
      "\n",
      "Epoch 00086: val_loss improved from 946039.08333 to 890485.72276, saving model to best.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 890485.72276\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 890485.72276\n",
      "\n",
      "Epoch 00089: val_loss improved from 890485.72276 to 861198.51763, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 861198.51763\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 861198.51763\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 861198.51763\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 861198.51763\n",
      "\n",
      "Epoch 00094: val_loss improved from 861198.51763 to 842384.39744, saving model to best.h5\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 842384.39744\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 842384.39744\n",
      "\n",
      "Epoch 00097: val_loss improved from 842384.39744 to 803242.22596, saving model to best.h5\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 803242.22596\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 803242.22596\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 803242.22596\n",
      "\n",
      "Epoch 00101: val_loss improved from 803242.22596 to 761150.13622, saving model to best.h5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 761150.13622\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 761150.13622\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 761150.13622\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 761150.13622\n",
      "\n",
      "Epoch 00106: val_loss improved from 761150.13622 to 731338.51282, saving model to best.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 731338.51282 to 721147.18429, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 721147.18429\n",
      "\n",
      "Epoch 00109: val_loss improved from 721147.18429 to 709425.34455, saving model to best.h5\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 709425.34455\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 709425.34455\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 709425.34455\n",
      "\n",
      "Epoch 00113: val_loss improved from 709425.34455 to 672445.57532, saving model to best.h5\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 672445.57532\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 672445.57532\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 672445.57532\n",
      "\n",
      "Epoch 00117: val_loss improved from 672445.57532 to 671670.06731, saving model to best.h5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 671670.06731\n",
      "\n",
      "Epoch 00119: val_loss improved from 671670.06731 to 646982.57372, saving model to best.h5\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 646982.57372\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 646982.57372\n",
      "\n",
      "Epoch 00122: val_loss improved from 646982.57372 to 632681.31731, saving model to best.h5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 632681.31731\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 632681.31731\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 632681.31731\n",
      "\n",
      "Epoch 00126: val_loss improved from 632681.31731 to 583652.60096, saving model to best.h5\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 583652.60096\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00128: val_loss improved from 583652.60096 to 577920.52564, saving model to best.h5\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 577920.52564\n",
      "\n",
      "Epoch 00130: val_loss improved from 577920.52564 to 563519.66667, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 563519.66667\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 563519.66667\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 563519.66667\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 563519.66667\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 563519.66667\n",
      "\n",
      "Epoch 00136: val_loss improved from 563519.66667 to 562297.59776, saving model to best.h5\n",
      "\n",
      "Epoch 00137: val_loss improved from 562297.59776 to 537722.41026, saving model to best.h5\n",
      "\n",
      "Epoch 00138: val_loss improved from 537722.41026 to 524422.76603, saving model to best.h5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 524422.76603\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 524422.76603\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 524422.76603\n",
      "\n",
      "Epoch 00142: val_loss improved from 524422.76603 to 513383.14263, saving model to best.h5\n",
      "\n",
      "Epoch 00143: val_loss improved from 513383.14263 to 487877.84776, saving model to best.h5\n",
      "\n",
      "Epoch 00144: val_loss improved from 487877.84776 to 482241.19551, saving model to best.h5\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 482241.19551\n",
      "\n",
      "Epoch 00157: val_loss improved from 482241.19551 to 443683.19151, saving model to best.h5\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 443683.19151\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 443683.19151\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 443683.19151\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 443683.19151\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 443683.19151\n",
      "\n",
      "Epoch 00163: val_loss improved from 443683.19151 to 396431.02724, saving model to best.h5\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 396431.02724\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 396431.02724\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 396431.02724\n",
      "\n",
      "Epoch 00167: val_loss improved from 396431.02724 to 393074.00160, saving model to best.h5\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 393074.00160\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 393074.00160\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 393074.00160\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 393074.00160\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 393074.00160\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 393074.00160\n",
      "\n",
      "Epoch 00174: val_loss improved from 393074.00160 to 339417.19471, saving model to best.h5\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 339417.19471\n",
      "\n",
      "Epoch 00176: val_loss improved from 339417.19471 to 333166.27724, saving model to best.h5\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 333166.27724\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 333166.27724\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 333166.27724\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 333166.27724\n",
      "\n",
      "Epoch 00181: val_loss improved from 333166.27724 to 326510.61619, saving model to best.h5\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 326510.61619\n",
      "\n",
      "Epoch 00193: val_loss improved from 326510.61619 to 324534.56571, saving model to best.h5\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 324534.56571\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 324534.56571\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 324534.56571\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 324534.56571\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 324534.56571\n",
      "\n",
      "Epoch 00199: val_loss improved from 324534.56571 to 290526.13061, saving model to best.h5\n",
      "\n",
      "Epoch 00200: val_loss improved from 290526.13061 to 265701.05769, saving model to best.h5\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 265701.05769\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 265701.05769\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 265701.05769\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 265701.05769\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 265701.05769\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 265701.05769\n",
      "\n",
      "Epoch 00207: val_loss improved from 265701.05769 to 256535.53285, saving model to best.h5\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 256535.53285\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 256535.53285\n",
      "\n",
      "Epoch 00210: val_loss improved from 256535.53285 to 247938.96755, saving model to best.h5\n",
      "\n",
      "Epoch 00211: val_loss improved from 247938.96755 to 245875.97155, saving model to best.h5\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 245875.97155\n",
      "\n",
      "Epoch 00213: val_loss improved from 245875.97155 to 242435.52003, saving model to best.h5\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 242435.52003\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 242435.52003\n",
      "\n",
      "Epoch 00216: val_loss improved from 242435.52003 to 229322.39904, saving model to best.h5\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 229322.39904\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 229322.39904\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 229322.39904\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 229322.39904\n",
      "\n",
      "Epoch 00221: val_loss improved from 229322.39904 to 211688.03165, saving model to best.h5\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 211688.03165\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 211688.03165\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 211688.03165\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 211688.03165\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 211688.03165\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 211688.03165\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 211688.03165\n",
      "\n",
      "Epoch 00229: val_loss improved from 211688.03165 to 200589.57412, saving model to best.h5\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 200589.57412\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 200589.57412\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 200589.57412\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 200589.57412\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 200589.57412\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 200589.57412\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 200589.57412\n",
      "\n",
      "Epoch 00237: val_loss improved from 200589.57412 to 195212.37901, saving model to best.h5\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 195212.37901\n",
      "\n",
      "Epoch 00239: val_loss improved from 195212.37901 to 191909.64904, saving model to best.h5\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 191909.64904\n",
      "\n",
      "Epoch 00241: val_loss improved from 191909.64904 to 178924.48117, saving model to best.h5\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 178924.48117\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 178924.48117\n",
      "\n",
      "Epoch 00244: val_loss improved from 178924.48117 to 176700.12460, saving model to best.h5\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 176700.12460\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 176700.12460\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 176700.12460\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 176700.12460\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 176700.12460\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 176700.12460\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 176700.12460\n",
      "\n",
      "Epoch 00252: val_loss improved from 176700.12460 to 167903.97716, saving model to best.h5\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 167903.97716\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 167903.97716\n",
      "\n",
      "Epoch 00255: val_loss improved from 167903.97716 to 162275.16627, saving model to best.h5\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 162275.16627\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 162275.16627\n",
      "\n",
      "Epoch 00258: val_loss improved from 162275.16627 to 160551.92748, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00259: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 160551.92748\n",
      "\n",
      "Epoch 00269: val_loss improved from 160551.92748 to 153385.41466, saving model to best.h5\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 153385.41466\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 153385.41466\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 153385.41466\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 153385.41466\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 153385.41466\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 153385.41466\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 153385.41466\n",
      "\n",
      "Epoch 00277: val_loss improved from 153385.41466 to 145550.31611, saving model to best.h5\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 145550.31611\n",
      "\n",
      "Epoch 00289: val_loss improved from 145550.31611 to 136068.99239, saving model to best.h5\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 136068.99239\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 136068.99239\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 136068.99239\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 136068.99239\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 136068.99239\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 136068.99239\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 136068.99239\n",
      "\n",
      "Epoch 00297: val_loss improved from 136068.99239 to 130142.10377, saving model to best.h5\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 130142.10377\n",
      "\n",
      "Epoch 00311: val_loss improved from 130142.10377 to 121586.97296, saving model to best.h5\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 121586.97296\n",
      "\n",
      "Epoch 00328: val_loss improved from 121586.97296 to 117393.56210, saving model to best.h5\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 117393.56210\n",
      "\n",
      "Epoch 00344: val_loss improved from 117393.56210 to 111868.42768, saving model to best.h5\n",
      "\n",
      "Epoch 00345: val_loss improved from 111868.42768 to 109527.32171, saving model to best.h5\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 109527.32171\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 109527.32171\n",
      "\n",
      "Epoch 00348: val_loss improved from 109527.32171 to 108703.56611, saving model to best.h5\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 108703.56611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00402: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 108703.56611\n",
      "\n",
      "Epoch 00410: val_loss improved from 108703.56611 to 107220.44311, saving model to best.h5\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 107220.44311\n",
      "\n",
      "Epoch 00412: val_loss improved from 107220.44311 to 105632.76082, saving model to best.h5\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 105632.76082\n",
      "\n",
      "Epoch 00421: val_loss improved from 105632.76082 to 99356.86118, saving model to best.h5\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 99356.86118\n",
      "\n",
      "Epoch 00464: val_loss improved from 99356.86118 to 98209.97196, saving model to best.h5\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 98209.97196\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 98209.97196\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 98209.97196\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 98209.97196\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 98209.97196\n",
      "\n",
      "Epoch 00470: val_loss improved from 98209.97196 to 43798.68324, saving model to best.h5\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 43798.68324\n",
      "\n",
      "Epoch 00482: val_loss improved from 43798.68324 to 40190.03380, saving model to best.h5\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 40190.03380\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 40190.03380\n",
      "\n",
      "Epoch 00485: val_loss improved from 40190.03380 to 40110.42438, saving model to best.h5\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 40110.42438\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 40110.42438\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 40110.42438\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 40110.42438\n",
      "\n",
      "Epoch 00490: val_loss improved from 40110.42438 to 38979.32662, saving model to best.h5\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 38979.32662\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 38979.32662\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 38979.32662\n",
      "\n",
      "Epoch 00494: val_loss improved from 38979.32662 to 38627.05098, saving model to best.h5\n",
      "\n",
      "Epoch 00495: val_loss improved from 38627.05098 to 38616.88632, saving model to best.h5\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 38616.88632\n",
      "\n",
      "Epoch 00497: val_loss improved from 38616.88632 to 38370.21124, saving model to best.h5\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 38370.21124\n",
      "\n",
      "Epoch 00512: val_loss improved from 38370.21124 to 37990.73578, saving model to best.h5\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 37990.73578\n",
      "\n",
      "Epoch 00529: val_loss improved from 37990.73578 to 36968.71444, saving model to best.h5\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 36968.71444\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 36968.71444\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 36968.71444\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 36968.71444\n",
      "\n",
      "Epoch 00534: val_loss improved from 36968.71444 to 36690.81155, saving model to best.h5\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 36690.81155\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 36690.81155\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 36690.81155\n",
      "\n",
      "Epoch 00538: val_loss improved from 36690.81155 to 36293.32702, saving model to best.h5\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 36293.32702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00545: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 36293.32702\n",
      "\n",
      "Epoch 00566: val_loss improved from 36293.32702 to 35432.01152, saving model to best.h5\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 35432.01152\n",
      "\n",
      "Epoch 00599: val_loss improved from 35432.01152 to 34881.84024, saving model to best.h5\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 34881.84024\n",
      "\n",
      "Epoch 00610: val_loss improved from 34881.84024 to 34531.23362, saving model to best.h5\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 34531.23362\n",
      "\n",
      "Epoch 00612: val_loss improved from 34531.23362 to 34352.51753, saving model to best.h5\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 34352.51753\n",
      "\n",
      "Epoch 00614: val_loss improved from 34352.51753 to 34320.47185, saving model to best.h5\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 34320.47185\n",
      "\n",
      "Epoch 00633: val_loss improved from 34320.47185 to 33897.03105, saving model to best.h5\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 33897.03105\n",
      "\n",
      "Epoch 00653: val_loss improved from 33897.03105 to 33861.78516, saving model to best.h5\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 33861.78516\n",
      "\n",
      "Epoch 00655: val_loss improved from 33861.78516 to 33554.02845, saving model to best.h5\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 33554.02845\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 33554.02845\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 33554.02845\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 33554.02845\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 33554.02845\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 33554.02845\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 33554.02845\n",
      "\n",
      "Epoch 00663: val_loss improved from 33554.02845 to 33409.82322, saving model to best.h5\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 33409.82322\n",
      "\n",
      "Epoch 00679: val_loss improved from 33409.82322 to 33196.73458, saving model to best.h5\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 33196.73458\n",
      "\n",
      "Epoch 00692: val_loss improved from 33196.73458 to 32984.56500, saving model to best.h5\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 32984.56500\n",
      "\n",
      "Epoch 00732: val_loss improved from 32984.56500 to 32257.47446, saving model to best.h5\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 32257.47446\n",
      "\n",
      "Epoch 00742: val_loss improved from 32257.47446 to 32089.25351, saving model to best.h5\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 32089.25351\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 32089.25351\n",
      "\n",
      "Epoch 00745: val_loss improved from 32089.25351 to 31916.10717, saving model to best.h5\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 31916.10717\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 31916.10717\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 31916.10717\n",
      "\n",
      "Epoch 00749: val_loss improved from 31916.10717 to 31853.35056, saving model to best.h5\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 31853.35056\n",
      "\n",
      "Epoch 00771: val_loss improved from 31853.35056 to 31744.81030, saving model to best.h5\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 31744.81030\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 31744.81030\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 31744.81030\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 31744.81030\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 31744.81030\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 31744.81030\n",
      "\n",
      "Epoch 00778: val_loss improved from 31744.81030 to 31220.34736, saving model to best.h5\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 31220.34736\n",
      "\n",
      "Epoch 00798: val_loss improved from 31220.34736 to 30874.29297, saving model to best.h5\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 30874.29297\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 30874.29297\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 30874.29297\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 30874.29297\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 30874.29297\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 30874.29297\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 30874.29297\n",
      "\n",
      "Epoch 00806: val_loss improved from 30874.29297 to 30579.98693, saving model to best.h5\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 30579.98693\n",
      "\n",
      "Epoch 00825: val_loss improved from 30579.98693 to 30384.82547, saving model to best.h5\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 30384.82547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00828: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 30384.82547\n",
      "\n",
      "Epoch 00841: val_loss improved from 30384.82547 to 29418.17864, saving model to best.h5\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 29418.17864\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 29418.17864\n",
      "\n",
      "Epoch 00844: val_loss improved from 29418.17864 to 29353.07222, saving model to best.h5\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 29353.07222\n",
      "\n",
      "Epoch 00856: val_loss improved from 29353.07222 to 29116.83719, saving model to best.h5\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 29116.83719\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 29116.83719\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 29116.83719\n",
      "\n",
      "Epoch 00860: val_loss improved from 29116.83719 to 28887.10927, saving model to best.h5\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 28887.10927\n",
      "\n",
      "Epoch 00907: val_loss improved from 28887.10927 to 28623.69281, saving model to best.h5\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 28623.69281\n",
      "\n",
      "Epoch 00917: val_loss improved from 28623.69281 to 27545.60722, saving model to best.h5\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 27545.60722\n",
      "\n",
      "Epoch 00930: val_loss improved from 27545.60722 to 27511.04046, saving model to best.h5\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 27511.04046\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00976: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 27511.04046\n",
      "\n",
      "Epoch 00999: val_loss improved from 27511.04046 to 25831.60727, saving model to best.h5\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 25831.60727\n",
      "\n",
      "Epoch 01074: val_loss improved from 25831.60727 to 23234.52003, saving model to best.h5\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 23234.52003\n",
      "\n",
      "Epoch 01088: val_loss improved from 23234.52003 to 22476.07447, saving model to best.h5\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 22476.07447\n",
      "\n",
      "Epoch 01097: val_loss improved from 22476.07447 to 22198.07497, saving model to best.h5\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 22198.07497\n",
      "\n",
      "Epoch 01122: val_loss improved from 22198.07497 to 21404.16171, saving model to best.h5\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 21404.16171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01126: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 21404.16171\n",
      "\n",
      "Epoch 01270: val_loss improved from 21404.16171 to 20248.51092, saving model to best.h5\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 20248.51092\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 20248.51092\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 20248.51092\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 20248.51092\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 20248.51092\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 20248.51092\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01277: val_loss did not improve from 20248.51092\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 20248.51092\n",
      "\n",
      "Epoch 01279: val_loss improved from 20248.51092 to 20162.05298, saving model to best.h5\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 20162.05298\n",
      "\n",
      "Epoch 01315: val_loss improved from 20162.05298 to 19028.01986, saving model to best.h5\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 19028.01986\n",
      "\n",
      "Epoch 01371: val_loss improved from 19028.01986 to 16767.61564, saving model to best.h5\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 16767.61564\n",
      "\n",
      "Epoch 01394: val_loss improved from 16767.61564 to 16016.06415, saving model to best.h5\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 16016.06415\n",
      "\n",
      "Epoch 01484: val_loss improved from 16016.06415 to 14334.59483, saving model to best.h5\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01514: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01529: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01546: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01547: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01550: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01551: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01553: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01556: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01558: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01560: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01562: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01568: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 14334.59483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01570: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01579: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01580: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01581: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01582: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01583: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01584: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01585: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01586: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01587: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01588: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01589: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01590: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01591: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01592: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01593: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01594: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01595: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01596: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01597: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01598: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01599: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01600: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01601: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01602: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01603: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01604: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01605: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01606: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01607: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01608: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01609: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01610: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01611: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01612: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01613: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01614: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01615: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01616: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01617: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01618: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01619: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01620: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01621: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01622: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01623: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01624: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01625: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01626: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01627: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01628: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01629: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01630: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01631: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01632: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01633: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01634: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01635: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01636: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01637: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01638: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01639: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01640: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01641: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01642: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01643: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01644: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01645: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01646: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01647: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01648: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01649: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01650: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01651: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01652: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01653: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01654: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01655: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01656: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01657: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01658: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01659: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01660: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01661: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01662: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01663: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01664: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01665: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01666: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01667: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01668: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01669: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01670: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01671: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01672: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01673: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01674: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01675: val_loss did not improve from 14334.59483\n",
      "\n",
      "Epoch 01676: val_loss improved from 14334.59483 to 13390.81305, saving model to best.h5\n",
      "\n",
      "Epoch 01677: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01678: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01679: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01680: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01681: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01682: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01683: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01684: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01685: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01686: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01687: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01688: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01689: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01690: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01691: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01692: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01693: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01694: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01695: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01696: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01697: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01698: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01699: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01700: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01701: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01702: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01703: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01704: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01705: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01706: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01707: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01708: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01709: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01710: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01711: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01712: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01713: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01714: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01715: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01716: val_loss did not improve from 13390.81305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01717: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01718: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01719: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01720: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01721: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01722: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01723: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01724: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01725: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01726: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01727: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01728: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01729: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01730: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01731: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01732: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01733: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01734: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01735: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01736: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01737: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01738: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01739: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01740: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01741: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01742: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01743: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01744: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01745: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01746: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01747: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01748: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01749: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01750: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01751: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01752: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01753: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01754: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01755: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01756: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01757: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01758: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01759: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01760: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01761: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01762: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01763: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01764: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01765: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01766: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01767: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01768: val_loss did not improve from 13390.81305\n",
      "\n",
      "Epoch 01769: val_loss improved from 13390.81305 to 13117.26112, saving model to best.h5\n",
      "\n",
      "Epoch 01770: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01771: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01772: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01773: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01774: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01775: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01776: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01777: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01778: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01779: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01780: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01781: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01782: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01783: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01784: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01785: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01786: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01787: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01788: val_loss did not improve from 13117.26112\n",
      "\n",
      "Epoch 01789: val_loss improved from 13117.26112 to 12025.66071, saving model to best.h5\n",
      "\n",
      "Epoch 01790: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01791: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01792: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01793: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01794: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01795: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01796: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01797: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01798: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01799: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01800: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01801: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01802: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01803: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01804: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01805: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01806: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01807: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01808: val_loss did not improve from 12025.66071\n",
      "\n",
      "Epoch 01809: val_loss improved from 12025.66071 to 10813.31523, saving model to best.h5\n",
      "\n",
      "Epoch 01810: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01811: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01812: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01813: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01814: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01815: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01816: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01817: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01818: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01819: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01820: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01821: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01822: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01823: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01824: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01825: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01826: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01827: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01828: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01829: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01830: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01831: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01832: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01833: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01834: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01835: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01836: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01837: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01838: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01839: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01840: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01841: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01842: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01843: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01844: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01845: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01846: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01847: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01848: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01849: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01850: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01851: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01852: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01853: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01854: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01855: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01856: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01857: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01858: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01859: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01860: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01861: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01862: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01863: val_loss did not improve from 10813.31523\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01864: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01865: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01866: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01867: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01868: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01869: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01870: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01871: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01872: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01873: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01874: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01875: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01876: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01877: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01878: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01879: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01880: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01881: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01882: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01883: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01884: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01885: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01886: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01887: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01888: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01889: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01890: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01891: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01892: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01893: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01894: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01895: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01896: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01897: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01898: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01899: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01900: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01901: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01902: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01903: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01904: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01905: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01906: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01907: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01908: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01909: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01910: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01911: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01912: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01913: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01914: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01915: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01916: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01917: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01918: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01919: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01920: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01921: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01922: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01923: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01924: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01925: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01926: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01927: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01928: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01929: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01930: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01931: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01932: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01933: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01934: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01935: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01936: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01937: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01938: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01939: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01940: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01941: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01942: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01943: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01944: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01945: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01946: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01947: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01948: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01949: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01950: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01951: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01952: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01953: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01954: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01955: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01956: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01957: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01958: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01959: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01960: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01961: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01962: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01963: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01964: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01965: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01966: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01967: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01968: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01969: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01970: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01971: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01972: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01973: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01974: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01975: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01976: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01977: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01978: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01979: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01980: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01981: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01982: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01983: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01984: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01985: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01986: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01987: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01988: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01989: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01990: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01991: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01992: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01993: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01994: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01995: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01996: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01997: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01998: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 01999: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02000: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02001: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02002: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02003: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02004: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02005: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02006: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02007: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02008: val_loss did not improve from 10813.31523\n",
      "\n",
      "Epoch 02009: val_loss did not improve from 10813.31523\n",
      "Epoch 02009: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 316617373643.48718, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 316617373643.48718 to 102529850971.89743, saving model to best.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 102529850971.89743 to 100863648584.20512, saving model to best.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 100863648584.20512 to 13622512.61538, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 13622512.61538\n",
      "\n",
      "Epoch 00014: val_loss improved from 13622512.61538 to 4679671.80769, saving model to best.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 4679671.80769\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 4679671.80769\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 4679671.80769\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 4679671.80769\n",
      "\n",
      "Epoch 00019: val_loss improved from 4679671.80769 to 1111340.13141, saving model to best.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1111340.13141\n",
      "\n",
      "Epoch 00031: val_loss improved from 1111340.13141 to 1038684.04167, saving model to best.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1038684.04167\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1038684.04167\n",
      "\n",
      "Epoch 00034: val_loss improved from 1038684.04167 to 101583.39583, saving model to best.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 101583.39583\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 101583.39583\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 101583.39583\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 101583.39583\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 101583.39583\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 101583.39583\n",
      "\n",
      "Epoch 00041: val_loss improved from 101583.39583 to 97485.96715, saving model to best.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 97485.96715 to 93106.14103, saving model to best.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 93106.14103\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 93106.14103\n",
      "\n",
      "Epoch 00045: val_loss improved from 93106.14103 to 64905.46434, saving model to best.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 64905.46434 to 64207.60216, saving model to best.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 64207.60216\n",
      "\n",
      "Epoch 00048: val_loss improved from 64207.60216 to 61287.67248, saving model to best.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 61287.67248\n",
      "\n",
      "Epoch 00050: val_loss improved from 61287.67248 to 60686.70092, saving model to best.h5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 60686.70092\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 60686.70092\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 60686.70092\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 60686.70092\n",
      "\n",
      "Epoch 00055: val_loss improved from 60686.70092 to 60360.36238, saving model to best.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 60360.36238\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 60360.36238\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 60360.36238\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 60360.36238\n",
      "\n",
      "Epoch 00060: val_loss improved from 60360.36238 to 60104.50661, saving model to best.h5\n",
      "\n",
      "Epoch 00061: val_loss improved from 60104.50661 to 59213.32472, saving model to best.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 59213.32472\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 59213.32472\n",
      "\n",
      "Epoch 00064: val_loss improved from 59213.32472 to 58765.69611, saving model to best.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 58765.69611\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 58765.69611\n",
      "\n",
      "Epoch 00067: val_loss improved from 58765.69611 to 58706.43389, saving model to best.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 58706.43389 to 58549.02424, saving model to best.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 58549.02424 to 58286.90244, saving model to best.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 58286.90244\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 58286.90244\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 58286.90244\n",
      "\n",
      "Epoch 00073: val_loss improved from 58286.90244 to 57793.52704, saving model to best.h5\n",
      "\n",
      "Epoch 00074: val_loss improved from 57793.52704 to 57492.39523, saving model to best.h5\n",
      "\n",
      "Epoch 00075: val_loss improved from 57492.39523 to 57480.35617, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 57480.35617\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 57480.35617\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 57480.35617\n",
      "\n",
      "Epoch 00079: val_loss improved from 57480.35617 to 56925.11478, saving model to best.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 56925.11478 to 56786.68650, saving model to best.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 56786.68650\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 56786.68650\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 56786.68650\n",
      "\n",
      "Epoch 00084: val_loss improved from 56786.68650 to 56437.39844, saving model to best.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 56437.39844 to 56171.55950, saving model to best.h5\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 56171.55950\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 56171.55950\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 56171.55950\n",
      "\n",
      "Epoch 00089: val_loss improved from 56171.55950 to 55769.53265, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 55769.53265\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 55769.53265\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 55769.53265\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 55769.53265\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 55769.53265\n",
      "\n",
      "Epoch 00095: val_loss improved from 55769.53265 to 55677.54647, saving model to best.h5\n",
      "\n",
      "Epoch 00096: val_loss improved from 55677.54647 to 55185.85236, saving model to best.h5\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 55185.85236\n",
      "\n",
      "Epoch 00098: val_loss improved from 55185.85236 to 54758.47256, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss improved from 54758.47256 to 54663.29828, saving model to best.h5\n",
      "\n",
      "Epoch 00100: val_loss improved from 54663.29828 to 54390.78145, saving model to best.h5\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 54390.78145\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 54390.78145\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 54390.78145\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 54390.78145\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 54390.78145\n",
      "\n",
      "Epoch 00106: val_loss improved from 54390.78145 to 54301.75781, saving model to best.h5\n",
      "\n",
      "Epoch 00107: val_loss improved from 54301.75781 to 53644.29287, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 53644.29287\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 53644.29287\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 53644.29287\n",
      "\n",
      "Epoch 00111: val_loss improved from 53644.29287 to 53622.23197, saving model to best.h5\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 53622.23197\n",
      "\n",
      "Epoch 00123: val_loss improved from 53622.23197 to 52113.18309, saving model to best.h5\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 52113.18309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00131: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 52113.18309\n",
      "\n",
      "Epoch 00139: val_loss improved from 52113.18309 to 52044.34996, saving model to best.h5\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 52044.34996\n",
      "\n",
      "Epoch 00148: val_loss improved from 52044.34996 to 51035.94752, saving model to best.h5\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 51035.94752\n",
      "\n",
      "Epoch 00150: val_loss improved from 51035.94752 to 50070.41627, saving model to best.h5\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 50070.41627\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 50070.41627\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 50070.41627\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 50070.41627\n",
      "\n",
      "Epoch 00155: val_loss improved from 50070.41627 to 49598.24539, saving model to best.h5\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 49598.24539\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 49598.24539\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 49598.24539\n",
      "\n",
      "Epoch 00159: val_loss improved from 49598.24539 to 49402.33073, saving model to best.h5\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 49402.33073\n",
      "\n",
      "Epoch 00171: val_loss improved from 49402.33073 to 48943.58554, saving model to best.h5\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 48943.58554\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 48943.58554\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 48943.58554\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 48943.58554\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 48943.58554\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 48943.58554\n",
      "\n",
      "Epoch 00178: val_loss improved from 48943.58554 to 48819.07272, saving model to best.h5\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 48819.07272\n",
      "\n",
      "Epoch 00197: val_loss improved from 48819.07272 to 48384.37240, saving model to best.h5\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 48384.37240\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 48384.37240\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 48384.37240\n",
      "\n",
      "Epoch 00201: val_loss improved from 48384.37240 to 48312.17989, saving model to best.h5\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 48312.17989\n",
      "\n",
      "Epoch 00239: val_loss improved from 48312.17989 to 47872.53305, saving model to best.h5\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 47872.53305\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 47872.53305\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 47872.53305\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 47872.53305\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 47872.53305\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 47872.53305\n",
      "\n",
      "Epoch 00246: val_loss improved from 47872.53305 to 47722.03806, saving model to best.h5\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 47722.03806\n",
      "\n",
      "Epoch 00256: val_loss improved from 47722.03806 to 47676.91026, saving model to best.h5\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 47676.91026\n",
      "\n",
      "Epoch 00270: val_loss improved from 47676.91026 to 47619.30709, saving model to best.h5\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 47619.30709\n",
      "\n",
      "Epoch 00279: val_loss improved from 47619.30709 to 47388.15986, saving model to best.h5\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 47388.15986\n",
      "\n",
      "Epoch 00293: val_loss improved from 47388.15986 to 47203.50821, saving model to best.h5\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 47203.50821\n",
      "\n",
      "Epoch 00303: val_loss improved from 47203.50821 to 47065.67167, saving model to best.h5\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 47065.67167\n",
      "\n",
      "Epoch 00320: val_loss improved from 47065.67167 to 46838.63081, saving model to best.h5\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 46838.63081\n",
      "\n",
      "Epoch 00322: val_loss improved from 46838.63081 to 46801.08874, saving model to best.h5\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 46801.08874\n",
      "\n",
      "Epoch 00360: val_loss improved from 46801.08874 to 46435.80028, saving model to best.h5\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 46435.80028\n",
      "\n",
      "Epoch 00369: val_loss improved from 46435.80028 to 46239.83333, saving model to best.h5\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 46239.83333\n",
      "\n",
      "Epoch 00407: val_loss improved from 46239.83333 to 45655.68950, saving model to best.h5\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 45655.68950\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 45655.68950\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 45655.68950\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 45655.68950\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 45655.68950\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 45655.68950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00414: val_loss improved from 45655.68950 to 45279.88642, saving model to best.h5\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 45279.88642\n",
      "\n",
      "Epoch 00424: val_loss improved from 45279.88642 to 45263.63982, saving model to best.h5\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 45263.63982\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 45263.63982\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 45263.63982\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 45263.63982\n",
      "\n",
      "Epoch 00429: val_loss improved from 45263.63982 to 44915.66627, saving model to best.h5\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 44915.66627\n",
      "\n",
      "Epoch 00440: val_loss improved from 44915.66627 to 44798.46935, saving model to best.h5\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 44798.46935\n",
      "\n",
      "Epoch 00455: val_loss improved from 44798.46935 to 44718.91406, saving model to best.h5\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 44718.91406\n",
      "\n",
      "Epoch 00470: val_loss improved from 44718.91406 to 44545.24539, saving model to best.h5\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 44545.24539\n",
      "\n",
      "Epoch 00491: val_loss improved from 44545.24539 to 44002.01703, saving model to best.h5\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 44002.01703\n",
      "\n",
      "Epoch 00520: val_loss improved from 44002.01703 to 43789.13782, saving model to best.h5\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 43789.13782\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 43789.13782\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 43789.13782\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 43789.13782\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 43789.13782\n",
      "\n",
      "Epoch 00526: val_loss improved from 43789.13782 to 43436.84756, saving model to best.h5\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 43436.84756\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 43436.84756\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 43436.84756\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 43436.84756\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 43436.84756\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 43436.84756\n",
      "\n",
      "Epoch 00533: val_loss improved from 43436.84756 to 43293.61639, saving model to best.h5\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 43293.61639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00557: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 43293.61639\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00708: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 43293.61639\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 43293.61639\n",
      "Epoch 00733: early stopping\n",
      "20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 297125331806.31580, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 297125331806.31580 to 111139290704.84210, saving model to best.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 111139290704.84210\n",
      "\n",
      "Epoch 00004: val_loss improved from 111139290704.84210 to 6705408512.00000, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6705408512.00000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6705408512.00000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6705408512.00000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6705408512.00000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6705408512.00000\n",
      "\n",
      "Epoch 00010: val_loss improved from 6705408512.00000 to 5547988911.15790, saving model to best.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 5547988911.15790 to 453560380.63158, saving model to best.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 453560380.63158\n",
      "\n",
      "Epoch 00013: val_loss improved from 453560380.63158 to 86950935.57895, saving model to best.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 86950935.57895\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 86950935.57895\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 86950935.57895\n",
      "\n",
      "Epoch 00017: val_loss improved from 86950935.57895 to 21287980.10526, saving model to best.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 21287980.10526\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 21287980.10526\n",
      "\n",
      "Epoch 00020: val_loss improved from 21287980.10526 to 5475812.52632, saving model to best.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 5475812.52632\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 5475812.52632\n",
      "\n",
      "Epoch 00023: val_loss improved from 5475812.52632 to 1421177.95395, saving model to best.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1421177.95395\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1421177.95395\n",
      "\n",
      "Epoch 00026: val_loss improved from 1421177.95395 to 209148.08717, saving model to best.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 209148.08717\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 209148.08717\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 209148.08717\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 209148.08717\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 209148.08717\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 209148.08717\n",
      "\n",
      "Epoch 00033: val_loss improved from 209148.08717 to 151734.08121, saving model to best.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 151734.08121\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 151734.08121\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 151734.08121\n",
      "\n",
      "Epoch 00037: val_loss improved from 151734.08121 to 140409.09169, saving model to best.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 140409.09169\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 140409.09169\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 140409.09169\n",
      "\n",
      "Epoch 00041: val_loss improved from 140409.09169 to 124786.60896, saving model to best.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 124786.60896\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 124786.60896\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 124786.60896\n",
      "\n",
      "Epoch 00045: val_loss improved from 124786.60896 to 114635.38220, saving model to best.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 114635.38220\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 114635.38220\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 114635.38220\n",
      "\n",
      "Epoch 00049: val_loss improved from 114635.38220 to 107503.07463, saving model to best.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 107503.07463\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 107503.07463\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 107503.07463\n",
      "\n",
      "Epoch 00053: val_loss improved from 107503.07463 to 100896.15049, saving model to best.h5\n",
      "\n",
      "Epoch 00054: val_loss improved from 100896.15049 to 100163.55366, saving model to best.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 100163.55366\n",
      "\n",
      "Epoch 00056: val_loss improved from 100163.55366 to 98130.07124, saving model to best.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 98130.07124\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 98130.07124\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 98130.07124\n",
      "\n",
      "Epoch 00060: val_loss improved from 98130.07124 to 93845.72245, saving model to best.h5\n",
      "\n",
      "Epoch 00061: val_loss improved from 93845.72245 to 93065.24229, saving model to best.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 93065.24229\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 93065.24229\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 93065.24229\n",
      "\n",
      "Epoch 00065: val_loss improved from 93065.24229 to 91968.29163, saving model to best.h5\n",
      "\n",
      "Epoch 00066: val_loss improved from 91968.29163 to 91769.57299, saving model to best.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 91769.57299\n",
      "\n",
      "Epoch 00068: val_loss improved from 91769.57299 to 90256.50463, saving model to best.h5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 90256.50463\n",
      "\n",
      "Epoch 00070: val_loss improved from 90256.50463 to 90096.82833, saving model to best.h5\n",
      "\n",
      "Epoch 00071: val_loss improved from 90096.82833 to 89204.37202, saving model to best.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 89204.37202\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 89204.37202\n",
      "\n",
      "Epoch 00074: val_loss improved from 89204.37202 to 88463.32432, saving model to best.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 88463.32432\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 88463.32432\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 88463.32432\n",
      "\n",
      "Epoch 00078: val_loss improved from 88463.32432 to 88161.92177, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 88161.92177\n",
      "\n",
      "Epoch 00089: val_loss improved from 88161.92177 to 87874.30212, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss improved from 87874.30212 to 86647.57566, saving model to best.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 86647.57566 to 86100.53516, saving model to best.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 86100.53516\n",
      "\n",
      "Epoch 00093: val_loss improved from 86100.53516 to 84799.33316, saving model to best.h5\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 84799.33316\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 84799.33316\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 84799.33316\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 84799.33316\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 84799.33316\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 84799.33316\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 84799.33316\n",
      "\n",
      "Epoch 00101: val_loss improved from 84799.33316 to 83364.46525, saving model to best.h5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 83364.46525\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00110: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 83364.46525\n",
      "\n",
      "Epoch 00118: val_loss improved from 83364.46525 to 81781.91951, saving model to best.h5\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 81781.91951\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 81781.91951\n",
      "\n",
      "Epoch 00121: val_loss improved from 81781.91951 to 80581.88292, saving model to best.h5\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 80581.88292\n",
      "\n",
      "Epoch 00135: val_loss improved from 80581.88292 to 79410.85886, saving model to best.h5\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 79410.85886\n",
      "\n",
      "Epoch 00157: val_loss improved from 79410.85886 to 78942.44058, saving model to best.h5\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 78942.44058\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 78942.44058\n",
      "\n",
      "Epoch 00160: val_loss improved from 78942.44058 to 78854.61770, saving model to best.h5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 78854.61770\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 78854.61770\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 78854.61770\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 78854.61770\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 78854.61770\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 78854.61770\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 78854.61770\n",
      "\n",
      "Epoch 00168: val_loss improved from 78854.61770 to 78742.43339, saving model to best.h5\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 78742.43339\n",
      "\n",
      "Epoch 00184: val_loss improved from 78742.43339 to 78158.61472, saving model to best.h5\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 78158.61472\n",
      "\n",
      "Epoch 00202: val_loss improved from 78158.61472 to 77546.75504, saving model to best.h5\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 77546.75504\n",
      "\n",
      "Epoch 00204: val_loss improved from 77546.75504 to 77379.42979, saving model to best.h5\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 77379.42979\n",
      "\n",
      "Epoch 00223: val_loss improved from 77379.42979 to 76867.38240, saving model to best.h5\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 76867.38240\n",
      "\n",
      "Epoch 00238: val_loss improved from 76867.38240 to 76351.70826, saving model to best.h5\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 76351.70826\n",
      "\n",
      "Epoch 00250: val_loss improved from 76351.70826 to 76004.53176, saving model to best.h5\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 76004.53176\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00261: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 76004.53176\n",
      "\n",
      "Epoch 00264: val_loss improved from 76004.53176 to 75659.25606, saving model to best.h5\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 75659.25606\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 75659.25606\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 75659.25606\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 75659.25606\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 75659.25606\n",
      "\n",
      "Epoch 00270: val_loss improved from 75659.25606 to 75230.23458, saving model to best.h5\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 75230.23458\n",
      "\n",
      "Epoch 00285: val_loss improved from 75230.23458 to 75032.71032, saving model to best.h5\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 75032.71032\n",
      "\n",
      "Epoch 00304: val_loss improved from 75032.71032 to 74126.79811, saving model to best.h5\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 74126.79811\n",
      "\n",
      "Epoch 00319: val_loss improved from 74126.79811 to 73638.23581, saving model to best.h5\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 73638.23581\n",
      "\n",
      "Epoch 00338: val_loss improved from 73638.23581 to 73135.05530, saving model to best.h5\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 73135.05530\n",
      "\n",
      "Epoch 00366: val_loss improved from 73135.05530 to 71221.71854, saving model to best.h5\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 71221.71854\n",
      "\n",
      "Epoch 00404: val_loss improved from 71221.71854 to 69630.59067, saving model to best.h5\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 69630.59067\n",
      "\n",
      "Epoch 00406: val_loss improved from 69630.59067 to 69596.43503, saving model to best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00407: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 69596.43503\n",
      "\n",
      "Epoch 00444: val_loss improved from 69596.43503 to 69521.62993, saving model to best.h5\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 69521.62993\n",
      "\n",
      "Epoch 00460: val_loss improved from 69521.62993 to 68143.03207, saving model to best.h5\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 68143.03207\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 68143.03207\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 68143.03207\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 68143.03207\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 68143.03207\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 68143.03207\n",
      "\n",
      "Epoch 00467: val_loss improved from 68143.03207 to 67368.23150, saving model to best.h5\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 67368.23150\n",
      "\n",
      "Epoch 00513: val_loss improved from 67368.23150 to 65180.93997, saving model to best.h5\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 65180.93997\n",
      "\n",
      "Epoch 00526: val_loss improved from 65180.93997 to 64636.92763, saving model to best.h5\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 64636.92763\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00554: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 64636.92763\n",
      "\n",
      "Epoch 00591: val_loss improved from 64636.92763 to 62866.81127, saving model to best.h5\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 62866.81127\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 62866.81127\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 62866.81127\n",
      "\n",
      "Epoch 00595: val_loss improved from 62866.81127 to 62299.94572, saving model to best.h5\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 62299.94572\n",
      "\n",
      "Epoch 00632: val_loss improved from 62299.94572 to 61845.41643, saving model to best.h5\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 61845.41643\n",
      "\n",
      "Epoch 00641: val_loss improved from 61845.41643 to 61303.87767, saving model to best.h5\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 61303.87767\n",
      "\n",
      "Epoch 00658: val_loss improved from 61303.87767 to 56893.54245, saving model to best.h5\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 56893.54245\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00701: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 56893.54245\n",
      "\n",
      "Epoch 00704: val_loss improved from 56893.54245 to 53419.69685, saving model to best.h5\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 53419.69685\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 53419.69685\n",
      "\n",
      "Epoch 00707: val_loss improved from 53419.69685 to 51684.84611, saving model to best.h5\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 51684.84611\n",
      "\n",
      "Epoch 00709: val_loss improved from 51684.84611 to 51457.35310, saving model to best.h5\n",
      "\n",
      "Epoch 00710: val_loss improved from 51457.35310 to 51234.53084, saving model to best.h5\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 51234.53084\n",
      "\n",
      "Epoch 00735: val_loss improved from 51234.53084 to 50177.94932, saving model to best.h5\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 50177.94932\n",
      "\n",
      "Epoch 00765: val_loss improved from 50177.94932 to 47504.75905, saving model to best.h5\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 47504.75905\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 47504.75905\n",
      "Epoch 00965: early stopping\n",
      "21\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 401498446578.52631, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 401498446578.52631 to 152432104501.89474, saving model to best.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 152432104501.89474\n",
      "\n",
      "Epoch 00004: val_loss improved from 152432104501.89474 to 7632801441.68421, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 7632801441.68421\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 7632801441.68421\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 7632801441.68421\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 7632801441.68421\n",
      "\n",
      "Epoch 00009: val_loss improved from 7632801441.68421 to 5486798282.10526, saving model to best.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5486798282.10526\n",
      "\n",
      "Epoch 00011: val_loss improved from 5486798282.10526 to 543749096.42105, saving model to best.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 543749096.42105\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 543749096.42105\n",
      "\n",
      "Epoch 00014: val_loss improved from 543749096.42105 to 137470805.05263, saving model to best.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 137470805.05263\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 137470805.05263\n",
      "\n",
      "Epoch 00017: val_loss improved from 137470805.05263 to 3812530.68421, saving model to best.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3812530.68421\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3812530.68421\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3812530.68421\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 3812530.68421\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00022: val_loss did not improve from 3812530.68421\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 3812530.68421\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3812530.68421\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 3812530.68421\n",
      "\n",
      "Epoch 00026: val_loss improved from 3812530.68421 to 3809141.93421, saving model to best.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 3809141.93421\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 3809141.93421\n",
      "\n",
      "Epoch 00029: val_loss improved from 3809141.93421 to 797895.34211, saving model to best.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 797895.34211\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 797895.34211\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 797895.34211\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 797895.34211\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 797895.34211\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 797895.34211\n",
      "\n",
      "Epoch 00036: val_loss improved from 797895.34211 to 698429.44737, saving model to best.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 698429.44737\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 698429.44737\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 698429.44737\n",
      "\n",
      "Epoch 00040: val_loss improved from 698429.44737 to 655577.81250, saving model to best.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 655577.81250 to 616014.06579, saving model to best.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 616014.06579\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 616014.06579\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 616014.06579\n",
      "\n",
      "Epoch 00045: val_loss improved from 616014.06579 to 610048.72368, saving model to best.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 610048.72368\n",
      "\n",
      "Epoch 00047: val_loss improved from 610048.72368 to 606039.48684, saving model to best.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 606039.48684\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 606039.48684\n",
      "\n",
      "Epoch 00050: val_loss improved from 606039.48684 to 603052.20395, saving model to best.h5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 603052.20395\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 603052.20395\n",
      "\n",
      "Epoch 00053: val_loss improved from 603052.20395 to 601907.10197, saving model to best.h5\n",
      "\n",
      "Epoch 00054: val_loss improved from 601907.10197 to 601466.55592, saving model to best.h5\n",
      "\n",
      "Epoch 00055: val_loss improved from 601466.55592 to 595260.36184, saving model to best.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 595260.36184\n",
      "\n",
      "Epoch 00064: val_loss improved from 595260.36184 to 590045.14474, saving model to best.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 590045.14474\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 590045.14474\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 590045.14474\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 590045.14474\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 590045.14474\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 590045.14474\n",
      "\n",
      "Epoch 00071: val_loss improved from 590045.14474 to 584828.16118, saving model to best.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 584828.16118\n",
      "\n",
      "Epoch 00073: val_loss improved from 584828.16118 to 570834.44737, saving model to best.h5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 570834.44737\n",
      "\n",
      "Epoch 00075: val_loss improved from 570834.44737 to 567818.03289, saving model to best.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 567818.03289\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 567818.03289\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 567818.03289\n",
      "\n",
      "Epoch 00079: val_loss improved from 567818.03289 to 565331.55921, saving model to best.h5\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 565331.55921\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 565331.55921\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 565331.55921\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 565331.55921\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 565331.55921\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 565331.55921\n",
      "\n",
      "Epoch 00086: val_loss improved from 565331.55921 to 564349.67434, saving model to best.h5\n",
      "\n",
      "Epoch 00087: val_loss improved from 564349.67434 to 551181.41118, saving model to best.h5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 551181.41118\n",
      "\n",
      "Epoch 00089: val_loss improved from 551181.41118 to 550001.21053, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 550001.21053\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 550001.21053\n",
      "\n",
      "Epoch 00092: val_loss improved from 550001.21053 to 543810.30592, saving model to best.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 543810.30592\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 543810.30592\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 543810.30592\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 543810.30592\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 543810.30592\n",
      "\n",
      "Epoch 00098: val_loss improved from 543810.30592 to 540705.44079, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 540705.44079\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 540705.44079\n",
      "\n",
      "Epoch 00101: val_loss improved from 540705.44079 to 539969.30921, saving model to best.h5\n",
      "\n",
      "Epoch 00102: val_loss improved from 539969.30921 to 536376.87171, saving model to best.h5\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 536376.87171\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 536376.87171\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 536376.87171\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 536376.87171\n",
      "\n",
      "Epoch 00107: val_loss improved from 536376.87171 to 535748.21382, saving model to best.h5\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 535748.21382\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 535748.21382\n",
      "\n",
      "Epoch 00110: val_loss improved from 535748.21382 to 532453.46382, saving model to best.h5\n",
      "\n",
      "Epoch 00111: val_loss improved from 532453.46382 to 522511.59046, saving model to best.h5\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 522511.59046\n",
      "\n",
      "Epoch 00125: val_loss improved from 522511.59046 to 522321.92928, saving model to best.h5\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 522321.92928\n",
      "\n",
      "Epoch 00127: val_loss improved from 522321.92928 to 513257.98849, saving model to best.h5\n",
      "\n",
      "Epoch 00128: val_loss improved from 513257.98849 to 492663.61842, saving model to best.h5\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 492663.61842\n",
      "\n",
      "Epoch 00130: val_loss improved from 492663.61842 to 490125.14638, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 490125.14638\n",
      "\n",
      "Epoch 00139: val_loss improved from 490125.14638 to 477055.16941, saving model to best.h5\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 477055.16941\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 477055.16941\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 477055.16941\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 477055.16941\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 477055.16941\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 477055.16941\n",
      "\n",
      "Epoch 00146: val_loss improved from 477055.16941 to 467675.60362, saving model to best.h5\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 467675.60362\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 467675.60362\n",
      "\n",
      "Epoch 00149: val_loss improved from 467675.60362 to 463303.49178, saving model to best.h5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00150: val_loss did not improve from 463303.49178\n",
      "\n",
      "Epoch 00151: val_loss improved from 463303.49178 to 460881.78454, saving model to best.h5\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 460881.78454\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 460881.78454\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 460881.78454\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 460881.78454\n",
      "\n",
      "Epoch 00156: val_loss improved from 460881.78454 to 454764.61020, saving model to best.h5\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 454764.61020\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 454764.61020\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 454764.61020\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 454764.61020\n",
      "\n",
      "Epoch 00161: val_loss improved from 454764.61020 to 448491.55921, saving model to best.h5\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 448491.55921\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 448491.55921\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 448491.55921\n",
      "\n",
      "Epoch 00165: val_loss improved from 448491.55921 to 442865.62993, saving model to best.h5\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 442865.62993\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 442865.62993\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 442865.62993\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 442865.62993\n",
      "\n",
      "Epoch 00170: val_loss improved from 442865.62993 to 439604.29770, saving model to best.h5\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 439604.29770\n",
      "\n",
      "Epoch 00172: val_loss improved from 439604.29770 to 438361.00658, saving model to best.h5\n",
      "\n",
      "Epoch 00173: val_loss improved from 438361.00658 to 432701.17270, saving model to best.h5\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 432701.17270\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 432701.17270\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 432701.17270\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 432701.17270\n",
      "\n",
      "Epoch 00178: val_loss improved from 432701.17270 to 425736.22368, saving model to best.h5\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 425736.22368\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 425736.22368\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 425736.22368\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 425736.22368\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 425736.22368\n",
      "\n",
      "Epoch 00184: val_loss improved from 425736.22368 to 421741.88816, saving model to best.h5\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 421741.88816\n",
      "\n",
      "Epoch 00199: val_loss improved from 421741.88816 to 401740.80428, saving model to best.h5\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 401740.80428\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 401740.80428\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 401740.80428\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 401740.80428\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 401740.80428\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 401740.80428\n",
      "\n",
      "Epoch 00206: val_loss improved from 401740.80428 to 393949.05757, saving model to best.h5\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 393949.05757\n",
      "\n",
      "Epoch 00229: val_loss improved from 393949.05757 to 373456.64309, saving model to best.h5\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 373456.64309\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 373456.64309\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 373456.64309\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 373456.64309\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 373456.64309\n",
      "\n",
      "Epoch 00235: val_loss improved from 373456.64309 to 365457.74507, saving model to best.h5\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 365457.74507\n",
      "\n",
      "Epoch 00250: val_loss improved from 365457.74507 to 350376.89309, saving model to best.h5\n",
      "\n",
      "Epoch 00251: val_loss improved from 350376.89309 to 349473.80263, saving model to best.h5\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 349473.80263\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 349473.80263\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 349473.80263\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 349473.80263\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 349473.80263\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 349473.80263\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 349473.80263\n",
      "\n",
      "Epoch 00259: val_loss improved from 349473.80263 to 348262.36678, saving model to best.h5\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 348262.36678\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 348262.36678\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 348262.36678\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 348262.36678\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 348262.36678\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 348262.36678\n",
      "\n",
      "Epoch 00266: val_loss improved from 348262.36678 to 336805.67434, saving model to best.h5\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 336805.67434\n",
      "\n",
      "Epoch 00268: val_loss improved from 336805.67434 to 335114.88487, saving model to best.h5\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 335114.88487\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 335114.88487\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 335114.88487\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 335114.88487\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 335114.88487\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 335114.88487\n",
      "\n",
      "Epoch 00275: val_loss improved from 335114.88487 to 329437.34868, saving model to best.h5\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 329437.34868\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00285: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 329437.34868\n",
      "\n",
      "Epoch 00288: val_loss improved from 329437.34868 to 329320.21711, saving model to best.h5\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 329320.21711\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 329320.21711\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 329320.21711\n",
      "\n",
      "Epoch 00292: val_loss improved from 329320.21711 to 318171.48191, saving model to best.h5\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 318171.48191\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 318171.48191\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 318171.48191\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 318171.48191\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 318171.48191\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 318171.48191\n",
      "\n",
      "Epoch 00299: val_loss improved from 318171.48191 to 317819.53454, saving model to best.h5\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 317819.53454\n",
      "\n",
      "Epoch 00301: val_loss improved from 317819.53454 to 312698.27632, saving model to best.h5\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 312698.27632\n",
      "\n",
      "Epoch 00311: val_loss improved from 312698.27632 to 304007.69572, saving model to best.h5\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 304007.69572\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 304007.69572\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 304007.69572\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 304007.69572\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 304007.69572\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 304007.69572\n",
      "\n",
      "Epoch 00318: val_loss improved from 304007.69572 to 301823.46382, saving model to best.h5\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 301823.46382\n",
      "\n",
      "Epoch 00335: val_loss improved from 301823.46382 to 289967.69490, saving model to best.h5\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 289967.69490\n",
      "\n",
      "Epoch 00344: val_loss improved from 289967.69490 to 285368.81168, saving model to best.h5\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 285368.81168\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 285368.81168\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 285368.81168\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 285368.81168\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 285368.81168\n",
      "\n",
      "Epoch 00350: val_loss improved from 285368.81168 to 281732.36266, saving model to best.h5\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 281732.36266\n",
      "\n",
      "Epoch 00366: val_loss improved from 281732.36266 to 274280.15132, saving model to best.h5\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 274280.15132\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 274280.15132\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 274280.15132\n",
      "\n",
      "Epoch 00370: val_loss improved from 274280.15132 to 273697.86431, saving model to best.h5\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 273697.86431\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 273697.86431\n",
      "\n",
      "Epoch 00373: val_loss improved from 273697.86431 to 272200.09951, saving model to best.h5\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 272200.09951\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 272200.09951\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 272200.09951\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 272200.09951\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 272200.09951\n",
      "\n",
      "Epoch 00379: val_loss improved from 272200.09951 to 270900.89227, saving model to best.h5\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 270900.89227\n",
      "\n",
      "Epoch 00395: val_loss improved from 270900.89227 to 268549.82155, saving model to best.h5\n",
      "\n",
      "Epoch 00396: val_loss improved from 268549.82155 to 266655.08470, saving model to best.h5\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 266655.08470\n",
      "\n",
      "Epoch 00409: val_loss improved from 266655.08470 to 264284.53536, saving model to best.h5\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 264284.53536\n",
      "\n",
      "Epoch 00432: val_loss improved from 264284.53536 to 255067.83882, saving model to best.h5\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 255067.83882\n",
      "\n",
      "Epoch 00443: val_loss improved from 255067.83882 to 253560.58882, saving model to best.h5\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 253560.58882\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 253560.58882\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 253560.58882\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 253560.58882\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 253560.58882\n",
      "\n",
      "Epoch 00449: val_loss improved from 253560.58882 to 251859.14885, saving model to best.h5\n",
      "\n",
      "Epoch 00450: val_loss improved from 251859.14885 to 251230.36924, saving model to best.h5\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 251230.36924\n",
      "\n",
      "Epoch 00461: val_loss improved from 251230.36924 to 249321.69984, saving model to best.h5\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 249321.69984\n",
      "\n",
      "Epoch 00489: val_loss improved from 249321.69984 to 245892.50658, saving model to best.h5\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 245892.50658\n",
      "\n",
      "Epoch 00499: val_loss improved from 245892.50658 to 240790.79523, saving model to best.h5\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 240790.79523\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 240790.79523\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 240790.79523\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 240790.79523\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 240790.79523\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 240790.79523\n",
      "\n",
      "Epoch 00506: val_loss improved from 240790.79523 to 239296.51974, saving model to best.h5\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 239296.51974\n",
      "\n",
      "Epoch 00515: val_loss improved from 239296.51974 to 236673.04523, saving model to best.h5\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 236673.04523\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00557: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 236673.04523\n",
      "\n",
      "Epoch 00560: val_loss improved from 236673.04523 to 231944.72697, saving model to best.h5\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 231944.72697\n",
      "\n",
      "Epoch 00574: val_loss improved from 231944.72697 to 228732.67188, saving model to best.h5\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 228732.67188\n",
      "\n",
      "Epoch 00614: val_loss improved from 228732.67188 to 227759.97286, saving model to best.h5\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 227759.97286\n",
      "\n",
      "Epoch 00636: val_loss improved from 227759.97286 to 222472.44819, saving model to best.h5\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 222472.44819\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00699: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 222472.44819\n",
      "\n",
      "Epoch 00767: val_loss improved from 222472.44819 to 206654.69243, saving model to best.h5\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 206654.69243\n",
      "\n",
      "Epoch 00883: val_loss improved from 206654.69243 to 185532.38076, saving model to best.h5\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 185532.38076\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 185532.38076\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 185532.38076\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 185532.38076\n",
      "\n",
      "Epoch 00888: val_loss improved from 185532.38076 to 183488.04523, saving model to best.h5\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 183488.04523\n",
      "\n",
      "Epoch 00963: val_loss improved from 183488.04523 to 182440.56332, saving model to best.h5\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 182440.56332\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 182440.56332\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 182440.56332\n",
      "\n",
      "Epoch 00967: val_loss improved from 182440.56332 to 182434.64145, saving model to best.h5\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 182434.64145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00984: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 182434.64145\n",
      "\n",
      "Epoch 01011: val_loss improved from 182434.64145 to 179327.00822, saving model to best.h5\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 179327.00822\n",
      "\n",
      "Epoch 01065: val_loss improved from 179327.00822 to 178592.35197, saving model to best.h5\n",
      "\n",
      "Epoch 01066: val_loss improved from 178592.35197 to 178544.21875, saving model to best.h5\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 178544.21875\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 178544.21875\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 178544.21875\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 178544.21875\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 178544.21875\n",
      "\n",
      "Epoch 01072: val_loss improved from 178544.21875 to 178079.47451, saving model to best.h5\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 178079.47451\n",
      "\n",
      "Epoch 01092: val_loss improved from 178079.47451 to 176899.75329, saving model to best.h5\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 176899.75329\n",
      "\n",
      "Epoch 01094: val_loss improved from 176899.75329 to 176775.65707, saving model to best.h5\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 176775.65707\n",
      "\n",
      "Epoch 01113: val_loss improved from 176775.65707 to 175740.77220, saving model to best.h5\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 175740.77220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01128: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 175740.77220\n",
      "\n",
      "Epoch 01134: val_loss improved from 175740.77220 to 174948.09457, saving model to best.h5\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 174948.09457\n",
      "\n",
      "Epoch 01149: val_loss improved from 174948.09457 to 174044.85526, saving model to best.h5\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 174044.85526\n",
      "\n",
      "Epoch 01164: val_loss improved from 174044.85526 to 173539.78865, saving model to best.h5\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 173539.78865\n",
      "\n",
      "Epoch 01182: val_loss improved from 173539.78865 to 171486.77385, saving model to best.h5\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 171486.77385\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 171486.77385\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 171486.77385\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 171486.77385\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 171486.77385\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 171486.77385\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 171486.77385\n",
      "\n",
      "Epoch 01190: val_loss improved from 171486.77385 to 170147.78454, saving model to best.h5\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 170147.78454\n",
      "\n",
      "Epoch 01203: val_loss improved from 170147.78454 to 169237.42681, saving model to best.h5\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 169237.42681\n",
      "\n",
      "Epoch 01218: val_loss improved from 169237.42681 to 168536.34293, saving model to best.h5\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 168536.34293\n",
      "\n",
      "Epoch 01282: val_loss improved from 168536.34293 to 165162.88405, saving model to best.h5\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 165162.88405\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 165162.88405\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 165162.88405\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 165162.88405\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 165162.88405\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 165162.88405\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 165162.88405\n",
      "\n",
      "Epoch 01290: val_loss improved from 165162.88405 to 159881.61020, saving model to best.h5\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 159881.61020\n",
      "\n",
      "Epoch 01313: val_loss improved from 159881.61020 to 158451.50493, saving model to best.h5\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 158451.50493\n",
      "\n",
      "Epoch 01352: val_loss improved from 158451.50493 to 155200.25329, saving model to best.h5\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 155200.25329\n",
      "\n",
      "Epoch 01395: val_loss improved from 155200.25329 to 151746.73766, saving model to best.h5\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 151746.73766\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01407: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 151746.73766\n",
      "\n",
      "Epoch 01498: val_loss improved from 151746.73766 to 126453.61349, saving model to best.h5\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 126453.61349\n",
      "\n",
      "Epoch 01514: val_loss improved from 126453.61349 to 122931.27303, saving model to best.h5\n",
      "\n",
      "Epoch 01515: val_loss improved from 122931.27303 to 105714.19202, saving model to best.h5\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 105714.19202\n",
      "\n",
      "Epoch 01529: val_loss improved from 105714.19202 to 105083.93133, saving model to best.h5\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 105083.93133\n",
      "\n",
      "Epoch 01539: val_loss improved from 105083.93133 to 101782.81086, saving model to best.h5\n",
      "\n",
      "Epoch 01540: val_loss improved from 101782.81086 to 101325.20518, saving model to best.h5\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 101325.20518\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 101325.20518\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 101325.20518\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 101325.20518\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 101325.20518\n",
      "\n",
      "Epoch 01546: val_loss did not improve from 101325.20518\n",
      "\n",
      "Epoch 01547: val_loss improved from 101325.20518 to 92509.94408, saving model to best.h5\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 92509.94408\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 92509.94408\n",
      "\n",
      "Epoch 01550: val_loss improved from 92509.94408 to 88866.46094, saving model to best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01551: val_loss improved from 88866.46094 to 87936.31949, saving model to best.h5\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 87936.31949\n",
      "\n",
      "Epoch 01553: val_loss improved from 87936.31949 to 87113.37829, saving model to best.h5\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 87113.37829\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 87113.37829\n",
      "\n",
      "Epoch 01556: val_loss did not improve from 87113.37829\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 87113.37829\n",
      "\n",
      "Epoch 01558: val_loss improved from 87113.37829 to 84584.38939, saving model to best.h5\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01560: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01562: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01568: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01570: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01579: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01580: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01581: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01582: val_loss did not improve from 84584.38939\n",
      "\n",
      "Epoch 01583: val_loss improved from 84584.38939 to 79907.73396, saving model to best.h5\n",
      "\n",
      "Epoch 01584: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01585: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01586: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01587: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01588: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01589: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01590: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01591: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01592: val_loss did not improve from 79907.73396\n",
      "\n",
      "Epoch 01593: val_loss improved from 79907.73396 to 77964.58676, saving model to best.h5\n",
      "\n",
      "Epoch 01594: val_loss did not improve from 77964.58676\n",
      "\n",
      "Epoch 01595: val_loss improved from 77964.58676 to 76509.47574, saving model to best.h5\n",
      "\n",
      "Epoch 01596: val_loss improved from 76509.47574 to 76349.91324, saving model to best.h5\n",
      "\n",
      "Epoch 01597: val_loss did not improve from 76349.91324\n",
      "\n",
      "Epoch 01598: val_loss did not improve from 76349.91324\n",
      "\n",
      "Epoch 01599: val_loss did not improve from 76349.91324\n",
      "\n",
      "Epoch 01600: val_loss did not improve from 76349.91324\n",
      "\n",
      "Epoch 01601: val_loss improved from 76349.91324 to 75785.23931, saving model to best.h5\n",
      "\n",
      "Epoch 01602: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01603: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01604: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01605: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01606: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01607: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01608: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01609: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01610: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01611: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01612: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01613: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01614: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01615: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01616: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01617: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01618: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01619: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01620: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01621: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01622: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01623: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01624: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01625: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01626: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01627: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01628: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01629: val_loss did not improve from 75785.23931\n",
      "\n",
      "Epoch 01630: val_loss improved from 75785.23931 to 72124.12993, saving model to best.h5\n",
      "\n",
      "Epoch 01631: val_loss did not improve from 72124.12993\n",
      "\n",
      "Epoch 01632: val_loss did not improve from 72124.12993\n",
      "\n",
      "Epoch 01633: val_loss did not improve from 72124.12993\n",
      "\n",
      "Epoch 01634: val_loss did not improve from 72124.12993\n",
      "\n",
      "Epoch 01635: val_loss did not improve from 72124.12993\n",
      "\n",
      "Epoch 01636: val_loss did not improve from 72124.12993\n",
      "\n",
      "Epoch 01637: val_loss did not improve from 72124.12993\n",
      "\n",
      "Epoch 01638: val_loss improved from 72124.12993 to 70817.04605, saving model to best.h5\n",
      "\n",
      "Epoch 01639: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01640: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01641: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01642: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01643: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01644: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01645: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01646: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01647: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01648: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01649: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01650: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01651: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01652: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01653: val_loss did not improve from 70817.04605\n",
      "\n",
      "Epoch 01654: val_loss improved from 70817.04605 to 69330.38446, saving model to best.h5\n",
      "\n",
      "Epoch 01655: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01656: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01657: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01658: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01659: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01660: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01661: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01662: val_loss did not improve from 69330.38446\n",
      "\n",
      "Epoch 01663: val_loss improved from 69330.38446 to 69207.34910, saving model to best.h5\n",
      "\n",
      "Epoch 01664: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01665: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01666: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01667: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01668: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01669: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01670: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01671: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01672: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01673: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01674: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01675: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01676: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01677: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01678: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01679: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01680: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01681: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01682: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01683: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01684: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01685: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01686: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01687: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01688: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01689: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01690: val_loss did not improve from 69207.34910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01691: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01692: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01693: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01694: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01695: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01696: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01697: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01698: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01699: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01700: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01701: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01702: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01703: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01704: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01705: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01706: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01707: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01708: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01709: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01710: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01711: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01712: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01713: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01714: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01715: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01716: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01717: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01718: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01719: val_loss did not improve from 69207.34910\n",
      "\n",
      "Epoch 01720: val_loss improved from 69207.34910 to 65755.53988, saving model to best.h5\n",
      "\n",
      "Epoch 01721: val_loss did not improve from 65755.53988\n",
      "\n",
      "Epoch 01722: val_loss improved from 65755.53988 to 64970.09128, saving model to best.h5\n",
      "\n",
      "Epoch 01723: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01724: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01725: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01726: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01727: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01728: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01729: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01730: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01731: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01732: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01733: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01734: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01735: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01736: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01737: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01738: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01739: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01740: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01741: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01742: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01743: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01744: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01745: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01746: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01747: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01748: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01749: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01750: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01751: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01752: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01753: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01754: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01755: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01756: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01757: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01758: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01759: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01760: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01761: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01762: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01763: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01764: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01765: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01766: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01767: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01768: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01769: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01770: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01771: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01772: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01773: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01774: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01775: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01776: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01777: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01778: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01779: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01780: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01781: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01782: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01783: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01784: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01785: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01786: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01787: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01788: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01789: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01790: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01791: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01792: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01793: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01794: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01795: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01796: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01797: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01798: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01799: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01800: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01801: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01802: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01803: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01804: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01805: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01806: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01807: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01808: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01809: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01810: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01811: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01812: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01813: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01814: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01815: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01816: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01817: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01818: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01819: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01820: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01821: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01822: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01823: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01824: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01825: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01826: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01827: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01828: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01829: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01830: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01831: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01832: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01833: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01834: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01835: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01836: val_loss did not improve from 64970.09128\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01837: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01838: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01839: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01840: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01841: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01842: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01843: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01844: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01845: val_loss did not improve from 64970.09128\n",
      "\n",
      "Epoch 01846: val_loss improved from 64970.09128 to 47118.41365, saving model to best.h5\n",
      "\n",
      "Epoch 01847: val_loss did not improve from 47118.41365\n",
      "\n",
      "Epoch 01848: val_loss improved from 47118.41365 to 32464.23273, saving model to best.h5\n",
      "\n",
      "Epoch 01849: val_loss did not improve from 32464.23273\n",
      "\n",
      "Epoch 01850: val_loss improved from 32464.23273 to 29307.57823, saving model to best.h5\n",
      "\n",
      "Epoch 01851: val_loss improved from 29307.57823 to 29163.19994, saving model to best.h5\n",
      "\n",
      "Epoch 01852: val_loss did not improve from 29163.19994\n",
      "\n",
      "Epoch 01853: val_loss did not improve from 29163.19994\n",
      "\n",
      "Epoch 01854: val_loss did not improve from 29163.19994\n",
      "\n",
      "Epoch 01855: val_loss did not improve from 29163.19994\n",
      "\n",
      "Epoch 01856: val_loss did not improve from 29163.19994\n",
      "\n",
      "Epoch 01857: val_loss did not improve from 29163.19994\n",
      "\n",
      "Epoch 01858: val_loss did not improve from 29163.19994\n",
      "\n",
      "Epoch 01859: val_loss improved from 29163.19994 to 29035.38353, saving model to best.h5\n",
      "\n",
      "Epoch 01860: val_loss did not improve from 29035.38353\n",
      "\n",
      "Epoch 01861: val_loss did not improve from 29035.38353\n",
      "\n",
      "Epoch 01862: val_loss did not improve from 29035.38353\n",
      "\n",
      "Epoch 01863: val_loss did not improve from 29035.38353\n",
      "\n",
      "Epoch 01864: val_loss did not improve from 29035.38353\n",
      "\n",
      "Epoch 01865: val_loss improved from 29035.38353 to 27529.40604, saving model to best.h5\n",
      "\n",
      "Epoch 01866: val_loss improved from 27529.40604 to 26661.00041, saving model to best.h5\n",
      "\n",
      "Epoch 01867: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01868: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01869: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01870: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01871: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01872: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01873: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01874: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01875: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01876: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01877: val_loss did not improve from 26661.00041\n",
      "\n",
      "Epoch 01878: val_loss improved from 26661.00041 to 26506.24702, saving model to best.h5\n",
      "\n",
      "Epoch 01879: val_loss did not improve from 26506.24702\n",
      "\n",
      "Epoch 01880: val_loss did not improve from 26506.24702\n",
      "\n",
      "Epoch 01881: val_loss did not improve from 26506.24702\n",
      "\n",
      "Epoch 01882: val_loss did not improve from 26506.24702\n",
      "\n",
      "Epoch 01883: val_loss did not improve from 26506.24702\n",
      "\n",
      "Epoch 01884: val_loss did not improve from 26506.24702\n",
      "\n",
      "Epoch 01885: val_loss did not improve from 26506.24702\n",
      "\n",
      "Epoch 01886: val_loss improved from 26506.24702 to 26347.74537, saving model to best.h5\n",
      "\n",
      "Epoch 01887: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01888: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01889: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01890: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01891: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01892: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01893: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01894: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01895: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01896: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01897: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01898: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01899: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01900: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01901: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01902: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01903: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01904: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01905: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01906: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01907: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01908: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01909: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01910: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01911: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01912: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01913: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01914: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01915: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01916: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01917: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01918: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01919: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01920: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01921: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01922: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01923: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01924: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01925: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01926: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01927: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01928: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01929: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01930: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01931: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01932: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01933: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01934: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01935: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01936: val_loss did not improve from 26347.74537\n",
      "\n",
      "Epoch 01937: val_loss improved from 26347.74537 to 26060.89772, saving model to best.h5\n",
      "\n",
      "Epoch 01938: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01939: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01940: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01941: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01942: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01943: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01944: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01945: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01946: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01947: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01948: val_loss did not improve from 26060.89772\n",
      "\n",
      "Epoch 01949: val_loss improved from 26060.89772 to 26015.48468, saving model to best.h5\n",
      "\n",
      "Epoch 01950: val_loss did not improve from 26015.48468\n",
      "\n",
      "Epoch 01951: val_loss improved from 26015.48468 to 25928.25319, saving model to best.h5\n",
      "\n",
      "Epoch 01952: val_loss did not improve from 25928.25319\n",
      "\n",
      "Epoch 01953: val_loss did not improve from 25928.25319\n",
      "\n",
      "Epoch 01954: val_loss did not improve from 25928.25319\n",
      "\n",
      "Epoch 01955: val_loss did not improve from 25928.25319\n",
      "\n",
      "Epoch 01956: val_loss did not improve from 25928.25319\n",
      "\n",
      "Epoch 01957: val_loss did not improve from 25928.25319\n",
      "\n",
      "Epoch 01958: val_loss improved from 25928.25319 to 25805.04410, saving model to best.h5\n",
      "\n",
      "Epoch 01959: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01960: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01961: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01962: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01963: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01964: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01965: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01966: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01967: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01968: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01969: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01970: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01971: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01972: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01973: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01974: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01975: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01976: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01977: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01978: val_loss did not improve from 25805.04410\n",
      "\n",
      "Epoch 01979: val_loss improved from 25805.04410 to 25688.88579, saving model to best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01980: val_loss did not improve from 25688.88579\n",
      "\n",
      "Epoch 01981: val_loss did not improve from 25688.88579\n",
      "\n",
      "Epoch 01982: val_loss improved from 25688.88579 to 25622.16643, saving model to best.h5\n",
      "\n",
      "Epoch 01983: val_loss did not improve from 25622.16643\n",
      "\n",
      "Epoch 01984: val_loss did not improve from 25622.16643\n",
      "\n",
      "Epoch 01985: val_loss did not improve from 25622.16643\n",
      "\n",
      "Epoch 01986: val_loss did not improve from 25622.16643\n",
      "\n",
      "Epoch 01987: val_loss did not improve from 25622.16643\n",
      "\n",
      "Epoch 01988: val_loss improved from 25622.16643 to 25552.63168, saving model to best.h5\n",
      "\n",
      "Epoch 01989: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01990: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01991: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01992: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01993: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01994: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01995: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01996: val_loss did not improve from 25552.63168\n",
      "\n",
      "Epoch 01997: val_loss improved from 25552.63168 to 25502.40265, saving model to best.h5\n",
      "\n",
      "Epoch 01998: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 01999: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02000: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02001: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02002: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02003: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02004: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02005: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02006: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02007: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02008: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02009: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02010: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02011: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02012: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02013: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02014: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02015: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02016: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02017: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02018: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02019: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02020: val_loss did not improve from 25502.40265\n",
      "\n",
      "Epoch 02021: val_loss improved from 25502.40265 to 25464.23880, saving model to best.h5\n",
      "\n",
      "Epoch 02022: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02023: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02024: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02025: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02026: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02027: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02028: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02029: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02030: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02031: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02032: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02033: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02034: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02035: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02036: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02037: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02038: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02039: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02040: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02041: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02042: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02043: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02044: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02045: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02046: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02047: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02048: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02049: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02050: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02051: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02052: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02053: val_loss did not improve from 25464.23880\n",
      "\n",
      "Epoch 02054: val_loss improved from 25464.23880 to 25357.40985, saving model to best.h5\n",
      "\n",
      "Epoch 02055: val_loss did not improve from 25357.40985\n",
      "\n",
      "Epoch 02056: val_loss did not improve from 25357.40985\n",
      "\n",
      "Epoch 02057: val_loss improved from 25357.40985 to 25316.13024, saving model to best.h5\n",
      "\n",
      "Epoch 02058: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02059: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02060: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02061: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02062: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02063: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02064: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02065: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02066: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02067: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02068: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02069: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02070: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02071: val_loss did not improve from 25316.13024\n",
      "\n",
      "Epoch 02072: val_loss improved from 25316.13024 to 24981.07484, saving model to best.h5\n",
      "\n",
      "Epoch 02073: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02074: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02075: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02076: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02077: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02078: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02079: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02080: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02081: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02082: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02083: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02084: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02085: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02086: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02087: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02088: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02089: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02090: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02091: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02092: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02093: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02094: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02095: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02096: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02097: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02098: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02099: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02100: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02101: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02102: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02103: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02104: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02105: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02106: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02107: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02108: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02109: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02110: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02111: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02112: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02113: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02114: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02115: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02116: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02117: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02118: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02119: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02120: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02121: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02122: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02123: val_loss did not improve from 24981.07484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02124: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02125: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02126: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02127: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02128: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02129: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02130: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02131: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02132: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02133: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02134: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02135: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02136: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02137: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02138: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02139: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02140: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02141: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02142: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02143: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02144: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02145: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02146: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02147: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02148: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02149: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02150: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02151: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02152: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02153: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02154: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02155: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02156: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02157: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02158: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02159: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02160: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02161: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02162: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02163: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02164: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02165: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02166: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02167: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02168: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02169: val_loss did not improve from 24981.07484\n",
      "\n",
      "Epoch 02170: val_loss improved from 24981.07484 to 24534.97276, saving model to best.h5\n",
      "\n",
      "Epoch 02171: val_loss did not improve from 24534.97276\n",
      "\n",
      "Epoch 02172: val_loss did not improve from 24534.97276\n",
      "\n",
      "Epoch 02173: val_loss improved from 24534.97276 to 24431.53896, saving model to best.h5\n",
      "\n",
      "Epoch 02174: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02175: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02176: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02177: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02178: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02179: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02180: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02181: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02182: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02183: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02184: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02185: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02186: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02187: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02188: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02189: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02190: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02191: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02192: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02193: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02194: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02195: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02196: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02197: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02198: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02199: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02200: val_loss did not improve from 24431.53896\n",
      "\n",
      "Epoch 02201: val_loss improved from 24431.53896 to 24198.12500, saving model to best.h5\n",
      "\n",
      "Epoch 02202: val_loss did not improve from 24198.12500\n",
      "\n",
      "Epoch 02203: val_loss did not improve from 24198.12500\n",
      "\n",
      "Epoch 02204: val_loss did not improve from 24198.12500\n",
      "\n",
      "Epoch 02205: val_loss did not improve from 24198.12500\n",
      "\n",
      "Epoch 02206: val_loss did not improve from 24198.12500\n",
      "\n",
      "Epoch 02207: val_loss did not improve from 24198.12500\n",
      "\n",
      "Epoch 02208: val_loss did not improve from 24198.12500\n",
      "\n",
      "Epoch 02209: val_loss improved from 24198.12500 to 24145.64895, saving model to best.h5\n",
      "\n",
      "Epoch 02210: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02211: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02212: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02213: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02214: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02215: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02216: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02217: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02218: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02219: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02220: val_loss did not improve from 24145.64895\n",
      "\n",
      "Epoch 02221: val_loss improved from 24145.64895 to 23904.37377, saving model to best.h5\n",
      "\n",
      "Epoch 02222: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02223: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02224: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02225: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02226: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02227: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02228: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02229: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02230: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02231: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02232: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02233: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02234: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02235: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02236: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02237: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02238: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02239: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02240: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02241: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02242: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02243: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02244: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02245: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02246: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02247: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02248: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02249: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02250: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02251: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02252: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02253: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02254: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02255: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02256: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02257: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02258: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02259: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02260: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02261: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02262: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02263: val_loss did not improve from 23904.37377\n",
      "\n",
      "Epoch 02264: val_loss improved from 23904.37377 to 23783.68493, saving model to best.h5\n",
      "\n",
      "Epoch 02265: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02266: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02267: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02268: val_loss did not improve from 23783.68493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02269: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02270: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02271: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02272: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02273: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02274: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02275: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02276: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02277: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02278: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02279: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02280: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02281: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02282: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02283: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02284: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02285: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02286: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02287: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02288: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02289: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02290: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02291: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02292: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02293: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02294: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02295: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02296: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02297: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02298: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02299: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02300: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02301: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02302: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02303: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02304: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02305: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02306: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02307: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02308: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02309: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02310: val_loss did not improve from 23783.68493\n",
      "\n",
      "Epoch 02311: val_loss improved from 23783.68493 to 23206.07977, saving model to best.h5\n",
      "\n",
      "Epoch 02312: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02313: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02314: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02315: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02316: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02317: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02318: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02319: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02320: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02321: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02322: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02323: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02324: val_loss did not improve from 23206.07977\n",
      "\n",
      "Epoch 02325: val_loss improved from 23206.07977 to 22861.01151, saving model to best.h5\n",
      "\n",
      "Epoch 02326: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02327: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02328: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02329: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02330: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02331: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02332: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02333: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02334: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02335: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02336: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02337: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02338: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02339: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02340: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02341: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02342: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02343: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02344: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02345: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02346: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02347: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02348: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02349: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02350: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02351: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02352: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02353: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02354: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02355: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02356: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02357: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02358: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02359: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02360: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02361: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02362: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02363: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02364: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02365: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02366: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02367: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02368: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02369: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02370: val_loss did not improve from 22861.01151\n",
      "\n",
      "Epoch 02371: val_loss improved from 22861.01151 to 22819.17938, saving model to best.h5\n",
      "\n",
      "Epoch 02372: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02373: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02374: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02375: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02376: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02377: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02378: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02379: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02380: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02381: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02382: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02383: val_loss did not improve from 22819.17938\n",
      "\n",
      "Epoch 02384: val_loss improved from 22819.17938 to 22628.04862, saving model to best.h5\n",
      "\n",
      "Epoch 02385: val_loss did not improve from 22628.04862\n",
      "\n",
      "Epoch 02386: val_loss did not improve from 22628.04862\n",
      "\n",
      "Epoch 02387: val_loss did not improve from 22628.04862\n",
      "\n",
      "Epoch 02388: val_loss did not improve from 22628.04862\n",
      "\n",
      "Epoch 02389: val_loss did not improve from 22628.04862\n",
      "\n",
      "Epoch 02390: val_loss did not improve from 22628.04862\n",
      "\n",
      "Epoch 02391: val_loss improved from 22628.04862 to 22437.13744, saving model to best.h5\n",
      "\n",
      "Epoch 02392: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02393: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02394: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02395: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02396: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02397: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02398: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02399: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02400: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02401: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02402: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02403: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02404: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02405: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02406: val_loss did not improve from 22437.13744\n",
      "\n",
      "Epoch 02407: val_loss improved from 22437.13744 to 22292.95282, saving model to best.h5\n",
      "\n",
      "Epoch 02408: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02409: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02410: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02411: val_loss did not improve from 22292.95282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02412: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02413: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02414: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02415: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02416: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02417: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02418: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02419: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02420: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02421: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02422: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02423: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02424: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02425: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02426: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02427: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02428: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02429: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02430: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02431: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02432: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02433: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02434: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02435: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02436: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02437: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02438: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02439: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02440: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02441: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02442: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02443: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02444: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02445: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02446: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02447: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02448: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02449: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02450: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02451: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02452: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02453: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02454: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02455: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02456: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02457: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02458: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02459: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02460: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02461: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02462: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02463: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02464: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02465: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02466: val_loss did not improve from 22292.95282\n",
      "\n",
      "Epoch 02467: val_loss improved from 22292.95282 to 21964.66211, saving model to best.h5\n",
      "\n",
      "Epoch 02468: val_loss did not improve from 21964.66211\n",
      "\n",
      "Epoch 02469: val_loss did not improve from 21964.66211\n",
      "\n",
      "Epoch 02470: val_loss did not improve from 21964.66211\n",
      "\n",
      "Epoch 02471: val_loss did not improve from 21964.66211\n",
      "\n",
      "Epoch 02472: val_loss did not improve from 21964.66211\n",
      "\n",
      "Epoch 02473: val_loss did not improve from 21964.66211\n",
      "\n",
      "Epoch 02474: val_loss did not improve from 21964.66211\n",
      "\n",
      "Epoch 02475: val_loss improved from 21964.66211 to 21876.11071, saving model to best.h5\n",
      "\n",
      "Epoch 02476: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02477: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02478: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02479: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02480: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02481: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02482: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02483: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02484: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02485: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02486: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02487: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02488: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02489: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02490: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02491: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02492: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02493: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02494: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02495: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02496: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02497: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02498: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02499: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02500: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02501: val_loss did not improve from 21876.11071\n",
      "\n",
      "Epoch 02502: val_loss improved from 21876.11071 to 21356.13487, saving model to best.h5\n",
      "\n",
      "Epoch 02503: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02504: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02505: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02506: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02507: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02508: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02509: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02510: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02511: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02512: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02513: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02514: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02515: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02516: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02517: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02518: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02519: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02520: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02521: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02522: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02523: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02524: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02525: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02526: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02527: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02528: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02529: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02530: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02531: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02532: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02533: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02534: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02535: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02536: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02537: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02538: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02539: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02540: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02541: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02542: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02543: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02544: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02545: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02546: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02547: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02548: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02549: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02550: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02551: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02552: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02553: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02554: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02555: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02556: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02557: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02558: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02559: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02560: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02561: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02562: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02563: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02564: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02565: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02566: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02567: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02568: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02569: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02570: val_loss did not improve from 21356.13487\n",
      "\n",
      "Epoch 02571: val_loss improved from 21356.13487 to 21097.85115, saving model to best.h5\n",
      "\n",
      "Epoch 02572: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02573: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02574: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02575: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02576: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02577: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02578: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02579: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02580: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02581: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02582: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02583: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02584: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02585: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02586: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02587: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02588: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02589: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02590: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02591: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02592: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02593: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02594: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02595: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02596: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02597: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02598: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02599: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02600: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02601: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02602: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02603: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02604: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02605: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02606: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02607: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02608: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02609: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02610: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02611: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02612: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02613: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02614: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02615: val_loss did not improve from 21097.85115\n",
      "\n",
      "Epoch 02616: val_loss improved from 21097.85115 to 20113.51598, saving model to best.h5\n",
      "\n",
      "Epoch 02617: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02618: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02619: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02620: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02621: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02622: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02623: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02624: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02625: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02626: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02627: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02628: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02629: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02630: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02631: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02632: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02633: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02634: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02635: val_loss did not improve from 20113.51598\n",
      "\n",
      "Epoch 02636: val_loss improved from 20113.51598 to 19286.43159, saving model to best.h5\n",
      "\n",
      "Epoch 02637: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02638: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02639: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02640: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02641: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02642: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02643: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02644: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02645: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02646: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02647: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02648: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02649: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02650: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02651: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02652: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02653: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02654: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02655: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02656: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02657: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02658: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02659: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02660: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02661: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02662: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02663: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02664: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02665: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02666: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02667: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02668: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02669: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02670: val_loss did not improve from 19286.43159\n",
      "\n",
      "Epoch 02671: val_loss improved from 19286.43159 to 19107.77015, saving model to best.h5\n",
      "\n",
      "Epoch 02672: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02673: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02674: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02675: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02676: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02677: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02678: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02679: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02680: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02681: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02682: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02683: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02684: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02685: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02686: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02687: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02688: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02689: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02690: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02691: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02692: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02693: val_loss did not improve from 19107.77015\n",
      "\n",
      "Epoch 02694: val_loss improved from 19107.77015 to 19073.43832, saving model to best.h5\n",
      "\n",
      "Epoch 02695: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02696: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02697: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02698: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02699: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02700: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02701: val_loss did not improve from 19073.43832\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02702: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02703: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02704: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02705: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02706: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02707: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02708: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02709: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02710: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02711: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02712: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02713: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02714: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02715: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02716: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02717: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02718: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02719: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02720: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02721: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02722: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02723: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02724: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02725: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02726: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02727: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02728: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02729: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02730: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02731: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02732: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02733: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02734: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02735: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02736: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02737: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02738: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02739: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02740: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02741: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02742: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02743: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02744: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02745: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02746: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02747: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02748: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02749: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02750: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02751: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02752: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02753: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02754: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02755: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02756: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02757: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02758: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02759: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02760: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02761: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02762: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02763: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02764: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02765: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02766: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02767: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02768: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02769: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02770: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02771: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02772: val_loss did not improve from 19073.43832\n",
      "\n",
      "Epoch 02773: val_loss improved from 19073.43832 to 18437.37397, saving model to best.h5\n",
      "\n",
      "Epoch 02774: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02775: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02776: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02777: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02778: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02779: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02780: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02781: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02782: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02783: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02784: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02785: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02786: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02787: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02788: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02789: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02790: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02791: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02792: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02793: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02794: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02795: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02796: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02797: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02798: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02799: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02800: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02801: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02802: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02803: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02804: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02805: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02806: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02807: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02808: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02809: val_loss did not improve from 18437.37397\n",
      "\n",
      "Epoch 02810: val_loss improved from 18437.37397 to 16661.25725, saving model to best.h5\n",
      "\n",
      "Epoch 02811: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02812: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02813: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02814: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02815: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02816: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02817: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02818: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02819: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02820: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02821: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02822: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02823: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02824: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02825: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02826: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02827: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02828: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02829: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02830: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02831: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02832: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02833: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02834: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02835: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02836: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02837: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02838: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02839: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02840: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02841: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02842: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02843: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02844: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02845: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02846: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02847: val_loss did not improve from 16661.25725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02848: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02849: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02850: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02851: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02852: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02853: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02854: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02855: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02856: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02857: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02858: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02859: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02860: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02861: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02862: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02863: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02864: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02865: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02866: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02867: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02868: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02869: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02870: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02871: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02872: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02873: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02874: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02875: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02876: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02877: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02878: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02879: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02880: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02881: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02882: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02883: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02884: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02885: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02886: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02887: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02888: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02889: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02890: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02891: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02892: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02893: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02894: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02895: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02896: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02897: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02898: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02899: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02900: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02901: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02902: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02903: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02904: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02905: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02906: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02907: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02908: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02909: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02910: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02911: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02912: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02913: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02914: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02915: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02916: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02917: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02918: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02919: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02920: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02921: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02922: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02923: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02924: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02925: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02926: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02927: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02928: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02929: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02930: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02931: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02932: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02933: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02934: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02935: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02936: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02937: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02938: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02939: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02940: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02941: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02942: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02943: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02944: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02945: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02946: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02947: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02948: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02949: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02950: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02951: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02952: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02953: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02954: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02955: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02956: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02957: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02958: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02959: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02960: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02961: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02962: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02963: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02964: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02965: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02966: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02967: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02968: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02969: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02970: val_loss did not improve from 16661.25725\n",
      "\n",
      "Epoch 02971: val_loss improved from 16661.25725 to 16237.86595, saving model to best.h5\n",
      "\n",
      "Epoch 02972: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02973: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02974: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02975: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02976: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02977: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02978: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02979: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02980: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02981: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02982: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02983: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02984: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02985: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02986: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02987: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02988: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02989: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02990: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02991: val_loss did not improve from 16237.86595\n",
      "\n",
      "Epoch 02992: val_loss improved from 16237.86595 to 15996.19475, saving model to best.h5\n",
      "\n",
      "Epoch 02993: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 02994: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 02995: val_loss did not improve from 15996.19475\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02996: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 02997: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 02998: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 02999: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03000: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03001: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03002: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03003: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03004: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03005: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03006: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03007: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03008: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03009: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03010: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03011: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03012: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03013: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03014: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03015: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03016: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03017: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03018: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03019: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03020: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03021: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03022: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03023: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03024: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03025: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03026: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03027: val_loss did not improve from 15996.19475\n",
      "\n",
      "Epoch 03028: val_loss improved from 15996.19475 to 14061.12968, saving model to best.h5\n",
      "\n",
      "Epoch 03029: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03030: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03031: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03032: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03033: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03034: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03035: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03036: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03037: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03038: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03039: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03040: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03041: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03042: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03043: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03044: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03045: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03046: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03047: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03048: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03049: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03050: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03051: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03052: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03053: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03054: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03055: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03056: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03057: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03058: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03059: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03060: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03061: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03062: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03063: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03064: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03065: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03066: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03067: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03068: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03069: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03070: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03071: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03072: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03073: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03074: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03075: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03076: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03077: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03078: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03079: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03080: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03081: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03082: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03083: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03084: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03085: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03086: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03087: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03088: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03089: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03090: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03091: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03092: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03093: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03094: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03095: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03096: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03097: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03098: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03099: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03100: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03101: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03102: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03103: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03104: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03105: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03106: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03107: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03108: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03109: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03110: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03111: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03112: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03113: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03114: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03115: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03116: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03117: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03118: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03119: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03120: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03121: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03122: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03123: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03124: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03125: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03126: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03127: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03128: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03129: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03130: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03131: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03132: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03133: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03134: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03135: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03136: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03137: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03138: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03139: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03140: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03141: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03142: val_loss did not improve from 14061.12968\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03143: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03144: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03145: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03146: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03147: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03148: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03149: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03150: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03151: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03152: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03153: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03154: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03155: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03156: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03157: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03158: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03159: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03160: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03161: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03162: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03163: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03164: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03165: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03166: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03167: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03168: val_loss did not improve from 14061.12968\n",
      "\n",
      "Epoch 03169: val_loss improved from 14061.12968 to 13913.54302, saving model to best.h5\n",
      "\n",
      "Epoch 03170: val_loss improved from 13913.54302 to 13740.45369, saving model to best.h5\n",
      "\n",
      "Epoch 03171: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03172: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03173: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03174: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03175: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03176: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03177: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03178: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03179: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03180: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03181: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03182: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03183: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03184: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03185: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03186: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03187: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03188: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03189: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03190: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03191: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03192: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03193: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03194: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03195: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03196: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03197: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03198: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03199: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03200: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03201: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03202: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03203: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03204: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03205: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03206: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03207: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03208: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03209: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03210: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03211: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03212: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03213: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03214: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03215: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03216: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03217: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03218: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03219: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03220: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03221: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03222: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03223: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03224: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03225: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03226: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03227: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03228: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03229: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03230: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03231: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03232: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03233: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03234: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03235: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03236: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03237: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03238: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03239: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03240: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03241: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03242: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03243: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03244: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03245: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03246: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03247: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03248: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03249: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03250: val_loss did not improve from 13740.45369\n",
      "\n",
      "Epoch 03251: val_loss improved from 13740.45369 to 13163.37001, saving model to best.h5\n",
      "\n",
      "Epoch 03252: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03253: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03254: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03255: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03256: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03257: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03258: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03259: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03260: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03261: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03262: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03263: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03264: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03265: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03266: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03267: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03268: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03269: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03270: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03271: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03272: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03273: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03274: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03275: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03276: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03277: val_loss did not improve from 13163.37001\n",
      "\n",
      "Epoch 03278: val_loss improved from 13163.37001 to 12713.61123, saving model to best.h5\n",
      "\n",
      "Epoch 03279: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03280: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03281: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03282: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03283: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03284: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03285: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03286: val_loss did not improve from 12713.61123\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03287: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03288: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03289: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03290: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03291: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03292: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03293: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03294: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03295: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03296: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03297: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03298: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03299: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03300: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03301: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03302: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03303: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03304: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03305: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03306: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03307: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03308: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03309: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03310: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03311: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03312: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03313: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03314: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03315: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03316: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03317: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03318: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03319: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03320: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03321: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03322: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03323: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03324: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03325: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03326: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03327: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03328: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03329: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03330: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03331: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03332: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03333: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03334: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03335: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03336: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03337: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03338: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03339: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03340: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03341: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03342: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03343: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03344: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03345: val_loss did not improve from 12713.61123\n",
      "\n",
      "Epoch 03346: val_loss improved from 12713.61123 to 11835.15034, saving model to best.h5\n",
      "\n",
      "Epoch 03347: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03348: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03349: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03350: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03351: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03352: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03353: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03354: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03355: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03356: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03357: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03358: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03359: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03360: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03361: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03362: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03363: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03364: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03365: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03366: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03367: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03368: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03369: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03370: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03371: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03372: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03373: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03374: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03375: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03376: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03377: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03378: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03379: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03380: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03381: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03382: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03383: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03384: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03385: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03386: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03387: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03388: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03389: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03390: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03391: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03392: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03393: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03394: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03395: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03396: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03397: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03398: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03399: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03400: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03401: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03402: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03403: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03404: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03405: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03406: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03407: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03408: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03409: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03410: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03411: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03412: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03413: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03414: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03415: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03416: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03417: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03418: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03419: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03420: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03421: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03422: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03423: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03424: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03425: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03426: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03427: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03428: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03429: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03430: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03431: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03432: val_loss did not improve from 11835.15034\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03433: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03434: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03435: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03436: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03437: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03438: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03439: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03440: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03441: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03442: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03443: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03444: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03445: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03446: val_loss did not improve from 11835.15034\n",
      "\n",
      "Epoch 03447: val_loss improved from 11835.15034 to 11541.38687, saving model to best.h5\n",
      "\n",
      "Epoch 03448: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03449: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03450: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03451: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03452: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03453: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03454: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03455: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03456: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03457: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03458: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03459: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03460: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03461: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03462: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03463: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03464: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03465: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03466: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03467: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03468: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03469: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03470: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03471: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03472: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03473: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03474: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03475: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03476: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03477: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03478: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03479: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03480: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03481: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03482: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03483: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03484: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03485: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03486: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03487: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03488: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03489: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03490: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03491: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03492: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03493: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03494: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03495: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03496: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03497: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03498: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03499: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03500: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03501: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03502: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03503: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03504: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03505: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03506: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03507: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03508: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03509: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03510: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03511: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03512: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03513: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03514: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03515: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03516: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03517: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03518: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03519: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03520: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03521: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03522: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03523: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03524: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03525: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03526: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03527: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03528: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03529: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03530: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03531: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03532: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03533: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03534: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03535: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03536: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03537: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03538: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03539: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03540: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03541: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03542: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03543: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03544: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03545: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03546: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03547: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03548: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03549: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03550: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03551: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03552: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03553: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03554: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03555: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03556: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03557: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03558: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03559: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03560: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03561: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03562: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03563: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03564: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03565: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03566: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03567: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03568: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03569: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03570: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03571: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03572: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03573: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03574: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03575: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03576: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03577: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03578: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03579: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03580: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03581: val_loss did not improve from 11541.38687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 03582: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03583: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03584: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03585: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03586: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03587: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03588: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03589: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03590: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03591: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03592: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03593: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03594: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03595: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03596: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03597: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03598: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03599: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03600: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03601: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03602: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03603: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03604: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03605: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03606: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03607: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03608: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03609: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03610: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03611: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03612: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03613: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03614: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03615: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03616: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03617: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03618: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03619: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03620: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03621: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03622: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03623: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03624: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03625: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03626: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03627: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03628: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03629: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03630: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03631: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03632: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03633: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03634: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03635: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03636: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03637: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03638: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03639: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03640: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03641: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03642: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03643: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03644: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03645: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03646: val_loss did not improve from 11541.38687\n",
      "\n",
      "Epoch 03647: val_loss did not improve from 11541.38687\n",
      "Epoch 03647: early stopping\n",
      "22\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 599289733551.15784, saving model to best.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 599289733551.15784 to 59583179075.36842, saving model to best.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 59583179075.36842\n",
      "\n",
      "Epoch 00004: val_loss improved from 59583179075.36842 to 3189771223.57895, saving model to best.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3189771223.57895\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3189771223.57895\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3189771223.57895\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3189771223.57895\n",
      "\n",
      "Epoch 00009: val_loss improved from 3189771223.57895 to 1445523610.94737, saving model to best.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1445523610.94737\n",
      "\n",
      "Epoch 00011: val_loss improved from 1445523610.94737 to 132807252.63158, saving model to best.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 132807252.63158\n",
      "\n",
      "Epoch 00013: val_loss improved from 132807252.63158 to 5325374.81579, saving model to best.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 5325374.81579\n",
      "\n",
      "Epoch 00024: val_loss improved from 5325374.81579 to 2425448.22368, saving model to best.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2425448.22368\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2425448.22368\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2425448.22368\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2425448.22368\n",
      "\n",
      "Epoch 00029: val_loss improved from 2425448.22368 to 1268504.28947, saving model to best.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1268504.28947\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1268504.28947\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1268504.28947\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1268504.28947\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1268504.28947\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1268504.28947\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1268504.28947\n",
      "\n",
      "Epoch 00037: val_loss improved from 1268504.28947 to 1138700.84868, saving model to best.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1138700.84868\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1138700.84868\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1138700.84868\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1138700.84868\n",
      "\n",
      "Epoch 00042: val_loss improved from 1138700.84868 to 1102656.63158, saving model to best.h5\n",
      "\n",
      "Epoch 00043: val_loss improved from 1102656.63158 to 1070721.44079, saving model to best.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1070721.44079\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1070721.44079\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1070721.44079\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1070721.44079\n",
      "\n",
      "Epoch 00048: val_loss improved from 1070721.44079 to 1042567.96711, saving model to best.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 1042567.96711 to 1039499.61842, saving model to best.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1039499.61842\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1039499.61842\n",
      "\n",
      "Epoch 00052: val_loss improved from 1039499.61842 to 1032268.20395, saving model to best.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 1032268.20395 to 1019179.11184, saving model to best.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1019179.11184\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1019179.11184\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1019179.11184\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1019179.11184\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1019179.11184\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1019179.11184\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1019179.11184\n",
      "\n",
      "Epoch 00061: val_loss improved from 1019179.11184 to 976184.57237, saving model to best.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 976184.57237\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 976184.57237\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 976184.57237\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 976184.57237\n",
      "\n",
      "Epoch 00066: val_loss improved from 976184.57237 to 944517.13158, saving model to best.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 944517.13158\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 944517.13158\n",
      "\n",
      "Epoch 00069: val_loss improved from 944517.13158 to 905622.74342, saving model to best.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 905622.74342\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 905622.74342\n",
      "\n",
      "Epoch 00072: val_loss improved from 905622.74342 to 885494.76974, saving model to best.h5\n",
      "\n",
      "Epoch 00073: val_loss improved from 885494.76974 to 857794.12500, saving model to best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00074: val_loss did not improve from 857794.12500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 857794.12500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 857794.12500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 857794.12500\n",
      "\n",
      "Epoch 00078: val_loss improved from 857794.12500 to 822201.63487, saving model to best.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 822201.63487\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 822201.63487\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 822201.63487\n",
      "\n",
      "Epoch 00082: val_loss improved from 822201.63487 to 772881.57566, saving model to best.h5\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 772881.57566\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 772881.57566\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 772881.57566\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 772881.57566\n",
      "\n",
      "Epoch 00087: val_loss improved from 772881.57566 to 725217.43092, saving model to best.h5\n",
      "\n",
      "Epoch 00088: val_loss improved from 725217.43092 to 715722.28947, saving model to best.h5\n",
      "\n",
      "Epoch 00089: val_loss improved from 715722.28947 to 715069.75000, saving model to best.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 715069.75000\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 715069.75000\n",
      "\n",
      "Epoch 00092: val_loss improved from 715069.75000 to 689873.97368, saving model to best.h5\n",
      "\n",
      "Epoch 00093: val_loss improved from 689873.97368 to 683484.55592, saving model to best.h5\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 683484.55592\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 683484.55592\n",
      "\n",
      "Epoch 00096: val_loss improved from 683484.55592 to 668705.96053, saving model to best.h5\n",
      "\n",
      "Epoch 00097: val_loss improved from 668705.96053 to 655336.16776, saving model to best.h5\n",
      "\n",
      "Epoch 00098: val_loss improved from 655336.16776 to 643813.87829, saving model to best.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 643813.87829\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 643813.87829\n",
      "\n",
      "Epoch 00101: val_loss improved from 643813.87829 to 635164.69737, saving model to best.h5\n",
      "\n",
      "Epoch 00102: val_loss improved from 635164.69737 to 592696.38158, saving model to best.h5\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 592696.38158\n",
      "\n",
      "Epoch 00104: val_loss improved from 592696.38158 to 587685.40461, saving model to best.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 587685.40461 to 581216.14803, saving model to best.h5\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 581216.14803\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 581216.14803\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 581216.14803\n",
      "\n",
      "Epoch 00109: val_loss improved from 581216.14803 to 534915.03618, saving model to best.h5\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 534915.03618\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 534915.03618\n",
      "\n",
      "Epoch 00112: val_loss improved from 534915.03618 to 516152.12171, saving model to best.h5\n",
      "\n",
      "Epoch 00113: val_loss improved from 516152.12171 to 504441.83224, saving model to best.h5\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 504441.83224\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 504441.83224\n",
      "\n",
      "Epoch 00116: val_loss improved from 504441.83224 to 503972.63487, saving model to best.h5\n",
      "\n",
      "Epoch 00117: val_loss improved from 503972.63487 to 481594.95395, saving model to best.h5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 481594.95395\n",
      "\n",
      "Epoch 00119: val_loss improved from 481594.95395 to 471821.37171, saving model to best.h5\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 471821.37171\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 471821.37171\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 471821.37171\n",
      "\n",
      "Epoch 00123: val_loss improved from 471821.37171 to 439140.04605, saving model to best.h5\n",
      "\n",
      "Epoch 00124: val_loss improved from 439140.04605 to 435187.91118, saving model to best.h5\n",
      "\n",
      "Epoch 00125: val_loss improved from 435187.91118 to 434451.25658, saving model to best.h5\n",
      "\n",
      "Epoch 00126: val_loss improved from 434451.25658 to 427913.73355, saving model to best.h5\n",
      "\n",
      "Epoch 00127: val_loss improved from 427913.73355 to 420055.48684, saving model to best.h5\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 420055.48684\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 420055.48684\n",
      "\n",
      "Epoch 00130: val_loss improved from 420055.48684 to 408784.99178, saving model to best.h5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 408784.99178\n",
      "\n",
      "Epoch 00132: val_loss improved from 408784.99178 to 397522.38651, saving model to best.h5\n",
      "\n",
      "Epoch 00133: val_loss improved from 397522.38651 to 394033.62500, saving model to best.h5\n",
      "\n",
      "Epoch 00134: val_loss improved from 394033.62500 to 384138.49507, saving model to best.h5\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 384138.49507\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 384138.49507\n",
      "\n",
      "Epoch 00137: val_loss improved from 384138.49507 to 383167.15625, saving model to best.h5\n",
      "\n",
      "Epoch 00138: val_loss improved from 383167.15625 to 371326.65954, saving model to best.h5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 371326.65954\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 371326.65954\n",
      "\n",
      "Epoch 00141: val_loss improved from 371326.65954 to 368793.27632, saving model to best.h5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 368793.27632\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 368793.27632\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 368793.27632\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 368793.27632\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 368793.27632\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 368793.27632\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 368793.27632\n",
      "\n",
      "Epoch 00149: val_loss improved from 368793.27632 to 334482.17270, saving model to best.h5\n",
      "\n",
      "Epoch 00150: val_loss improved from 334482.17270 to 330667.69901, saving model to best.h5\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 330667.69901\n",
      "\n",
      "Epoch 00152: val_loss improved from 330667.69901 to 326876.41447, saving model to best.h5\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 326876.41447\n",
      "\n",
      "Epoch 00154: val_loss improved from 326876.41447 to 325892.54441, saving model to best.h5\n",
      "\n",
      "Epoch 00155: val_loss improved from 325892.54441 to 318656.78947, saving model to best.h5\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 318656.78947\n",
      "\n",
      "Epoch 00175: val_loss improved from 318656.78947 to 308169.07237, saving model to best.h5\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 308169.07237\n",
      "\n",
      "Epoch 00177: val_loss improved from 308169.07237 to 275666.64967, saving model to best.h5\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 275666.64967\n",
      "\n",
      "Epoch 00187: val_loss improved from 275666.64967 to 257125.40789, saving model to best.h5\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 257125.40789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00196: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 257125.40789\n",
      "\n",
      "Epoch 00198: val_loss improved from 257125.40789 to 248144.32401, saving model to best.h5\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 248144.32401\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 248144.32401\n",
      "\n",
      "Epoch 00201: val_loss improved from 248144.32401 to 233003.29441, saving model to best.h5\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 233003.29441\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 233003.29441\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 233003.29441\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 233003.29441\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 233003.29441\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 233003.29441\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 233003.29441\n",
      "\n",
      "Epoch 00209: val_loss improved from 233003.29441 to 224765.39145, saving model to best.h5\n",
      "\n",
      "Epoch 00210: val_loss improved from 224765.39145 to 221195.22697, saving model to best.h5\n",
      "\n",
      "Epoch 00211: val_loss improved from 221195.22697 to 216805.35691, saving model to best.h5\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 216805.35691\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 216805.35691\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 216805.35691\n",
      "\n",
      "Epoch 00215: val_loss improved from 216805.35691 to 213987.13980, saving model to best.h5\n",
      "\n",
      "Epoch 00216: val_loss improved from 213987.13980 to 209245.08388, saving model to best.h5\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 209245.08388\n",
      "\n",
      "Epoch 00225: val_loss improved from 209245.08388 to 199595.13405, saving model to best.h5\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 199595.13405\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 199595.13405\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 199595.13405\n",
      "\n",
      "Epoch 00229: val_loss improved from 199595.13405 to 194838.33306, saving model to best.h5\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 194838.33306\n",
      "\n",
      "Epoch 00231: val_loss improved from 194838.33306 to 188753.49671, saving model to best.h5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 188753.49671\n",
      "\n",
      "Epoch 00240: val_loss improved from 188753.49671 to 176630.66530, saving model to best.h5\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 176630.66530\n",
      "\n",
      "Epoch 00242: val_loss improved from 176630.66530 to 174636.37582, saving model to best.h5\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 174636.37582\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 174636.37582\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 174636.37582\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 174636.37582\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 174636.37582\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 174636.37582\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 174636.37582\n",
      "\n",
      "Epoch 00250: val_loss improved from 174636.37582 to 163755.09868, saving model to best.h5\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 163755.09868\n",
      "\n",
      "Epoch 00252: val_loss improved from 163755.09868 to 161468.73931, saving model to best.h5\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 161468.73931\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 161468.73931\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 161468.73931\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 161468.73931\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 161468.73931\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 161468.73931\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 161468.73931\n",
      "\n",
      "Epoch 00260: val_loss improved from 161468.73931 to 154102.11595, saving model to best.h5\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 154102.11595\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 154102.11595\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 154102.11595\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 154102.11595\n",
      "\n",
      "Epoch 00265: val_loss improved from 154102.11595 to 147443.87911, saving model to best.h5\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 147443.87911\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 147443.87911\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 147443.87911\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 147443.87911\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 147443.87911\n",
      "\n",
      "Epoch 00271: val_loss improved from 147443.87911 to 143227.29523, saving model to best.h5\n",
      "\n",
      "Epoch 00272: val_loss improved from 143227.29523 to 141886.28947, saving model to best.h5\n",
      "\n",
      "Epoch 00273: val_loss improved from 141886.28947 to 141373.67023, saving model to best.h5\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 141373.67023\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 141373.67023\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 141373.67023\n",
      "\n",
      "Epoch 00277: val_loss improved from 141373.67023 to 138076.66365, saving model to best.h5\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 138076.66365\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 138076.66365\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 138076.66365\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 138076.66365\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 138076.66365\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 138076.66365\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 138076.66365\n",
      "\n",
      "Epoch 00285: val_loss improved from 138076.66365 to 132521.75411, saving model to best.h5\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 132521.75411\n",
      "\n",
      "Epoch 00287: val_loss improved from 132521.75411 to 131349.25247, saving model to best.h5\n",
      "\n",
      "Epoch 00288: val_loss improved from 131349.25247 to 124250.78043, saving model to best.h5\n",
      "\n",
      "Epoch 00289: val_loss improved from 124250.78043 to 122890.78454, saving model to best.h5\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 122890.78454\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 122890.78454\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 122890.78454\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 122890.78454\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 122890.78454\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 122890.78454\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 122890.78454\n",
      "\n",
      "Epoch 00297: val_loss improved from 122890.78454 to 118995.77220, saving model to best.h5\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 118995.77220\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 118995.77220\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 118995.77220\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 118995.77220\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 118995.77220\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 118995.77220\n",
      "\n",
      "Epoch 00304: val_loss improved from 118995.77220 to 110941.46875, saving model to best.h5\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 110941.46875\n",
      "\n",
      "Epoch 00314: val_loss improved from 110941.46875 to 107123.71258, saving model to best.h5\n",
      "\n",
      "Epoch 00315: val_loss improved from 107123.71258 to 102037.92804, saving model to best.h5\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 102037.92804\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 102037.92804\n",
      "\n",
      "Epoch 00318: val_loss improved from 102037.92804 to 101174.12993, saving model to best.h5\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 101174.12993\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 101174.12993\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 101174.12993\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 101174.12993\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 101174.12993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00324: val_loss improved from 101174.12993 to 95950.00822, saving model to best.h5\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 95950.00822\n",
      "\n",
      "Epoch 00334: val_loss improved from 95950.00822 to 90534.57854, saving model to best.h5\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 90534.57854\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 90534.57854\n",
      "\n",
      "Epoch 00337: val_loss improved from 90534.57854 to 88510.92928, saving model to best.h5\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 88510.92928\n",
      "\n",
      "Epoch 00350: val_loss improved from 88510.92928 to 86766.59169, saving model to best.h5\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 86766.59169\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 86766.59169\n",
      "\n",
      "Epoch 00353: val_loss improved from 86766.59169 to 79309.35321, saving model to best.h5\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 79309.35321\n",
      "\n",
      "Epoch 00364: val_loss improved from 79309.35321 to 75037.07689, saving model to best.h5\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 75037.07689\n",
      "\n",
      "Epoch 00366: val_loss improved from 75037.07689 to 74140.76316, saving model to best.h5\n",
      "\n",
      "Epoch 00367: val_loss improved from 74140.76316 to 73641.88281, saving model to best.h5\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 73641.88281\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 73641.88281\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 73641.88281\n",
      "\n",
      "Epoch 00371: val_loss improved from 73641.88281 to 72868.05222, saving model to best.h5\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 72868.05222\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 72868.05222\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 72868.05222\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 72868.05222\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 72868.05222\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 72868.05222\n",
      "\n",
      "Epoch 00378: val_loss improved from 72868.05222 to 69414.82196, saving model to best.h5\n",
      "\n",
      "Epoch 00379: val_loss improved from 69414.82196 to 69116.97903, saving model to best.h5\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 69116.97903\n",
      "\n",
      "Epoch 00388: val_loss improved from 69116.97903 to 67235.45354, saving model to best.h5\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 67235.45354\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 67235.45354\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 67235.45354\n",
      "\n",
      "Epoch 00392: val_loss improved from 67235.45354 to 66755.14391, saving model to best.h5\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 66755.14391\n",
      "\n",
      "Epoch 00394: val_loss improved from 66755.14391 to 64825.37130, saving model to best.h5\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 64825.37130\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 64825.37130\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 64825.37130\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 64825.37130\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 64825.37130\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 64825.37130\n",
      "\n",
      "Epoch 00401: val_loss improved from 64825.37130 to 63172.01686, saving model to best.h5\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 63172.01686\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 63172.01686\n",
      "\n",
      "Epoch 00404: val_loss improved from 63172.01686 to 62715.41242, saving model to best.h5\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 62715.41242\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 62715.41242\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 62715.41242\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 62715.41242\n",
      "\n",
      "Epoch 00409: val_loss improved from 62715.41242 to 62408.40255, saving model to best.h5\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 62408.40255\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 62408.40255\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 62408.40255\n",
      "\n",
      "Epoch 00413: val_loss improved from 62408.40255 to 59144.45724, saving model to best.h5\n",
      "\n",
      "Epoch 00414: val_loss improved from 59144.45724 to 59144.01974, saving model to best.h5\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 59144.01974\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 59144.01974\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 59144.01974\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 59144.01974\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 59144.01974\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 59144.01974\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 59144.01974\n",
      "\n",
      "Epoch 00422: val_loss improved from 59144.01974 to 56809.05160, saving model to best.h5\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 56809.05160\n",
      "\n",
      "Epoch 00454: val_loss improved from 56809.05160 to 52115.17660, saving model to best.h5\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 52115.17660\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 52115.17660\n",
      "\n",
      "Epoch 00457: val_loss improved from 52115.17660 to 51466.51460, saving model to best.h5\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 51466.51460\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 51466.51460\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 51466.51460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00461: val_loss improved from 51466.51460 to 50695.92023, saving model to best.h5\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 50695.92023\n",
      "\n",
      "Epoch 00476: val_loss improved from 50695.92023 to 50123.26090, saving model to best.h5\n",
      "\n",
      "Epoch 00477: val_loss improved from 50123.26090 to 48774.52714, saving model to best.h5\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 48774.52714\n",
      "\n",
      "Epoch 00487: val_loss improved from 48774.52714 to 48735.38117, saving model to best.h5\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 48735.38117\n",
      "\n",
      "Epoch 00498: val_loss improved from 48735.38117 to 47253.16160, saving model to best.h5\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 47253.16160\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 47253.16160\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 47253.16160\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 47253.16160\n",
      "\n",
      "Epoch 00503: val_loss improved from 47253.16160 to 46669.80695, saving model to best.h5\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 46669.80695\n",
      "\n",
      "Epoch 00536: val_loss improved from 46669.80695 to 44921.33861, saving model to best.h5\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 44921.33861\n",
      "\n",
      "Epoch 00552: val_loss improved from 44921.33861 to 44575.96772, saving model to best.h5\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 44575.96772\n",
      "\n",
      "Epoch 00572: val_loss improved from 44575.96772 to 44524.15060, saving model to best.h5\n",
      "\n",
      "Epoch 00573: val_loss improved from 44524.15060 to 43975.37222, saving model to best.h5\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 43975.37222\n",
      "\n",
      "Epoch 00591: val_loss improved from 43975.37222 to 43157.04564, saving model to best.h5\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 43157.04564\n",
      "\n",
      "Epoch 00639: val_loss improved from 43157.04564 to 42313.27436, saving model to best.h5\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 42313.27436\n",
      "\n",
      "Epoch 00680: val_loss improved from 42313.27436 to 42033.80561, saving model to best.h5\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 42033.80561\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 42033.80561\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 42033.80561\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 42033.80561\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 42033.80561\n",
      "\n",
      "Epoch 00686: val_loss improved from 42033.80561 to 41641.49147, saving model to best.h5\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 41641.49147\n",
      "\n",
      "Epoch 00724: val_loss improved from 41641.49147 to 41144.27704, saving model to best.h5\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 41144.27704\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00743: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 41144.27704\n",
      "\n",
      "Epoch 00781: val_loss improved from 41144.27704 to 40201.15779, saving model to best.h5\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 40201.15779\n",
      "\n",
      "Epoch 00822: val_loss improved from 40201.15779 to 39849.59262, saving model to best.h5\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 39849.59262\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00891: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 39849.59262\n",
      "\n",
      "Epoch 01002: val_loss improved from 39849.59262 to 28400.80474, saving model to best.h5\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 28400.80474\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 28400.80474\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 28400.80474\n",
      "\n",
      "Epoch 01006: val_loss improved from 28400.80474 to 27084.59832, saving model to best.h5\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 27084.59832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01040: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 27084.59832\n",
      "\n",
      "Epoch 01163: val_loss improved from 27084.59832 to 26830.62346, saving model to best.h5\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 26830.62346\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 26830.62346\n",
      "\n",
      "Epoch 01166: val_loss improved from 26830.62346 to 26663.44912, saving model to best.h5\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 26663.44912\n",
      "\n",
      "Epoch 01168: val_loss improved from 26663.44912 to 26546.28248, saving model to best.h5\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 26546.28248\n",
      "\n",
      "Epoch 01181: val_loss improved from 26546.28248 to 24990.98119, saving model to best.h5\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 24990.98119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01185: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 24990.98119\n",
      "\n",
      "Epoch 01194: val_loss improved from 24990.98119 to 24216.05962, saving model to best.h5\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 24216.05962\n",
      "\n",
      "Epoch 01211: val_loss improved from 24216.05962 to 23265.59169, saving model to best.h5\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 23265.59169\n",
      "\n",
      "Epoch 01223: val_loss improved from 23265.59169 to 23038.42475, saving model to best.h5\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 23038.42475\n",
      "\n",
      "Epoch 01238: val_loss improved from 23038.42475 to 22782.72070, saving model to best.h5\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 22782.72070\n",
      "\n",
      "Epoch 01267: val_loss improved from 22782.72070 to 22692.18822, saving model to best.h5\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 22692.18822\n",
      "\n",
      "Epoch 01310: val_loss improved from 22692.18822 to 22675.35434, saving model to best.h5\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 22675.35434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01330: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 22675.35434\n",
      "\n",
      "Epoch 01428: val_loss improved from 22675.35434 to 22433.65584, saving model to best.h5\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 22433.65584\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01478: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01501: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01502: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01503: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01504: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01505: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01506: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01507: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01508: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01509: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01510: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01511: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01512: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01513: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01514: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01515: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01516: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01517: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01518: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01519: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01520: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01521: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01522: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01523: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01524: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01525: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01526: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01527: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01528: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01529: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01530: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01531: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01532: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01533: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01534: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01535: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01536: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01537: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01538: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01539: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01540: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01541: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01542: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01543: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01544: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01545: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01546: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01547: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01548: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01549: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01550: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01551: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01552: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01553: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01554: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01555: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01556: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01557: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01558: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01559: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01560: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01561: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01562: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01563: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01564: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01565: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01566: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01567: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01568: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01569: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01570: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01571: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01572: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01573: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01574: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01575: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01576: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01577: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01578: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01579: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01580: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01581: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01582: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01583: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01584: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01585: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01586: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01587: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01588: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01589: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01590: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01591: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01592: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01593: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01594: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01595: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01596: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01597: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01598: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01599: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01600: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01601: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01602: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01603: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01604: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01605: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01606: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01607: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01608: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01609: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01610: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01611: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01612: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01613: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01614: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01615: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01616: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01617: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01618: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01619: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01620: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01621: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01622: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01623: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01624: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01625: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01626: val_loss did not improve from 22433.65584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01627: val_loss did not improve from 22433.65584\n",
      "\n",
      "Epoch 01628: val_loss did not improve from 22433.65584\n",
      "Epoch 01628: early stopping\n",
      "23\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss=[0]\n",
    "val_loss=[0]\n",
    "\n",
    "for x in range(19,24):\n",
    "    dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_\"+str(x)+\".csv\", header=None)\n",
    "    dataset = dataframe.values\n",
    "\n",
    "    X = dataset[:,0:x*23]\n",
    "    y = dataset[:,x*23]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=29)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(x*23, input_dim=x*23, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(x*23, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(x*23, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    numpy.random.seed(29)    \n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "    mc = ModelCheckpoint('best.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)    \n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10000, verbose=0, callbacks=[es,mc])\n",
    "\n",
    "    loss = numpy.append(loss, min(history.history['loss']))\n",
    "    val_loss = numpy.append(val_loss, min(history.history['val_loss']))\n",
    "\n",
    "    saved_model = load_model('best.h5')\n",
    "    tot_loss = numpy.append(tot_loss,metrics.mean_squared_error(y,saved_model.predict(X)))\n",
    "    \n",
    "    print(x)\n",
    "\n",
    "    \n",
    "loss = numpy.delete(loss,0)\n",
    "val_loss = numpy.delete(val_loss,0)\n",
    "plt.plot(loss, label='train')\n",
    "plt.plot(val_loss, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62 samples, validate on 31 samples\n",
      "Epoch 1/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 607304265067.3547 - val_loss: 120317403136.0000\n",
      "Epoch 2/10000\n",
      "62/62 [==============================] - 0s 129us/step - loss: 103461455673.8064 - val_loss: 267369414656.0000\n",
      "Epoch 3/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 234195675466.3226 - val_loss: 60262064128.0000\n",
      "Epoch 4/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 31070112310.7097 - val_loss: 33947230208.0000\n",
      "Epoch 5/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 47511501658.8387 - val_loss: 46556745728.0000\n",
      "Epoch 6/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 31295656761.8064 - val_loss: 133140544.0000\n",
      "Epoch 7/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 4414022272.0000 - val_loss: 22201583616.0000\n",
      "Epoch 8/10000\n",
      "62/62 [==============================] - 0s 105us/step - loss: 20619624117.6774 - val_loss: 8379373056.0000\n",
      "Epoch 9/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 4649020420.1290 - val_loss: 1318700032.0000\n",
      "Epoch 10/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 3731935083.3548 - val_loss: 8624617472.0000\n",
      "Epoch 11/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 7544577007.4839 - val_loss: 2345235200.0000\n",
      "Epoch 12/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 1221737024.3871 - val_loss: 1168866432.0000\n",
      "Epoch 13/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 2315699662.4516 - val_loss: 4151571968.0000\n",
      "Epoch 14/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 3408443804.9032 - val_loss: 688842048.0000\n",
      "Epoch 15/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 361913314.3226 - val_loss: 721520832.0000\n",
      "Epoch 16/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 1201858171.8710 - val_loss: 1929651072.0000\n",
      "Epoch 17/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 1598891875.0968 - val_loss: 370854080.0000\n",
      "Epoch 18/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 192093937.8935 - val_loss: 307403968.0000\n",
      "Epoch 19/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 535737984.0000 - val_loss: 905146112.0000\n",
      "Epoch 20/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 750423339.3548 - val_loss: 156044944.0000\n",
      "Epoch 21/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 81395083.5565 - val_loss: 186693536.0000\n",
      "Epoch 22/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 307606832.5161 - val_loss: 468026688.0000\n",
      "Epoch 23/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 369217840.5161 - val_loss: 51007032.0000\n",
      "Epoch 24/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 31337048.9677 - val_loss: 133501488.0000\n",
      "Epoch 25/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 192315616.0000 - val_loss: 241800912.0000\n",
      "Epoch 26/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 182618780.3871 - val_loss: 13341101.0000\n",
      "Epoch 27/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 14480938.5484 - val_loss: 94011216.0000\n",
      "Epoch 28/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 119649422.9677 - val_loss: 119824752.0000\n",
      "Epoch 29/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 82668094.1935 - val_loss: 1183475.3750\n",
      "Epoch 30/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 10027959.1613 - val_loss: 65390508.0000\n",
      "Epoch 31/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 73691490.0645 - val_loss: 52340564.0000\n",
      "Epoch 32/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 33221865.4194 - val_loss: 1116955.2500\n",
      "Epoch 33/10000\n",
      "62/62 [==============================] - 0s 129us/step - loss: 11155795.5484 - val_loss: 45489552.0000\n",
      "Epoch 34/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 43392926.5806 - val_loss: 18796526.0000\n",
      "Epoch 35/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 9938759.3347 - val_loss: 5106179.5000\n",
      "Epoch 36/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 12678080.6452 - val_loss: 26544890.0000\n",
      "Epoch 37/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 22128223.4194 - val_loss: 3449148.2500\n",
      "Epoch 38/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 2219071.2198 - val_loss: 8839246.0000\n",
      "Epoch 39/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 11988352.9355 - val_loss: 13025071.0000\n",
      "Epoch 40/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 8669093.6290 - val_loss: 101294.0703\n",
      "Epoch 41/10000\n",
      "62/62 [==============================] - 0s 104us/step - loss: 1585989.3085 - val_loss: 8170034.0000\n",
      "Epoch 42/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 8320919.7903 - val_loss: 3617598.7500\n",
      "Epoch 43/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 1999629.6593 - val_loss: 1397503.6250\n",
      "Epoch 44/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 2853917.9355 - val_loss: 5439441.5000\n",
      "Epoch 45/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 3105725.5343 - val_loss: 2334642.2500\n",
      "Epoch 46/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 1541421.7379 - val_loss: 136947.0000\n",
      "Epoch 47/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 191200.9718 - val_loss: 196387.8594\n",
      "Epoch 48/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 107170.4982 - val_loss: 141506.7344\n",
      "Epoch 49/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 178451.7681 - val_loss: 214688.2031\n",
      "Epoch 50/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 179373.5325 - val_loss: 97646.9766\n",
      "Epoch 51/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76030.7810 - val_loss: 199729.0312\n",
      "Epoch 52/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 160270.1951 - val_loss: 149038.1250\n",
      "Epoch 53/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 98867.8634 - val_loss: 105566.0938\n",
      "Epoch 54/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 109973.7898 - val_loss: 130463.5312\n",
      "Epoch 55/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 110408.1598 - val_loss: 97357.4297\n",
      "Epoch 56/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 80384.2019 - val_loss: 145843.3281\n",
      "Epoch 57/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 109343.2412 - val_loss: 115030.0625\n",
      "Epoch 58/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 83798.3228 - val_loss: 99151.4141\n",
      "Epoch 59/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 93908.4584 - val_loss: 104100.2422\n",
      "Epoch 60/10000\n",
      "62/62 [==============================] - 0s 101us/step - loss: 85558.2107 - val_loss: 99254.8906\n",
      "Epoch 61/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 83062.9982 - val_loss: 126472.6562\n",
      "Epoch 62/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 93662.2278 - val_loss: 100777.1016\n",
      "Epoch 63/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 85794.2939 - val_loss: 98811.5547\n",
      "Epoch 64/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 83847.6429 - val_loss: 95893.7500\n",
      "Epoch 65/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 84252.5562 - val_loss: 106092.7500\n",
      "Epoch 66/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 81262.5073 - val_loss: 99741.5703\n",
      "Epoch 67/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 81178.9569 - val_loss: 96259.8672\n",
      "Epoch 68/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 80550.6502 - val_loss: 95506.1094\n",
      "Epoch 69/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 77073.4619 - val_loss: 102212.0469\n",
      "Epoch 70/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 79321.0439 - val_loss: 101060.5859\n",
      "Epoch 71/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 97us/step - loss: 76799.6013 - val_loss: 95288.1797\n",
      "Epoch 72/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76705.0204 - val_loss: 95069.7188\n",
      "Epoch 73/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 79283.2342 - val_loss: 96036.3125\n",
      "Epoch 74/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 76265.5154 - val_loss: 96373.7578\n",
      "Epoch 75/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76167.0144 - val_loss: 96700.7578\n",
      "Epoch 76/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 76170.5454 - val_loss: 96410.1406\n",
      "Epoch 77/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 76271.0275 - val_loss: 95773.1016\n",
      "Epoch 78/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76245.4635 - val_loss: 96580.6328\n",
      "Epoch 79/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 75756.0648 - val_loss: 94935.3359\n",
      "Epoch 80/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 75944.1575 - val_loss: 94618.9688\n",
      "Epoch 81/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 75521.2394 - val_loss: 95835.3516\n",
      "Epoch 82/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 75365.8027 - val_loss: 96767.5391\n",
      "Epoch 83/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 75812.4446 - val_loss: 95178.8125\n",
      "Epoch 84/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 75728.8551 - val_loss: 95101.1484\n",
      "Epoch 85/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 76664.1399 - val_loss: 93656.9609\n",
      "Epoch 86/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 77899.4864 - val_loss: 94489.3281\n",
      "Epoch 87/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 74505.9995 - val_loss: 100651.0859\n",
      "Epoch 88/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 77079.6983 - val_loss: 95952.3047\n",
      "Epoch 89/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 74510.6870 - val_loss: 93186.6641\n",
      "Epoch 90/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 80293.8017 - val_loss: 92931.9141\n",
      "Epoch 91/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 74307.6638 - val_loss: 98312.7656\n",
      "Epoch 92/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 75879.2341 - val_loss: 99517.3984\n",
      "Epoch 93/10000\n",
      "62/62 [==============================] - 0s 78us/step - loss: 75931.6676 - val_loss: 93709.4766\n",
      "Epoch 94/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 76348.2087 - val_loss: 92710.4609\n",
      "Epoch 95/10000\n",
      "62/62 [==============================] - 0s 129us/step - loss: 76407.5514 - val_loss: 94278.9141\n",
      "Epoch 96/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 74089.5547 - val_loss: 96418.1641\n",
      "Epoch 97/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 76631.5444 - val_loss: 94104.1406\n",
      "Epoch 98/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 79724.2472 - val_loss: 92578.6641\n",
      "Epoch 99/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76060.4451 - val_loss: 93811.5938\n",
      "Epoch 100/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 73265.0244 - val_loss: 100738.6406\n",
      "Epoch 101/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76492.1159 - val_loss: 93900.3906\n",
      "Epoch 102/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 72758.1792 - val_loss: 91445.3359\n",
      "Epoch 103/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 74945.5082 - val_loss: 91307.3516\n",
      "Epoch 104/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 73329.7634 - val_loss: 94432.5859\n",
      "Epoch 105/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 74122.3211 - val_loss: 95780.9062\n",
      "Epoch 106/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 73439.6195 - val_loss: 91279.8828\n",
      "Epoch 107/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76972.7324 - val_loss: 90866.7109\n",
      "Epoch 108/10000\n",
      "62/62 [==============================] - 0s 89us/step - loss: 73386.7810 - val_loss: 95293.4141\n",
      "Epoch 109/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 76173.5612 - val_loss: 97012.7578\n",
      "Epoch 110/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 73232.7394 - val_loss: 90394.7031\n",
      "Epoch 111/10000\n",
      "62/62 [==============================] - 0s 121us/step - loss: 73461.9105 - val_loss: 90654.2500\n",
      "Epoch 112/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 74749.6598 - val_loss: 90735.8281\n",
      "Epoch 113/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 72200.4836 - val_loss: 95364.5391\n",
      "Epoch 114/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 81130.2626 - val_loss: 93347.5703\n",
      "Epoch 115/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 70201.1793 - val_loss: 91134.9219\n",
      "Epoch 116/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 83805.3352 - val_loss: 90587.3359\n",
      "Epoch 117/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 70954.8566 - val_loss: 101653.4766\n",
      "Epoch 118/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 79183.2646 - val_loss: 104851.1797\n",
      "Epoch 119/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 75299.4110 - val_loss: 89425.8438\n",
      "Epoch 120/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76138.2646 - val_loss: 90917.0859\n",
      "Epoch 121/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 75985.3939 - val_loss: 95199.7031\n",
      "Epoch 122/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 74299.3107 - val_loss: 95257.8047\n",
      "Epoch 123/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 76371.2412 - val_loss: 88857.6328\n",
      "Epoch 124/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 73125.9819 - val_loss: 89688.5859\n",
      "Epoch 125/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 72784.4690 - val_loss: 92149.4297\n",
      "Epoch 126/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 71024.4797 - val_loss: 88478.4609\n",
      "Epoch 127/10000\n",
      "62/62 [==============================] - 0s 84us/step - loss: 71731.6850 - val_loss: 88214.3672\n",
      "Epoch 128/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 71030.6058 - val_loss: 90827.3203\n",
      "Epoch 129/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 73673.4677 - val_loss: 93898.1406\n",
      "Epoch 130/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 70814.9882 - val_loss: 87746.0625\n",
      "Epoch 131/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 72133.1421 - val_loss: 87652.1016\n",
      "Epoch 132/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 71666.3904 - val_loss: 91937.1328\n",
      "Epoch 133/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 71326.2286 - val_loss: 90103.9531\n",
      "Epoch 134/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69764.6499 - val_loss: 87235.4219\n",
      "Epoch 135/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 74127.3062 - val_loss: 87100.8047\n",
      "Epoch 136/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 71117.7054 - val_loss: 95105.1016\n",
      "Epoch 137/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 72548.9766 - val_loss: 88052.7734\n",
      "Epoch 138/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 71071.4252 - val_loss: 86654.0391\n",
      "Epoch 139/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 74274.5376 - val_loss: 90171.5156\n",
      "Epoch 140/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 70622.3216 - val_loss: 86548.6328\n",
      "Epoch 141/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69811.8052 - val_loss: 87095.0625\n",
      "Epoch 142/10000\n",
      "62/62 [==============================] - 0s 86us/step - loss: 69082.2225 - val_loss: 89282.8281\n",
      "Epoch 143/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 69455.4614 - val_loss: 88640.8047\n",
      "Epoch 144/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69241.0141 - val_loss: 86421.3047\n",
      "Epoch 145/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 70094.2598 - val_loss: 85691.9844\n",
      "Epoch 146/10000\n",
      "62/62 [==============================] - 0s 82us/step - loss: 70277.8879 - val_loss: 89399.6484\n",
      "Epoch 147/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 69242.5541 - val_loss: 86531.4609\n",
      "Epoch 148/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 68346.6888 - val_loss: 85678.7109\n",
      "Epoch 149/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 68423.4239 - val_loss: 86040.2891\n",
      "Epoch 150/10000\n",
      "62/62 [==============================] - 0s 105us/step - loss: 68406.1389 - val_loss: 86673.1328\n",
      "Epoch 151/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69363.3914 - val_loss: 86787.3047\n",
      "Epoch 152/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 68044.2049 - val_loss: 87677.4844\n",
      "Epoch 153/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 70818.7291 - val_loss: 85057.2734\n",
      "Epoch 154/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69157.9143 - val_loss: 87590.9062\n",
      "Epoch 155/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 68154.7964 - val_loss: 84555.0000\n",
      "Epoch 156/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 68776.6246 - val_loss: 85063.6484\n",
      "Epoch 157/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 67791.1419 - val_loss: 84407.4844\n",
      "Epoch 158/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 67370.4280 - val_loss: 86112.4531\n",
      "Epoch 159/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 67367.1770 - val_loss: 85193.9219\n",
      "Epoch 160/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 67018.3516 - val_loss: 84021.9531\n",
      "Epoch 161/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 67574.8261 - val_loss: 84164.4766\n",
      "Epoch 162/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 66755.1229 - val_loss: 86479.1016\n",
      "Epoch 163/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 67856.6426 - val_loss: 84178.0703\n",
      "Epoch 164/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 69811.7591 - val_loss: 83910.9297\n",
      "Epoch 165/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 70225.1147 - val_loss: 88948.5625\n",
      "Epoch 166/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 70215.7689 - val_loss: 82823.4453\n",
      "Epoch 167/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 67654.4433 - val_loss: 83871.3672\n",
      "Epoch 168/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 66080.8747 - val_loss: 87015.6875\n",
      "Epoch 169/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 68152.1889 - val_loss: 83978.9844\n",
      "Epoch 170/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 66044.3821 - val_loss: 83277.4141\n",
      "Epoch 171/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69803.4990 - val_loss: 82310.0781\n",
      "Epoch 172/10000\n",
      "62/62 [==============================] - 0s 106us/step - loss: 69734.6825 - val_loss: 88212.1719\n",
      "Epoch 173/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 70525.8465 - val_loss: 81762.8203\n",
      "Epoch 174/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 66361.3309 - val_loss: 83405.3594\n",
      "Epoch 175/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 66348.7165 - val_loss: 84242.0391\n",
      "Epoch 176/10000\n",
      "62/62 [==============================] - 0s 338us/step - loss: 65785.0026 - val_loss: 83622.2422\n",
      "Epoch 177/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 65481.1152 - val_loss: 81899.6094\n",
      "Epoch 178/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 65151.1343 - val_loss: 82050.1406\n",
      "Epoch 179/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 65000.4042 - val_loss: 81713.2578\n",
      "Epoch 180/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69323.1792 - val_loss: 81943.7812\n",
      "Epoch 181/10000\n",
      "62/62 [==============================] - 0s 86us/step - loss: 64336.8306 - val_loss: 87483.1719\n",
      "Epoch 182/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 66776.2555 - val_loss: 81233.3438\n",
      "Epoch 183/10000\n",
      "62/62 [==============================] - 0s 100us/step - loss: 67831.0406 - val_loss: 80633.0703\n",
      "Epoch 184/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 67045.6939 - val_loss: 86235.2500\n",
      "Epoch 185/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 68601.6971 - val_loss: 81463.8984\n",
      "Epoch 186/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 63883.1319 - val_loss: 81192.1328\n",
      "Epoch 187/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 68275.5471 - val_loss: 81671.4219\n",
      "Epoch 188/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 64007.2273 - val_loss: 89766.8906\n",
      "Epoch 189/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 68129.9932 - val_loss: 79596.7969\n",
      "Epoch 190/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 63982.0290 - val_loss: 80979.4453\n",
      "Epoch 191/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 67105.5422 - val_loss: 82604.5781\n",
      "Epoch 192/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 65951.6424 - val_loss: 83847.0938\n",
      "Epoch 193/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 65283.8143 - val_loss: 79209.8906\n",
      "Epoch 194/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 65599.8745 - val_loss: 80188.9844\n",
      "Epoch 195/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 62819.1666 - val_loss: 85245.3906\n",
      "Epoch 196/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 65359.2991 - val_loss: 79090.5703\n",
      "Epoch 197/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 65030.2986 - val_loss: 78722.2031\n",
      "Epoch 198/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 63816.0572 - val_loss: 83312.1562\n",
      "Epoch 199/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 64804.2850 - val_loss: 83329.2656\n",
      "Epoch 200/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 63681.5227 - val_loss: 78082.6172\n",
      "Epoch 201/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 64099.1605 - val_loss: 78473.6406\n",
      "Epoch 202/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 62442.5592 - val_loss: 81607.4297\n",
      "Epoch 203/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 62921.1817 - val_loss: 77945.2109\n",
      "Epoch 204/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 64526.4808 - val_loss: 77421.0859\n",
      "Epoch 205/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 62265.7491 - val_loss: 82648.3125\n",
      "Epoch 206/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 66129.1706 - val_loss: 77781.0234\n",
      "Epoch 207/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 64807.3393 - val_loss: 78810.0469\n",
      "Epoch 208/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 63037.1943 - val_loss: 76848.3828\n",
      "Epoch 209/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 63369.0451 - val_loss: 79743.7344\n",
      "Epoch 210/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 63684.6436 - val_loss: 77459.5312\n",
      "Epoch 211/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 61017.2616 - val_loss: 76598.9453\n",
      "Epoch 212/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 62942.2065 - val_loss: 77755.5469\n",
      "Epoch 213/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 62272.7939 - val_loss: 80743.5000\n",
      "Epoch 214/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 64329.3259 - val_loss: 75953.1875\n",
      "Epoch 215/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 61978.3386 - val_loss: 78174.2891\n",
      "Epoch 216/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 61619.6363 - val_loss: 76965.0547\n",
      "Epoch 217/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 80us/step - loss: 66206.8366 - val_loss: 76735.3984\n",
      "Epoch 218/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 59949.3533 - val_loss: 76119.9219\n",
      "Epoch 219/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 65969.4428 - val_loss: 76020.5391\n",
      "Epoch 220/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 73530.1699 - val_loss: 86167.7891\n",
      "Epoch 221/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 65777.4242 - val_loss: 78818.8359\n",
      "Epoch 222/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 69302.4342 - val_loss: 77076.7109\n",
      "Epoch 223/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 71929.7059 - val_loss: 90238.8047\n",
      "Epoch 224/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 64998.9733 - val_loss: 79929.1484\n",
      "Epoch 225/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 69352.7389 - val_loss: 75220.5547\n",
      "Epoch 226/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 64051.6928 - val_loss: 87382.4609\n",
      "Epoch 227/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 62492.9302 - val_loss: 75324.0156\n",
      "Epoch 228/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 65191.5243 - val_loss: 74051.9297\n",
      "Epoch 229/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 62740.1074 - val_loss: 84741.1562\n",
      "Epoch 230/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 62893.1121 - val_loss: 73721.9766\n",
      "Epoch 231/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 77407.3475 - val_loss: 74055.7812\n",
      "Epoch 232/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 60420.0265 - val_loss: 97385.2656\n",
      "Epoch 233/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 70005.0868 - val_loss: 73311.6484\n",
      "Epoch 234/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 66750.4065 - val_loss: 73828.4609\n",
      "Epoch 235/10000\n",
      "62/62 [==============================] - 0s 74us/step - loss: 57980.8073 - val_loss: 87813.1641\n",
      "Epoch 236/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 65868.2156 - val_loss: 74509.2734\n",
      "Epoch 237/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 64938.8402 - val_loss: 73685.5078\n",
      "Epoch 238/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 63380.6014 - val_loss: 81415.7656\n",
      "Epoch 239/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 61278.9148 - val_loss: 72508.6562\n",
      "Epoch 240/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 58804.8288 - val_loss: 72309.5078\n",
      "Epoch 241/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 59277.4995 - val_loss: 73750.6172\n",
      "Epoch 242/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 59697.7109 - val_loss: 72530.2266\n",
      "Epoch 243/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 58194.6851 - val_loss: 71920.4922\n",
      "Epoch 244/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 58954.2022 - val_loss: 72456.0000\n",
      "Epoch 245/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 57697.3739 - val_loss: 73933.4297\n",
      "Epoch 246/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 62453.1265 - val_loss: 72742.7578\n",
      "Epoch 247/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 64823.5096 - val_loss: 75222.9453\n",
      "Epoch 248/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 65476.8775 - val_loss: 72911.2656\n",
      "Epoch 249/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 59127.4816 - val_loss: 80138.3203\n",
      "Epoch 250/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 61683.7198 - val_loss: 73001.9297\n",
      "Epoch 251/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 58891.4735 - val_loss: 71684.4609\n",
      "Epoch 252/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 60376.9704 - val_loss: 75309.2969\n",
      "Epoch 253/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 59400.8531 - val_loss: 71048.3516\n",
      "Epoch 254/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 56640.1255 - val_loss: 70834.1484\n",
      "Epoch 255/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 59090.8954 - val_loss: 71328.8594\n",
      "Epoch 256/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 56628.8989 - val_loss: 72066.1328\n",
      "Epoch 257/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 56835.7864 - val_loss: 70210.1406\n",
      "Epoch 258/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 58331.3712 - val_loss: 69995.8672\n",
      "Epoch 259/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 55878.2298 - val_loss: 75089.3594\n",
      "Epoch 260/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 59037.3784 - val_loss: 70561.7500\n",
      "Epoch 261/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 56720.8150 - val_loss: 70297.9453\n",
      "Epoch 262/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 58579.5439 - val_loss: 71027.7500\n",
      "Epoch 263/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 57790.8073 - val_loss: 69248.2891\n",
      "Epoch 264/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 60036.6279 - val_loss: 72932.6406\n",
      "Epoch 265/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 59094.6806 - val_loss: 68878.1875\n",
      "Epoch 266/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 56207.3002 - val_loss: 71884.8359\n",
      "Epoch 267/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 59592.0189 - val_loss: 69327.4922\n",
      "Epoch 268/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 62038.4403 - val_loss: 71025.6328\n",
      "Epoch 269/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 60292.9269 - val_loss: 69398.3828\n",
      "Epoch 270/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 58500.2043 - val_loss: 75916.6406\n",
      "Epoch 271/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 58581.0489 - val_loss: 68142.2109\n",
      "Epoch 272/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 61513.6003 - val_loss: 68285.7734\n",
      "Epoch 273/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 54513.4424 - val_loss: 80815.2734\n",
      "Epoch 274/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 61345.9810 - val_loss: 67920.3984\n",
      "Epoch 275/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 57999.3149 - val_loss: 67670.5000\n",
      "Epoch 276/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 54760.2152 - val_loss: 74087.6719\n",
      "Epoch 277/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 56791.7574 - val_loss: 67283.5781\n",
      "Epoch 278/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 55368.7858 - val_loss: 67342.5156\n",
      "Epoch 279/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 55369.0811 - val_loss: 70266.0938\n",
      "Epoch 280/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 55241.1457 - val_loss: 67876.2734\n",
      "Epoch 281/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 56398.9597 - val_loss: 66753.2656\n",
      "Epoch 282/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 55051.3616 - val_loss: 71470.1875\n",
      "Epoch 283/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 56386.9791 - val_loss: 66497.4609\n",
      "Epoch 284/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 63156.1637 - val_loss: 66402.2266\n",
      "Epoch 285/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 57689.8615 - val_loss: 80599.7578\n",
      "Epoch 286/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 62646.3349 - val_loss: 67237.7422\n",
      "Epoch 287/10000\n",
      "62/62 [==============================] - 0s 129us/step - loss: 56282.4848 - val_loss: 68285.4453\n",
      "Epoch 288/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 54218.7094 - val_loss: 67886.5469\n",
      "Epoch 289/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 55559.7094 - val_loss: 65715.1641\n",
      "Epoch 290/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53268.4323 - val_loss: 69893.0000\n",
      "Epoch 291/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 56221.3501 - val_loss: 66065.1641\n",
      "Epoch 292/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 52631.0905 - val_loss: 66036.8203\n",
      "Epoch 293/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 55175.2330 - val_loss: 68572.8281\n",
      "Epoch 294/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 58494.2094 - val_loss: 68045.0156\n",
      "Epoch 295/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 51453.7249 - val_loss: 68026.9141\n",
      "Epoch 296/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 58351.3541 - val_loss: 66157.7266\n",
      "Epoch 297/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 53464.4986 - val_loss: 67818.6953\n",
      "Epoch 298/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 53669.6760 - val_loss: 64869.9961\n",
      "Epoch 299/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 55163.7330 - val_loss: 64456.5352\n",
      "Epoch 300/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53709.8024 - val_loss: 70419.2891\n",
      "Epoch 301/10000\n",
      "62/62 [==============================] - 0s 113us/step - loss: 55797.3529 - val_loss: 64225.5391\n",
      "Epoch 302/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53049.5115 - val_loss: 66846.6406\n",
      "Epoch 303/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53250.9554 - val_loss: 67002.3359\n",
      "Epoch 304/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 54534.2848 - val_loss: 64531.8086\n",
      "Epoch 305/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 51774.3819 - val_loss: 65805.2109\n",
      "Epoch 306/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 52178.1096 - val_loss: 64026.0039\n",
      "Epoch 307/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 52960.2684 - val_loss: 64701.6758\n",
      "Epoch 308/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 57142.2786 - val_loss: 65060.0234\n",
      "Epoch 309/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 50486.9754 - val_loss: 65074.9961\n",
      "Epoch 310/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 55653.1526 - val_loss: 65619.4844\n",
      "Epoch 311/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 51910.3630 - val_loss: 68713.9141\n",
      "Epoch 312/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 52455.0625 - val_loss: 63040.9844\n",
      "Epoch 313/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 54884.1373 - val_loss: 63151.3633\n",
      "Epoch 314/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 51409.7088 - val_loss: 62707.7070\n",
      "Epoch 315/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 52461.8895 - val_loss: 63208.7070\n",
      "Epoch 316/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53319.7127 - val_loss: 65228.3984\n",
      "Epoch 317/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 50479.2263 - val_loss: 62837.7109\n",
      "Epoch 318/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 52833.7549 - val_loss: 63927.9688\n",
      "Epoch 319/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 58098.3707 - val_loss: 63762.5195\n",
      "Epoch 320/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 49112.4664 - val_loss: 65118.5273\n",
      "Epoch 321/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 55445.0874 - val_loss: 65015.6953\n",
      "Epoch 322/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 51698.0483 - val_loss: 63584.9805\n",
      "Epoch 323/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 58363.4764 - val_loss: 61464.0742\n",
      "Epoch 324/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 59657.0456 - val_loss: 67619.0703\n",
      "Epoch 325/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 49480.9007 - val_loss: 65288.0703\n",
      "Epoch 326/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 57018.1917 - val_loss: 62663.1641\n",
      "Epoch 327/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 50122.7996 - val_loss: 63281.1133\n",
      "Epoch 328/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 49926.3411 - val_loss: 60868.4883\n",
      "Epoch 329/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 50424.3138 - val_loss: 61314.3047\n",
      "Epoch 330/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 54719.8385 - val_loss: 62798.4805\n",
      "Epoch 331/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 49964.2074 - val_loss: 65785.9688\n",
      "Epoch 332/10000\n",
      "62/62 [==============================] - 0s 129us/step - loss: 58342.5722 - val_loss: 60394.4023\n",
      "Epoch 333/10000\n",
      "62/62 [==============================] - 0s 99us/step - loss: 53079.8259 - val_loss: 65053.6992\n",
      "Epoch 334/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53874.1585 - val_loss: 60488.0156\n",
      "Epoch 335/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 49785.3749 - val_loss: 66304.4453\n",
      "Epoch 336/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 51590.0880 - val_loss: 60884.8086\n",
      "Epoch 337/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 49273.0664 - val_loss: 59838.7344\n",
      "Epoch 338/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 50027.7810 - val_loss: 63561.4297\n",
      "Epoch 339/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 54232.7545 - val_loss: 60518.4180\n",
      "Epoch 340/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 48560.3240 - val_loss: 61322.0742\n",
      "Epoch 341/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 51552.8759 - val_loss: 66978.5469\n",
      "Epoch 342/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 52944.4926 - val_loss: 61330.1562\n",
      "Epoch 343/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 51968.8484 - val_loss: 59810.6328\n",
      "Epoch 344/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 51537.9575 - val_loss: 65784.9609\n",
      "Epoch 345/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 50489.4167 - val_loss: 59359.5312\n",
      "Epoch 346/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53490.6474 - val_loss: 59013.9688\n",
      "Epoch 347/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 50072.9085 - val_loss: 66619.1641\n",
      "Epoch 348/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 63728.3498 - val_loss: 58882.1406\n",
      "Epoch 349/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 47384.8711 - val_loss: 70699.8516\n",
      "Epoch 350/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53965.9429 - val_loss: 58328.8320\n",
      "Epoch 351/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 48840.0221 - val_loss: 58762.4766\n",
      "Epoch 352/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 48623.8042 - val_loss: 64242.0547\n",
      "Epoch 353/10000\n",
      "62/62 [==============================] - 0s 84us/step - loss: 51637.6265 - val_loss: 58104.4609\n",
      "Epoch 354/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 49184.7855 - val_loss: 58206.0898\n",
      "Epoch 355/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 48209.4570 - val_loss: 64900.5078\n",
      "Epoch 356/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 52417.2397 - val_loss: 57757.9531\n",
      "Epoch 357/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 48460.2239 - val_loss: 59945.9336\n",
      "Epoch 358/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 55318.4492 - val_loss: 57825.4922\n",
      "Epoch 359/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 55904.1930 - val_loss: 58168.7031\n",
      "Epoch 360/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 60125.3407 - val_loss: 68301.6562\n",
      "Epoch 361/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 67095.2417 - val_loss: 59637.4531\n",
      "Epoch 362/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 51977.2990 - val_loss: 71202.1953\n",
      "Epoch 363/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 97us/step - loss: 52037.3550 - val_loss: 58403.6445\n",
      "Epoch 364/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 52633.9856 - val_loss: 56857.6641\n",
      "Epoch 365/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 47087.3370 - val_loss: 57459.7070\n",
      "Epoch 366/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 47328.0278 - val_loss: 57368.5938\n",
      "Epoch 367/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 58611.3352 - val_loss: 56460.8945\n",
      "Epoch 368/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 53188.4337 - val_loss: 57355.6914\n",
      "Epoch 369/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53464.9743 - val_loss: 68393.2500\n",
      "Epoch 370/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 48837.1142 - val_loss: 60710.9141\n",
      "Epoch 371/10000\n",
      "62/62 [==============================] - 0s 64us/step - loss: 54978.9259 - val_loss: 58751.8906\n",
      "Epoch 372/10000\n",
      "62/62 [==============================] - 0s 81us/step - loss: 57250.6202 - val_loss: 59917.5977\n",
      "Epoch 373/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 53518.3004 - val_loss: 60529.3320\n",
      "Epoch 374/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 48145.7962 - val_loss: 75294.0391\n",
      "Epoch 375/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 64469.3412 - val_loss: 56671.8867\n",
      "Epoch 376/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 72420.2059 - val_loss: 56408.9531\n",
      "Epoch 377/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 71615.8334 - val_loss: 81900.1328\n",
      "Epoch 378/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 60912.6922 - val_loss: 76357.0859\n",
      "Epoch 379/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 60990.8328 - val_loss: 78443.2031\n",
      "Epoch 380/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 60987.5040 - val_loss: 55496.9102\n",
      "Epoch 381/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 48010.4981 - val_loss: 57710.6562\n",
      "Epoch 382/10000\n",
      "62/62 [==============================] - 0s 96us/step - loss: 47198.0340 - val_loss: 69076.3438\n",
      "Epoch 383/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 56956.1293 - val_loss: 56139.9453\n",
      "Epoch 384/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 50755.9805 - val_loss: 55825.2422\n",
      "Epoch 385/10000\n",
      "62/62 [==============================] - 0s 80us/step - loss: 46463.1304 - val_loss: 68236.3594\n",
      "Epoch 386/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 50737.6777 - val_loss: 56571.1602\n",
      "Epoch 387/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 50065.5411 - val_loss: 54327.7969\n",
      "Epoch 388/10000\n",
      "62/62 [==============================] - 0s 97us/step - loss: 44528.8328 - val_loss: 61637.1016\n",
      "Epoch 389/10000\n",
      "62/62 [==============================] - 0s 145us/step - loss: 47552.6443 - val_loss: 54209.2383\n",
      "Epoch 390/10000\n",
      "62/62 [==============================] - 0s 161us/step - loss: 47070.9623 - val_loss: 54008.6797\n",
      "Epoch 391/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 45794.9013 - val_loss: 54548.5742\n",
      "Epoch 392/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 45544.2538 - val_loss: 53872.4922\n",
      "Epoch 393/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 46522.8711 - val_loss: 54400.8672\n",
      "Epoch 394/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 45507.6367 - val_loss: 56363.0117\n",
      "Epoch 395/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 45006.2697 - val_loss: 53779.4609\n",
      "Epoch 396/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 49646.6477 - val_loss: 58876.0273\n",
      "Epoch 397/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 47588.5630 - val_loss: 56507.2188\n",
      "Epoch 398/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44335.5417 - val_loss: 54761.8242\n",
      "Epoch 399/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 47621.3737 - val_loss: 58934.5273\n",
      "Epoch 400/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 52279.8732 - val_loss: 53409.6523\n",
      "Epoch 401/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 60594.5272 - val_loss: 53140.0430\n",
      "Epoch 402/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 60481.2891 - val_loss: 65744.7031\n",
      "Epoch 403/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 60589.6489 - val_loss: 60226.2461\n",
      "Epoch 404/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 47389.1488 - val_loss: 77221.8594\n",
      "Epoch 405/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 57333.5839 - val_loss: 52965.1367\n",
      "Epoch 406/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 46670.3572 - val_loss: 52684.7852\n",
      "Epoch 407/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 43435.0917 - val_loss: 58830.6992\n",
      "Epoch 408/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49047.6322 - val_loss: 53204.6172\n",
      "Epoch 409/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 51947.8187 - val_loss: 52303.1641\n",
      "Epoch 410/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 45717.1797 - val_loss: 61973.3203\n",
      "Epoch 411/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 45033.4585 - val_loss: 57280.2422\n",
      "Epoch 412/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 50085.4308 - val_loss: 56090.3320\n",
      "Epoch 413/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 65312.8390 - val_loss: 51906.0039\n",
      "Epoch 414/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 74111.9098 - val_loss: 52360.9922\n",
      "Epoch 415/10000\n",
      "62/62 [==============================] - 0s 201us/step - loss: 52013.9955 - val_loss: 83118.7422\n",
      "Epoch 416/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 55541.9606 - val_loss: 70885.4219\n",
      "Epoch 417/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 63181.4466 - val_loss: 75031.1562\n",
      "Epoch 418/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 66849.5880 - val_loss: 52270.9688\n",
      "Epoch 419/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 43421.4585 - val_loss: 61958.0391\n",
      "Epoch 420/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 67699.3928 - val_loss: 63825.7773\n",
      "Epoch 421/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 46061.6454 - val_loss: 57462.3438\n",
      "Epoch 422/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 51629.3546 - val_loss: 55303.6133\n",
      "Epoch 423/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 44336.3560 - val_loss: 50923.0156\n",
      "Epoch 424/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 46648.2642 - val_loss: 51818.1797\n",
      "Epoch 425/10000\n",
      "62/62 [==============================] - 0s 483us/step - loss: 49300.3999 - val_loss: 52221.1445\n",
      "Epoch 426/10000\n",
      "62/62 [==============================] - 0s 189us/step - loss: 49122.2855 - val_loss: 51352.9492\n",
      "Epoch 427/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 43452.5864 - val_loss: 62907.4727\n",
      "Epoch 428/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 47479.3954 - val_loss: 55005.5391\n",
      "Epoch 429/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 50255.0159 - val_loss: 51211.1641\n",
      "Epoch 430/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 42342.6206 - val_loss: 56649.7461\n",
      "Epoch 431/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 44605.7277 - val_loss: 50810.6367\n",
      "Epoch 432/10000\n",
      "62/62 [==============================] - 0s 177us/step - loss: 45176.7331 - val_loss: 51633.0117\n",
      "Epoch 433/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 43814.2316 - val_loss: 50378.2461\n",
      "Epoch 434/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 43480.6121 - val_loss: 50833.7852\n",
      "Epoch 435/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 42041.3654 - val_loss: 49939.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 436/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 44492.2269 - val_loss: 54102.0117\n",
      "Epoch 437/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 46206.0066 - val_loss: 49958.7266\n",
      "Epoch 438/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 53967.7626 - val_loss: 49677.4961\n",
      "Epoch 439/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 52529.9880 - val_loss: 55527.5977\n",
      "Epoch 440/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 55589.2056 - val_loss: 52377.2070\n",
      "Epoch 441/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42293.9628 - val_loss: 71204.8516\n",
      "Epoch 442/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 50909.2504 - val_loss: 54237.3789\n",
      "Epoch 443/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 50245.2920 - val_loss: 51196.1055\n",
      "Epoch 444/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 42213.0543 - val_loss: 56721.0000\n",
      "Epoch 445/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 45127.7180 - val_loss: 49300.8867\n",
      "Epoch 446/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44403.9674 - val_loss: 49925.7969\n",
      "Epoch 447/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 49017.3587 - val_loss: 50536.4180\n",
      "Epoch 448/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 48502.6282 - val_loss: 49212.9102\n",
      "Epoch 449/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 46906.4418 - val_loss: 53106.7305\n",
      "Epoch 450/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 40973.8888 - val_loss: 53657.6367\n",
      "Epoch 451/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 47302.9273 - val_loss: 55567.2578\n",
      "Epoch 452/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 46050.7708 - val_loss: 48446.7383\n",
      "Epoch 453/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42564.4023 - val_loss: 48814.1406\n",
      "Epoch 454/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 41837.5930 - val_loss: 48426.1836\n",
      "Epoch 455/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42254.2388 - val_loss: 49081.8242\n",
      "Epoch 456/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42690.5520 - val_loss: 48263.7383\n",
      "Epoch 457/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 41988.3247 - val_loss: 47990.6719\n",
      "Epoch 458/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49442.7661 - val_loss: 48274.5977\n",
      "Epoch 459/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 41824.8758 - val_loss: 48610.0352\n",
      "Epoch 460/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 53231.4884 - val_loss: 49748.9258\n",
      "Epoch 461/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 40157.8889 - val_loss: 50726.1484\n",
      "Epoch 462/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 45002.0203 - val_loss: 53465.7227\n",
      "Epoch 463/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 43022.8053 - val_loss: 48376.7734\n",
      "Epoch 464/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 43484.4257 - val_loss: 47474.1953\n",
      "Epoch 465/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40719.6496 - val_loss: 49420.6016\n",
      "Epoch 466/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44194.0320 - val_loss: 47546.5156\n",
      "Epoch 467/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49204.0784 - val_loss: 48724.8086\n",
      "Epoch 468/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 51167.8322 - val_loss: 49195.3906\n",
      "Epoch 469/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 47294.9444 - val_loss: 49746.0352\n",
      "Epoch 470/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40753.2171 - val_loss: 67061.1875\n",
      "Epoch 471/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 48378.5330 - val_loss: 51981.3359\n",
      "Epoch 472/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 47683.6429 - val_loss: 51212.0781\n",
      "Epoch 473/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 53749.6353 - val_loss: 46714.7109\n",
      "Epoch 474/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46106.9570 - val_loss: 47516.8086\n",
      "Epoch 475/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 43120.6298 - val_loss: 59195.2070\n",
      "Epoch 476/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 42526.8018 - val_loss: 56167.6211\n",
      "Epoch 477/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50316.6206 - val_loss: 51495.3242\n",
      "Epoch 478/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 43401.4843 - val_loss: 46343.5703\n",
      "Epoch 479/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 42860.4170 - val_loss: 46305.4297\n",
      "Epoch 480/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 61703.7306 - val_loss: 46193.3672\n",
      "Epoch 481/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 39061.9546 - val_loss: 53095.9414\n",
      "Epoch 482/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 44331.3989 - val_loss: 64234.7656\n",
      "Epoch 483/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 50202.7985 - val_loss: 48428.8359\n",
      "Epoch 484/10000\n",
      "62/62 [==============================] - 0s 274us/step - loss: 72180.6348 - val_loss: 53932.8906\n",
      "Epoch 485/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 56526.5307 - val_loss: 51403.5000\n",
      "Epoch 486/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 53584.1094 - val_loss: 53510.1836\n",
      "Epoch 487/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 43931.0003 - val_loss: 76994.6094\n",
      "Epoch 488/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 53276.4677 - val_loss: 56586.2656\n",
      "Epoch 489/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 54098.3342 - val_loss: 65440.2812\n",
      "Epoch 490/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 58371.2509 - val_loss: 45622.3594\n",
      "Epoch 491/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 49408.2102 - val_loss: 45762.9766\n",
      "Epoch 492/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 50455.5205 - val_loss: 54449.2656\n",
      "Epoch 493/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 44600.0344 - val_loss: 52827.8086\n",
      "Epoch 494/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 42991.4635 - val_loss: 67938.5000\n",
      "Epoch 495/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 54890.1310 - val_loss: 55485.9141\n",
      "Epoch 496/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 61983.5902 - val_loss: 49835.0977\n",
      "Epoch 497/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 52844.6008 - val_loss: 48291.5586\n",
      "Epoch 498/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 58641.7951 - val_loss: 47019.6719\n",
      "Epoch 499/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 60807.3916 - val_loss: 56300.4844\n",
      "Epoch 500/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 61181.3434 - val_loss: 50269.2617\n",
      "Epoch 501/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 55881.9913 - val_loss: 61331.3867\n",
      "Epoch 502/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 45866.4107 - val_loss: 57214.7891\n",
      "Epoch 503/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 50011.0774 - val_loss: 60810.1406\n",
      "Epoch 504/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 47753.9529 - val_loss: 46046.9023\n",
      "Epoch 505/10000\n",
      "62/62 [==============================] - 0s 338us/step - loss: 47546.5079 - val_loss: 44807.9844\n",
      "Epoch 506/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 38604.0609 - val_loss: 44749.4297\n",
      "Epoch 507/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 39594.5717 - val_loss: 47759.8047\n",
      "Epoch 508/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 40750.1385 - val_loss: 44600.9844\n",
      "Epoch 509/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 55444.2170 - val_loss: 52386.0820\n",
      "Epoch 510/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 48443.5003 - val_loss: 45362.3516\n",
      "Epoch 511/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 57425.9894 - val_loss: 44019.1289\n",
      "Epoch 512/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 46749.7475 - val_loss: 54151.9688\n",
      "Epoch 513/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 39971.6895 - val_loss: 56285.3008\n",
      "Epoch 514/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 56104.4410 - val_loss: 52978.6523\n",
      "Epoch 515/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 47346.8472 - val_loss: 44424.9609\n",
      "Epoch 516/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 39060.9460 - val_loss: 51210.8828\n",
      "Epoch 517/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 45427.1894 - val_loss: 43578.4531\n",
      "Epoch 518/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 40384.2988 - val_loss: 45299.4766\n",
      "Epoch 519/10000\n",
      "62/62 [==============================] - 0s 296us/step - loss: 38501.6106 - val_loss: 43842.2930\n",
      "Epoch 520/10000\n",
      "62/62 [==============================] - 0s 323us/step - loss: 39295.0829 - val_loss: 46399.2539\n",
      "Epoch 521/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 38867.2414 - val_loss: 43731.7695\n",
      "Epoch 522/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 39498.7911 - val_loss: 44172.4883\n",
      "Epoch 523/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 44426.9331 - val_loss: 44229.6680\n",
      "Epoch 524/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 48057.8494 - val_loss: 46157.1211\n",
      "Epoch 525/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 48875.2554 - val_loss: 43480.1719\n",
      "Epoch 526/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 56830.6003 - val_loss: 42992.7266\n",
      "Epoch 527/10000\n",
      "62/62 [==============================] - 0s 298us/step - loss: 49132.5639 - val_loss: 50157.4609\n",
      "Epoch 528/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 50256.1053 - val_loss: 47091.7344\n",
      "Epoch 529/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 55132.0587 - val_loss: 52431.3008\n",
      "Epoch 530/10000\n",
      "62/62 [==============================] - 0s 338us/step - loss: 37082.4080 - val_loss: 58641.5938\n",
      "Epoch 531/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 54603.5174 - val_loss: 53309.8047\n",
      "Epoch 532/10000\n",
      "62/62 [==============================] - 0s 330us/step - loss: 41196.3872 - val_loss: 47708.5352\n",
      "Epoch 533/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 43950.7392 - val_loss: 49081.5078\n",
      "Epoch 534/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 41690.7621 - val_loss: 42492.8398\n",
      "Epoch 535/10000\n",
      "62/62 [==============================] - 0s 338us/step - loss: 38564.0180 - val_loss: 43304.5391\n",
      "Epoch 536/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 40968.3855 - val_loss: 43385.1094\n",
      "Epoch 537/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 39459.4378 - val_loss: 42738.4141\n",
      "Epoch 538/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 37913.9705 - val_loss: 46440.7773\n",
      "Epoch 539/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 39407.4045 - val_loss: 42199.4844\n",
      "Epoch 540/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 38836.8131 - val_loss: 45555.5273\n",
      "Epoch 541/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 38856.3882 - val_loss: 42158.2695\n",
      "Epoch 542/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 37540.7513 - val_loss: 42009.9805\n",
      "Epoch 543/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 37908.4653 - val_loss: 42703.3516\n",
      "Epoch 544/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 37540.0786 - val_loss: 41971.7852\n",
      "Epoch 545/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 38414.5699 - val_loss: 42043.6914\n",
      "Epoch 546/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 40557.5828 - val_loss: 43325.6758\n",
      "Epoch 547/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 38504.6345 - val_loss: 42552.4609\n",
      "Epoch 548/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 37070.8487 - val_loss: 42045.6641\n",
      "Epoch 549/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 38596.2058 - val_loss: 49544.0273\n",
      "Epoch 550/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 43035.9080 - val_loss: 43881.8047\n",
      "Epoch 551/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 61244.9471 - val_loss: 55286.5039\n",
      "Epoch 552/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 74798.9957 - val_loss: 44337.0742\n",
      "Epoch 553/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 57207.4627 - val_loss: 41475.7617\n",
      "Epoch 554/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 56158.6411 - val_loss: 48554.6406\n",
      "Epoch 555/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 45836.9047 - val_loss: 50476.1016\n",
      "Epoch 556/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 48475.8926 - val_loss: 61616.6992\n",
      "Epoch 557/10000\n",
      "62/62 [==============================] - 0s 282us/step - loss: 44127.3594 - val_loss: 55324.1094\n",
      "Epoch 558/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 52283.3823 - val_loss: 52322.0312\n",
      "Epoch 559/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 40007.3964 - val_loss: 46730.4023\n",
      "Epoch 560/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 41981.2553 - val_loss: 51163.9023\n",
      "Epoch 561/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 43229.8721 - val_loss: 43808.0273\n",
      "Epoch 562/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 43526.5781 - val_loss: 44458.0703\n",
      "Epoch 563/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 43714.3971 - val_loss: 40774.9258\n",
      "Epoch 564/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 45994.9018 - val_loss: 41659.6289\n",
      "Epoch 565/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 49515.7361 - val_loss: 40751.7305\n",
      "Epoch 566/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 63344.0418 - val_loss: 42286.9844\n",
      "Epoch 567/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 72438.7686 - val_loss: 40509.8828\n",
      "Epoch 568/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 56702.7251 - val_loss: 41472.8672\n",
      "Epoch 569/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 61280.3737 - val_loss: 48127.3594\n",
      "Epoch 570/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 86621.3034 - val_loss: 40557.2305\n",
      "Epoch 571/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 85720.0806 - val_loss: 43064.2930\n",
      "Epoch 572/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 79637.9796 - val_loss: 42213.6953\n",
      "Epoch 573/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 59067.6797 - val_loss: 60718.2891\n",
      "Epoch 574/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 48437.0042 - val_loss: 65370.5938\n",
      "Epoch 575/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 51770.0576 - val_loss: 84809.4219\n",
      "Epoch 576/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 51875.4399 - val_loss: 77432.6328\n",
      "Epoch 577/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 64453.8376 - val_loss: 93658.7734\n",
      "Epoch 578/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 76635.0554 - val_loss: 64977.4922\n",
      "Epoch 579/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 84073.8327 - val_loss: 65791.2344\n",
      "Epoch 580/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 225us/step - loss: 67341.5898 - val_loss: 40125.6758\n",
      "Epoch 581/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 50929.8994 - val_loss: 39927.7773\n",
      "Epoch 582/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 48156.0307 - val_loss: 46247.8281\n",
      "Epoch 583/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37436.7737 - val_loss: 50490.9883\n",
      "Epoch 584/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42186.5888 - val_loss: 63986.4961\n",
      "Epoch 585/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44787.3875 - val_loss: 61021.8203\n",
      "Epoch 586/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54627.5407 - val_loss: 68192.3828\n",
      "Epoch 587/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 58114.5552 - val_loss: 51005.3633\n",
      "Epoch 588/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 71024.9327 - val_loss: 62371.5508\n",
      "Epoch 589/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 69900.6540 - val_loss: 42613.1367\n",
      "Epoch 590/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 55887.7450 - val_loss: 39861.2461\n",
      "Epoch 591/10000\n",
      "62/62 [==============================] - 0s 218us/step - loss: 52564.1440 - val_loss: 41393.8867\n",
      "Epoch 592/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 54672.7815 - val_loss: 39636.8203\n",
      "Epoch 593/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50652.7863 - val_loss: 44753.7070\n",
      "Epoch 594/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 55712.3163 - val_loss: 40154.0586\n",
      "Epoch 595/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 45154.3783 - val_loss: 48147.4492\n",
      "Epoch 596/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42459.5972 - val_loss: 45582.7734\n",
      "Epoch 597/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 52137.4197 - val_loss: 45678.8203\n",
      "Epoch 598/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 35095.9554 - val_loss: 50256.7305\n",
      "Epoch 599/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42114.3351 - val_loss: 62315.8203\n",
      "Epoch 600/10000\n",
      "62/62 [==============================] - 0s 220us/step - loss: 47037.7453 - val_loss: 57033.2539\n",
      "Epoch 601/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52274.8327 - val_loss: 45920.1016\n",
      "Epoch 602/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 38929.8964 - val_loss: 39629.9961\n",
      "Epoch 603/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42870.4252 - val_loss: 47913.1133\n",
      "Epoch 604/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44423.4734 - val_loss: 40444.1250\n",
      "Epoch 605/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 38830.4390 - val_loss: 38667.4336\n",
      "Epoch 606/10000\n",
      "62/62 [==============================] - 0s 226us/step - loss: 35393.1574 - val_loss: 45121.5000\n",
      "Epoch 607/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39098.2542 - val_loss: 44344.3867\n",
      "Epoch 608/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42102.2678 - val_loss: 46448.5000\n",
      "Epoch 609/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 39569.2438 - val_loss: 38466.0664\n",
      "Epoch 610/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36000.3978 - val_loss: 39264.9883\n",
      "Epoch 611/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 38047.0430 - val_loss: 43524.8086\n",
      "Epoch 612/10000\n",
      "62/62 [==============================] - 0s 547us/step - loss: 38580.3484 - val_loss: 40046.3945\n",
      "Epoch 613/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 57912.3911 - val_loss: 63928.5156\n",
      "Epoch 614/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 70178.7646 - val_loss: 54410.3164\n",
      "Epoch 615/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 69411.0499 - val_loss: 52936.5664\n",
      "Epoch 616/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 66251.7760 - val_loss: 41680.9453\n",
      "Epoch 617/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 64198.3873 - val_loss: 42823.2539\n",
      "Epoch 618/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 62134.7553 - val_loss: 38186.4922\n",
      "Epoch 619/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 82085.4151 - val_loss: 44456.2617\n",
      "Epoch 620/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 63287.5897 - val_loss: 38029.1250\n",
      "Epoch 621/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 55950.1202 - val_loss: 37953.9336\n",
      "Epoch 622/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46930.6540 - val_loss: 43735.5820\n",
      "Epoch 623/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 45811.2472 - val_loss: 41142.6133\n",
      "Epoch 624/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33744.0808 - val_loss: 66231.1641\n",
      "Epoch 625/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50784.6807 - val_loss: 47431.8008\n",
      "Epoch 626/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 62234.8594 - val_loss: 38839.9805\n",
      "Epoch 627/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 38874.6569 - val_loss: 38178.5703\n",
      "Epoch 628/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42353.4793 - val_loss: 39366.6953\n",
      "Epoch 629/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44146.4846 - val_loss: 37757.6758\n",
      "Epoch 630/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 45553.3852 - val_loss: 37577.7930\n",
      "Epoch 631/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36110.5785 - val_loss: 38294.8633\n",
      "Epoch 632/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50461.6179 - val_loss: 37481.8711\n",
      "Epoch 633/10000\n",
      "62/62 [==============================] - 0s 208us/step - loss: 38933.6947 - val_loss: 37446.0273\n",
      "Epoch 634/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36558.9502 - val_loss: 60588.6602\n",
      "Epoch 635/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 48097.0247 - val_loss: 40638.6445\n",
      "Epoch 636/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37728.0975 - val_loss: 43583.6328\n",
      "Epoch 637/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 37360.4782 - val_loss: 37996.6367\n",
      "Epoch 638/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 43531.5617 - val_loss: 37435.3789\n",
      "Epoch 639/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36505.5703 - val_loss: 38285.6562\n",
      "Epoch 640/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35897.2563 - val_loss: 41300.0234\n",
      "Epoch 641/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36105.2400 - val_loss: 39178.8945\n",
      "Epoch 642/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37087.2605 - val_loss: 40484.7539\n",
      "Epoch 643/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44486.5694 - val_loss: 48585.8008\n",
      "Epoch 644/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 48471.6574 - val_loss: 37650.2109\n",
      "Epoch 645/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 35487.5280 - val_loss: 38755.0078\n",
      "Epoch 646/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36209.1591 - val_loss: 38216.5547\n",
      "Epoch 647/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 37636.5921 - val_loss: 38790.5078\n",
      "Epoch 648/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40577.9272 - val_loss: 46002.0938\n",
      "Epoch 649/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42019.9378 - val_loss: 40418.8281\n",
      "Epoch 650/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 41196.9341 - val_loss: 43141.6719\n",
      "Epoch 651/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40676.1143 - val_loss: 38821.7344\n",
      "Epoch 652/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 49423.4834 - val_loss: 53681.7461\n",
      "Epoch 653/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 47320.5305 - val_loss: 39884.6641\n",
      "Epoch 654/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 49481.1570 - val_loss: 51276.6992\n",
      "Epoch 655/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65034.3317 - val_loss: 58057.3984\n",
      "Epoch 656/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 80085.6316 - val_loss: 80689.1406\n",
      "Epoch 657/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 80658.9362 - val_loss: 61015.8516\n",
      "Epoch 658/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 69535.6724 - val_loss: 59062.4336\n",
      "Epoch 659/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59322.5047 - val_loss: 45068.5664\n",
      "Epoch 660/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 54263.2939 - val_loss: 48816.0742\n",
      "Epoch 661/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54269.8895 - val_loss: 43905.3398\n",
      "Epoch 662/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 74841.9972 - val_loss: 75045.7812\n",
      "Epoch 663/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65946.4357 - val_loss: 44825.8203\n",
      "Epoch 664/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 55610.8339 - val_loss: 52604.0117\n",
      "Epoch 665/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45330.9371 - val_loss: 36202.7109\n",
      "Epoch 666/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 40150.5804 - val_loss: 39617.9922\n",
      "Epoch 667/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 39348.4252 - val_loss: 36681.0781\n",
      "Epoch 668/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 35732.4308 - val_loss: 36079.1680\n",
      "Epoch 669/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 36894.6241 - val_loss: 36003.4844\n",
      "Epoch 670/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36835.3453 - val_loss: 38042.1719\n",
      "Epoch 671/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 38866.0505 - val_loss: 37354.7109\n",
      "Epoch 672/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 41804.6035 - val_loss: 44900.9023\n",
      "Epoch 673/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 47223.2461 - val_loss: 45712.2891\n",
      "Epoch 674/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 47624.8658 - val_loss: 49939.8008\n",
      "Epoch 675/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49361.4338 - val_loss: 48724.4180\n",
      "Epoch 676/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 46578.2901 - val_loss: 40463.5234\n",
      "Epoch 677/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 42998.6038 - val_loss: 44416.5664\n",
      "Epoch 678/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45202.9751 - val_loss: 50645.4727\n",
      "Epoch 679/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42990.2542 - val_loss: 40586.6992\n",
      "Epoch 680/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 41781.4296 - val_loss: 49608.5352\n",
      "Epoch 681/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 41310.5914 - val_loss: 37713.5156\n",
      "Epoch 682/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39301.1499 - val_loss: 48548.5039\n",
      "Epoch 683/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 58705.7739 - val_loss: 73520.5469\n",
      "Epoch 684/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 76429.8102 - val_loss: 106176.8125\n",
      "Epoch 685/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 79218.7111 - val_loss: 75903.6719\n",
      "Epoch 686/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 71204.6503 - val_loss: 100353.2109\n",
      "Epoch 687/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 72096.4968 - val_loss: 70567.1953\n",
      "Epoch 688/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59678.1262 - val_loss: 62932.7305\n",
      "Epoch 689/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 51026.8887 - val_loss: 37653.1914\n",
      "Epoch 690/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35199.6091 - val_loss: 43735.1133\n",
      "Epoch 691/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 38221.0974 - val_loss: 50385.7031\n",
      "Epoch 692/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49275.2219 - val_loss: 82596.3047\n",
      "Epoch 693/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 65846.5756 - val_loss: 97047.3438\n",
      "Epoch 694/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 76841.6560 - val_loss: 89899.7656\n",
      "Epoch 695/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 61914.1231 - val_loss: 77977.0469\n",
      "Epoch 696/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 63016.7157 - val_loss: 67561.0781\n",
      "Epoch 697/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 47328.3385 - val_loss: 60411.0312\n",
      "Epoch 698/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 49405.4030 - val_loss: 85139.9531\n",
      "Epoch 699/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 56950.2784 - val_loss: 76608.4219\n",
      "Epoch 700/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59117.3296 - val_loss: 94663.6953\n",
      "Epoch 701/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 64041.1535 - val_loss: 60757.0273\n",
      "Epoch 702/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50146.1515 - val_loss: 61418.3086\n",
      "Epoch 703/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46894.3378 - val_loss: 44499.6836\n",
      "Epoch 704/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45114.3642 - val_loss: 41315.3242\n",
      "Epoch 705/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 34068.5740 - val_loss: 41807.4258\n",
      "Epoch 706/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 36193.3672 - val_loss: 61315.0469\n",
      "Epoch 707/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 48980.8501 - val_loss: 40658.4492\n",
      "Epoch 708/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 46848.8082 - val_loss: 35629.6211\n",
      "Epoch 709/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46357.8509 - val_loss: 39057.6914\n",
      "Epoch 710/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 42812.6828 - val_loss: 37694.9297\n",
      "Epoch 711/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37722.5500 - val_loss: 35549.9688\n",
      "Epoch 712/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35067.2377 - val_loss: 40689.3672\n",
      "Epoch 713/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35321.7062 - val_loss: 37572.0430\n",
      "Epoch 714/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 38655.9928 - val_loss: 35057.2109\n",
      "Epoch 715/10000\n",
      "62/62 [==============================] - 0s 218us/step - loss: 33953.3154 - val_loss: 34622.3945\n",
      "Epoch 716/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34902.0122 - val_loss: 42423.8789\n",
      "Epoch 717/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36636.3589 - val_loss: 41269.2539\n",
      "Epoch 718/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 43142.0939 - val_loss: 71658.2109\n",
      "Epoch 719/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 51831.8116 - val_loss: 55284.7461\n",
      "Epoch 720/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46477.3896 - val_loss: 65776.5547\n",
      "Epoch 721/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 46278.9088 - val_loss: 66687.7031\n",
      "Epoch 722/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 52279.7931 - val_loss: 77425.5078\n",
      "Epoch 723/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 68250.0528 - val_loss: 37150.9297\n",
      "Epoch 724/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 209us/step - loss: 44193.7031 - val_loss: 35053.8594\n",
      "Epoch 725/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 32954.4391 - val_loss: 35814.2695\n",
      "Epoch 726/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 33987.6702 - val_loss: 41509.0273\n",
      "Epoch 727/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34379.9110 - val_loss: 40564.2109\n",
      "Epoch 728/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36222.7911 - val_loss: 58897.5234\n",
      "Epoch 729/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46308.7823 - val_loss: 40134.5781\n",
      "Epoch 730/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 35536.8034 - val_loss: 63163.9727\n",
      "Epoch 731/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45515.6049 - val_loss: 49682.2227\n",
      "Epoch 732/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 40614.3403 - val_loss: 79572.8438\n",
      "Epoch 733/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 51182.2931 - val_loss: 81303.7969\n",
      "Epoch 734/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 59455.9449 - val_loss: 90173.9062\n",
      "Epoch 735/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 62937.0523 - val_loss: 57802.9414\n",
      "Epoch 736/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 45412.4252 - val_loss: 70008.4062\n",
      "Epoch 737/10000\n",
      "62/62 [==============================] - 0s 579us/step - loss: 60508.2442 - val_loss: 40610.0312\n",
      "Epoch 738/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44918.3678 - val_loss: 39420.1562\n",
      "Epoch 739/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 51697.6129 - val_loss: 34610.5898\n",
      "Epoch 740/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 46085.0983 - val_loss: 35829.6523\n",
      "Epoch 741/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42407.8890 - val_loss: 41573.3398\n",
      "Epoch 742/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40839.1736 - val_loss: 39369.1992\n",
      "Epoch 743/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 40058.2301 - val_loss: 47176.3320\n",
      "Epoch 744/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39393.7799 - val_loss: 36768.2109\n",
      "Epoch 745/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36493.6382 - val_loss: 52359.8359\n",
      "Epoch 746/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 42841.9553 - val_loss: 36639.4219\n",
      "Epoch 747/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37333.1399 - val_loss: 35405.2070\n",
      "Epoch 748/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 32372.7851 - val_loss: 36078.0273\n",
      "Epoch 749/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 41289.7761 - val_loss: 33385.0234\n",
      "Epoch 750/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40087.9679 - val_loss: 41854.9531\n",
      "Epoch 751/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46762.5435 - val_loss: 56030.6406\n",
      "Epoch 752/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49737.6663 - val_loss: 55365.9414\n",
      "Epoch 753/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42564.5152 - val_loss: 57705.9805\n",
      "Epoch 754/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 52882.6683 - val_loss: 44480.6445\n",
      "Epoch 755/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36801.2679 - val_loss: 38663.1523\n",
      "Epoch 756/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40772.9963 - val_loss: 35967.2109\n",
      "Epoch 757/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 33362.4984 - val_loss: 34471.8828\n",
      "Epoch 758/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34421.5629 - val_loss: 36253.2656\n",
      "Epoch 759/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 32257.5628 - val_loss: 37839.3242\n",
      "Epoch 760/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 41791.8398 - val_loss: 33707.9180\n",
      "Epoch 761/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37966.8126 - val_loss: 35891.0742\n",
      "Epoch 762/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 33991.6605 - val_loss: 34665.4570\n",
      "Epoch 763/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34454.0801 - val_loss: 33745.0820\n",
      "Epoch 764/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 41049.1762 - val_loss: 42916.3906\n",
      "Epoch 765/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 52295.1436 - val_loss: 94271.7422\n",
      "Epoch 766/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 64884.8686 - val_loss: 99889.7812\n",
      "Epoch 767/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 70172.6557 - val_loss: 112618.0312\n",
      "Epoch 768/10000\n",
      "62/62 [==============================] - 0s 249us/step - loss: 62515.7185 - val_loss: 145302.4062\n",
      "Epoch 769/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 96270.3602 - val_loss: 131293.1094\n",
      "Epoch 770/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 85225.1176 - val_loss: 85051.6016\n",
      "Epoch 771/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 104825.0292 - val_loss: 42231.7891\n",
      "Epoch 772/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 82236.0418 - val_loss: 36744.5820\n",
      "Epoch 773/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 59946.6096 - val_loss: 41073.8594\n",
      "Epoch 774/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 60775.7462 - val_loss: 71201.5859\n",
      "Epoch 775/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 58571.8415 - val_loss: 67850.1797\n",
      "Epoch 776/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 55380.5522 - val_loss: 68661.9531\n",
      "Epoch 777/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 47374.8843 - val_loss: 97498.7188\n",
      "Epoch 778/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 70938.8615 - val_loss: 89908.7422\n",
      "Epoch 779/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54984.3477 - val_loss: 78045.9609\n",
      "Epoch 780/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 64257.7169 - val_loss: 68295.0938\n",
      "Epoch 781/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 58979.3367 - val_loss: 41510.8242\n",
      "Epoch 782/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 55501.6439 - val_loss: 34349.4727\n",
      "Epoch 783/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 37703.1289 - val_loss: 32840.8477\n",
      "Epoch 784/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34311.7814 - val_loss: 35218.0430\n",
      "Epoch 785/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34891.7325 - val_loss: 32769.2109\n",
      "Epoch 786/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 47990.2988 - val_loss: 38594.3789\n",
      "Epoch 787/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 38792.6165 - val_loss: 38506.3906\n",
      "Epoch 788/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 34677.4180 - val_loss: 33449.8047\n",
      "Epoch 789/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 32899.7715 - val_loss: 36764.2383\n",
      "Epoch 790/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 39051.2630 - val_loss: 35540.1836\n",
      "Epoch 791/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 34935.7461 - val_loss: 35192.1914\n",
      "Epoch 792/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40756.2859 - val_loss: 41616.9609\n",
      "Epoch 793/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40751.8066 - val_loss: 55617.9062\n",
      "Epoch 794/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 47715.8605 - val_loss: 91312.9062\n",
      "Epoch 795/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 63449.6324 - val_loss: 55090.7070\n",
      "Epoch 796/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40056.5430 - val_loss: 84916.4844\n",
      "Epoch 797/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 101940.1336 - val_loss: 32700.9355\n",
      "Epoch 798/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 61601.3989 - val_loss: 45962.5820\n",
      "Epoch 799/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54904.1928 - val_loss: 72133.9375\n",
      "Epoch 800/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 53674.0169 - val_loss: 47106.6484\n",
      "Epoch 801/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49675.3611 - val_loss: 33952.5352\n",
      "Epoch 802/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 39722.5096 - val_loss: 35797.8906\n",
      "Epoch 803/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 38461.4815 - val_loss: 40288.5312\n",
      "Epoch 804/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39177.4032 - val_loss: 52833.3086\n",
      "Epoch 805/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40530.8695 - val_loss: 45041.1680\n",
      "Epoch 806/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42694.2876 - val_loss: 39898.0664\n",
      "Epoch 807/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44695.2835 - val_loss: 33038.4258\n",
      "Epoch 808/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44074.8353 - val_loss: 42858.9102\n",
      "Epoch 809/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 41513.2767 - val_loss: 35814.8008\n",
      "Epoch 810/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37157.7869 - val_loss: 34698.3594\n",
      "Epoch 811/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39758.1051 - val_loss: 50189.1016\n",
      "Epoch 812/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 48305.6324 - val_loss: 106706.9219\n",
      "Epoch 813/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65711.1350 - val_loss: 87727.2500\n",
      "Epoch 814/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 79141.1799 - val_loss: 50478.5547\n",
      "Epoch 815/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59803.5869 - val_loss: 31914.3535\n",
      "Epoch 816/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 51372.2994 - val_loss: 37108.4297\n",
      "Epoch 817/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49261.0590 - val_loss: 64768.6055\n",
      "Epoch 818/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59814.7794 - val_loss: 121258.5859\n",
      "Epoch 819/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 81086.6271 - val_loss: 131861.4375\n",
      "Epoch 820/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 89630.9025 - val_loss: 68812.1562\n",
      "Epoch 821/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50819.3460 - val_loss: 77073.5703\n",
      "Epoch 822/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 77352.4501 - val_loss: 34788.1211\n",
      "Epoch 823/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 87427.1184 - val_loss: 48416.5078\n",
      "Epoch 824/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 64123.4045 - val_loss: 88164.9922\n",
      "Epoch 825/10000\n",
      "62/62 [==============================] - 0s 230us/step - loss: 63803.9470 - val_loss: 48848.4414\n",
      "Epoch 826/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 47175.1545 - val_loss: 37874.2070\n",
      "Epoch 827/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 57402.5232 - val_loss: 45618.1602\n",
      "Epoch 828/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45819.2988 - val_loss: 57956.0195\n",
      "Epoch 829/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 47986.7542 - val_loss: 67639.9609\n",
      "Epoch 830/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46426.6288 - val_loss: 55487.5078\n",
      "Epoch 831/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44303.6050 - val_loss: 60902.4023\n",
      "Epoch 832/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 39007.3601 - val_loss: 64887.7148\n",
      "Epoch 833/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 51896.2668 - val_loss: 62794.2773\n",
      "Epoch 834/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42519.7899 - val_loss: 58090.4297\n",
      "Epoch 835/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 73449.1741 - val_loss: 32681.6992\n",
      "Epoch 836/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 37293.4574 - val_loss: 31463.3535\n",
      "Epoch 837/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 77980.8163 - val_loss: 83866.7109\n",
      "Epoch 838/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 68412.1564 - val_loss: 62451.1328\n",
      "Epoch 839/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42189.3824 - val_loss: 67448.0547\n",
      "Epoch 840/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 59142.9031 - val_loss: 47455.4258\n",
      "Epoch 841/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 40761.9124 - val_loss: 36694.9531\n",
      "Epoch 842/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36640.7179 - val_loss: 38145.9336\n",
      "Epoch 843/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 37452.0980 - val_loss: 31753.5293\n",
      "Epoch 844/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 57942.5849 - val_loss: 51568.7344\n",
      "Epoch 845/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50024.5541 - val_loss: 83831.5000\n",
      "Epoch 846/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54725.8104 - val_loss: 133955.0156\n",
      "Epoch 847/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 109578.4005 - val_loss: 65152.4062\n",
      "Epoch 848/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 65305.4212 - val_loss: 33490.9336\n",
      "Epoch 849/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 59342.0391 - val_loss: 34473.3633\n",
      "Epoch 850/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36673.8917 - val_loss: 32127.1250\n",
      "Epoch 851/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 32629.7768 - val_loss: 39895.2656\n",
      "Epoch 852/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33417.0920 - val_loss: 45649.2734\n",
      "Epoch 853/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45889.9407 - val_loss: 34573.7305\n",
      "Epoch 854/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33969.1515 - val_loss: 31200.1699\n",
      "Epoch 855/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33293.7114 - val_loss: 31123.3105\n",
      "Epoch 856/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 31081.7321 - val_loss: 33323.6758\n",
      "Epoch 857/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33779.0290 - val_loss: 63594.3398\n",
      "Epoch 858/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 44613.8804 - val_loss: 51654.8320\n",
      "Epoch 859/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 41909.5339 - val_loss: 56371.7539\n",
      "Epoch 860/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46953.3325 - val_loss: 36966.6172\n",
      "Epoch 861/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 43239.5994 - val_loss: 31811.0332\n",
      "Epoch 862/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 49049.6368 - val_loss: 48913.6992\n",
      "Epoch 863/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59893.6613 - val_loss: 125655.5078\n",
      "Epoch 864/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 79593.3886 - val_loss: 172458.5469\n",
      "Epoch 865/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 133272.3647 - val_loss: 52802.1172\n",
      "Epoch 866/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 94628.6983 - val_loss: 32811.4219\n",
      "Epoch 867/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46856.3140 - val_loss: 42161.5117\n",
      "Epoch 868/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 225us/step - loss: 43241.5376 - val_loss: 52882.1406\n",
      "Epoch 869/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 57757.9604 - val_loss: 30830.3809\n",
      "Epoch 870/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 31604.5372 - val_loss: 35229.2305\n",
      "Epoch 871/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 32726.5480 - val_loss: 51347.6680\n",
      "Epoch 872/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44038.1597 - val_loss: 32726.2324\n",
      "Epoch 873/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 48049.9412 - val_loss: 36507.2539\n",
      "Epoch 874/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 38988.2897 - val_loss: 31335.5957\n",
      "Epoch 875/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45536.4841 - val_loss: 51294.8164\n",
      "Epoch 876/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44302.9359 - val_loss: 48181.8633\n",
      "Epoch 877/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36587.6178 - val_loss: 43211.0156\n",
      "Epoch 878/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 53489.9226 - val_loss: 30814.1348\n",
      "Epoch 879/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 41087.1914 - val_loss: 49357.4492\n",
      "Epoch 880/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44962.4740 - val_loss: 87052.6797\n",
      "Epoch 881/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 58119.5362 - val_loss: 112264.1797\n",
      "Epoch 882/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 68293.3686 - val_loss: 78082.7969\n",
      "Epoch 883/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 133759.0660 - val_loss: 36284.7617\n",
      "Epoch 884/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52798.0968 - val_loss: 58005.5859\n",
      "Epoch 885/10000\n",
      "62/62 [==============================] - 0s 243us/step - loss: 46215.8107 - val_loss: 54869.3594\n",
      "Epoch 886/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 60213.5081 - val_loss: 30594.6270\n",
      "Epoch 887/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 53187.8916 - val_loss: 78814.7188\n",
      "Epoch 888/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 58351.6385 - val_loss: 41027.1172\n",
      "Epoch 889/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36238.2564 - val_loss: 44826.6836\n",
      "Epoch 890/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 32278.8828 - val_loss: 55180.4141\n",
      "Epoch 891/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 46655.7828 - val_loss: 49038.6016\n",
      "Epoch 892/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 82314.6376 - val_loss: 49406.6641\n",
      "Epoch 893/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 67293.9904 - val_loss: 139153.1719\n",
      "Epoch 894/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 100001.9732 - val_loss: 85320.0000\n",
      "Epoch 895/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 50084.4839 - val_loss: 75199.5781\n",
      "Epoch 896/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 98679.5789 - val_loss: 30409.0000\n",
      "Epoch 897/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54535.3876 - val_loss: 53101.2891\n",
      "Epoch 898/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 48289.8632 - val_loss: 73206.0234\n",
      "Epoch 899/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 54445.0373 - val_loss: 72716.9688\n",
      "Epoch 900/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 55890.5722 - val_loss: 39581.1445\n",
      "Epoch 901/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 76865.4315 - val_loss: 46618.5312\n",
      "Epoch 902/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 50982.6803 - val_loss: 90183.4219\n",
      "Epoch 903/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 55242.6555 - val_loss: 96166.9609\n",
      "Epoch 904/10000\n",
      "62/62 [==============================] - 0s 218us/step - loss: 68084.4260 - val_loss: 75929.7734\n",
      "Epoch 905/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 82121.1724 - val_loss: 30305.1387\n",
      "Epoch 906/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59207.3916 - val_loss: 54824.6641\n",
      "Epoch 907/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 58070.6730 - val_loss: 144968.6406\n",
      "Epoch 908/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 81774.5919 - val_loss: 113989.3906\n",
      "Epoch 909/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 88651.7033 - val_loss: 65570.2812\n",
      "Epoch 910/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 75959.0718 - val_loss: 30242.3262\n",
      "Epoch 911/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 53030.3982 - val_loss: 41062.2266\n",
      "Epoch 912/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 55787.2555 - val_loss: 123175.2812\n",
      "Epoch 913/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 79226.5649 - val_loss: 68082.6484\n",
      "Epoch 914/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 68215.1885 - val_loss: 35255.4688\n",
      "Epoch 915/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 60648.8322 - val_loss: 48896.4531\n",
      "Epoch 916/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59748.0754 - val_loss: 133165.9375\n",
      "Epoch 917/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 84262.1632 - val_loss: 128023.3906\n",
      "Epoch 918/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 90671.7417 - val_loss: 54138.8516\n",
      "Epoch 919/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65093.0325 - val_loss: 32251.7461\n",
      "Epoch 920/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52249.9884 - val_loss: 40922.4570\n",
      "Epoch 921/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 72232.8904 - val_loss: 170258.1875\n",
      "Epoch 922/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 102622.9781 - val_loss: 167935.6250\n",
      "Epoch 923/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 135388.7631 - val_loss: 42193.2891\n",
      "Epoch 924/10000\n",
      "62/62 [==============================] - 0s 579us/step - loss: 120286.5675 - val_loss: 70873.5234\n",
      "Epoch 925/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 65977.9341 - val_loss: 100221.7344\n",
      "Epoch 926/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 58719.3768 - val_loss: 102653.0547\n",
      "Epoch 927/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 67123.7434 - val_loss: 90474.6641\n",
      "Epoch 928/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 93403.0058 - val_loss: 29971.4023\n",
      "Epoch 929/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 41508.6985 - val_loss: 30728.2773\n",
      "Epoch 930/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 55652.1772 - val_loss: 101579.5312\n",
      "Epoch 931/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65653.8909 - val_loss: 67574.5781\n",
      "Epoch 932/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 54995.0078 - val_loss: 49795.1172\n",
      "Epoch 933/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65157.0610 - val_loss: 35121.6562\n",
      "Epoch 934/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 36260.1057 - val_loss: 33899.8750\n",
      "Epoch 935/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 49866.1520 - val_loss: 139178.0000\n",
      "Epoch 936/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 95108.8047 - val_loss: 54198.0430\n",
      "Epoch 937/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 67722.5388 - val_loss: 29844.0664\n",
      "Epoch 938/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 51376.6691 - val_loss: 61623.6016\n",
      "Epoch 939/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 48831.1298 - val_loss: 33876.5234\n",
      "Epoch 940/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 49583.2377 - val_loss: 43350.5586\n",
      "Epoch 941/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39515.0121 - val_loss: 73808.7500\n",
      "Epoch 942/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 63883.3088 - val_loss: 30461.2715\n",
      "Epoch 943/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 51725.5984 - val_loss: 51457.9453\n",
      "Epoch 944/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 60349.0305 - val_loss: 222508.6562\n",
      "Epoch 945/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 101026.9728 - val_loss: 272932.9375\n",
      "Epoch 946/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 236431.7001 - val_loss: 52420.1016\n",
      "Epoch 947/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 125389.8135 - val_loss: 64770.9219\n",
      "Epoch 948/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 78767.1439 - val_loss: 157492.0938\n",
      "Epoch 949/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 108014.7955 - val_loss: 87962.2734\n",
      "Epoch 950/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 61881.2595 - val_loss: 47191.3906\n",
      "Epoch 951/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 64254.3934 - val_loss: 30037.0215\n",
      "Epoch 952/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65602.5249 - val_loss: 120955.3672\n",
      "Epoch 953/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 69129.3041 - val_loss: 171634.9062\n",
      "Epoch 954/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 168174.1779 - val_loss: 31645.8535\n",
      "Epoch 955/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 93371.6094 - val_loss: 118164.3203\n",
      "Epoch 956/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 75250.1798 - val_loss: 131228.9219\n",
      "Epoch 957/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 100620.3599 - val_loss: 55167.0469\n",
      "Epoch 958/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 74257.9962 - val_loss: 35011.6836\n",
      "Epoch 959/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 48556.8570 - val_loss: 68158.0938\n",
      "Epoch 960/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 53849.6692 - val_loss: 202190.2969\n",
      "Epoch 961/10000\n",
      "62/62 [==============================] - 0s 214us/step - loss: 132045.2190 - val_loss: 69093.0391\n",
      "Epoch 962/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 70504.3231 - val_loss: 35603.8086\n",
      "Epoch 963/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 86589.8810 - val_loss: 90384.4531\n",
      "Epoch 964/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65407.0854 - val_loss: 44845.2734\n",
      "Epoch 965/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50188.2233 - val_loss: 29315.2305\n",
      "Epoch 966/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 38050.2126 - val_loss: 53710.3633\n",
      "Epoch 967/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 45397.0988 - val_loss: 154540.5156\n",
      "Epoch 968/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 86333.1055 - val_loss: 152011.8594\n",
      "Epoch 969/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 193384.0600 - val_loss: 46021.9688\n",
      "Epoch 970/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 74142.4221 - val_loss: 138403.9531\n",
      "Epoch 971/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 84365.7429 - val_loss: 155813.4531\n",
      "Epoch 972/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 130967.3143 - val_loss: 31621.6660\n",
      "Epoch 973/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 100728.9551 - val_loss: 114948.2891\n",
      "Epoch 974/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 85419.1064 - val_loss: 332276.3125\n",
      "Epoch 975/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 201662.4640 - val_loss: 110131.7500\n",
      "Epoch 976/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 223677.1724 - val_loss: 77725.7812\n",
      "Epoch 977/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 129699.8246 - val_loss: 445302.3125\n",
      "Epoch 978/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 225141.4863 - val_loss: 294170.0625\n",
      "Epoch 979/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 359528.0968 - val_loss: 33110.4844\n",
      "Epoch 980/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 173929.7268 - val_loss: 374041.7500\n",
      "Epoch 981/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 200512.8673 - val_loss: 261192.1719\n",
      "Epoch 982/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 340145.2026 - val_loss: 48284.8594\n",
      "Epoch 983/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 82251.3677 - val_loss: 152322.1875\n",
      "Epoch 984/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 96636.5911 - val_loss: 68937.0469\n",
      "Epoch 985/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 109908.7566 - val_loss: 50034.5898\n",
      "Epoch 986/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 47457.6736 - val_loss: 31251.0879\n",
      "Epoch 987/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 37262.6376 - val_loss: 51001.0977\n",
      "Epoch 988/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 37866.9909 - val_loss: 42587.6172\n",
      "Epoch 989/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 36835.2059 - val_loss: 39253.4727\n",
      "Epoch 990/10000\n",
      "62/62 [==============================] - 0s 231us/step - loss: 41074.1879 - val_loss: 30607.4609\n",
      "Epoch 991/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45183.6222 - val_loss: 79791.7422\n",
      "Epoch 992/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 57356.2468 - val_loss: 65268.5234\n",
      "Epoch 993/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 85788.9385 - val_loss: 49824.0664\n",
      "Epoch 994/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 43445.4497 - val_loss: 29324.6426\n",
      "Epoch 995/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37339.4321 - val_loss: 60347.2031\n",
      "Epoch 996/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 78777.6930 - val_loss: 41778.9219\n",
      "Epoch 997/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 43370.7574 - val_loss: 117121.5625\n",
      "Epoch 998/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 119495.3876 - val_loss: 37293.5430\n",
      "Epoch 999/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50480.1517 - val_loss: 105197.7969\n",
      "Epoch 1000/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 76143.5326 - val_loss: 59519.8789\n",
      "Epoch 1001/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 88946.4945 - val_loss: 59162.8203\n",
      "Epoch 1002/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50018.8151 - val_loss: 107837.6797\n",
      "Epoch 1003/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 84605.8371 - val_loss: 43912.2930\n",
      "Epoch 1004/10000\n",
      "62/62 [==============================] - 0s 234us/step - loss: 102247.7213 - val_loss: 131919.2656\n",
      "Epoch 1005/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 77593.7912 - val_loss: 113321.5234\n",
      "Epoch 1006/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 151247.4405 - val_loss: 44859.9414\n",
      "Epoch 1007/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50184.0827 - val_loss: 94596.4141\n",
      "Epoch 1008/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 94703.3289 - val_loss: 36418.9766\n",
      "Epoch 1009/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 39060.4342 - val_loss: 50037.0508\n",
      "Epoch 1010/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33787.2193 - val_loss: 91065.5000\n",
      "Epoch 1011/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 77269.3425 - val_loss: 36077.4414\n",
      "Epoch 1012/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 225us/step - loss: 63625.8679 - val_loss: 68093.5859\n",
      "Epoch 1013/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 53314.8376 - val_loss: 171966.0469\n",
      "Epoch 1014/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 163159.7220 - val_loss: 29158.5879\n",
      "Epoch 1015/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54906.0376 - val_loss: 79326.2734\n",
      "Epoch 1016/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 63370.0833 - val_loss: 29706.5098\n",
      "Epoch 1017/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36374.5946 - val_loss: 36623.5859\n",
      "Epoch 1018/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33876.7800 - val_loss: 37029.7422\n",
      "Epoch 1019/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39011.1408 - val_loss: 33974.5859\n",
      "Epoch 1020/10000\n",
      "62/62 [==============================] - 0s 193us/step - loss: 32353.4580 - val_loss: 30166.8945\n",
      "Epoch 1021/10000\n",
      "62/62 [==============================] - 0s 235us/step - loss: 42358.7014 - val_loss: 84617.7266\n",
      "Epoch 1022/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 67681.1525 - val_loss: 29543.2344\n",
      "Epoch 1023/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 48579.0461 - val_loss: 63314.9453\n",
      "Epoch 1024/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50163.5571 - val_loss: 45305.1133\n",
      "Epoch 1025/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42638.6719 - val_loss: 29235.5098\n",
      "Epoch 1026/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 52136.8034 - val_loss: 111931.1875\n",
      "Epoch 1027/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 109128.9186 - val_loss: 28996.5391\n",
      "Epoch 1028/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 39673.7597 - val_loss: 57616.2656\n",
      "Epoch 1029/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 49890.8746 - val_loss: 29024.0664\n",
      "Epoch 1030/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 29415.2351 - val_loss: 29232.2109\n",
      "Epoch 1031/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34074.2564 - val_loss: 71735.7812\n",
      "Epoch 1032/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 77874.6736 - val_loss: 43606.8320\n",
      "Epoch 1033/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37271.1581 - val_loss: 29702.4824\n",
      "Epoch 1034/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 29512.7554 - val_loss: 29656.4766\n",
      "Epoch 1035/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 31817.0767 - val_loss: 37054.8047\n",
      "Epoch 1036/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 38742.4127 - val_loss: 37087.0859\n",
      "Epoch 1037/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42711.5984 - val_loss: 48962.6797\n",
      "Epoch 1038/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 37462.7384 - val_loss: 35532.1055\n",
      "Epoch 1039/10000\n",
      "62/62 [==============================] - 0s 224us/step - loss: 43702.2064 - val_loss: 34267.6602\n",
      "Epoch 1040/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 33270.3303 - val_loss: 53445.7891\n",
      "Epoch 1041/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 39861.7715 - val_loss: 34796.6758\n",
      "Epoch 1042/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 81657.7404 - val_loss: 134182.2188\n",
      "Epoch 1043/10000\n",
      "62/62 [==============================] - 0s 208us/step - loss: 74651.5062 - val_loss: 177321.7188\n",
      "Epoch 1044/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 213374.0111 - val_loss: 73454.9766\n",
      "Epoch 1045/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 59360.2227 - val_loss: 91009.5547\n",
      "Epoch 1046/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 97840.6709 - val_loss: 33240.1133\n",
      "Epoch 1047/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33990.7111 - val_loss: 39548.8281\n",
      "Epoch 1048/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 33806.8768 - val_loss: 29569.6719\n",
      "Epoch 1049/10000\n",
      "62/62 [==============================] - 0s 627us/step - loss: 28567.2536 - val_loss: 32094.2363\n",
      "Epoch 1050/10000\n",
      "62/62 [==============================] - 0s 218us/step - loss: 38128.4119 - val_loss: 46163.8594\n",
      "Epoch 1051/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 34693.2352 - val_loss: 48802.1133\n",
      "Epoch 1052/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 71676.6562 - val_loss: 58029.7266\n",
      "Epoch 1053/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 50270.2650 - val_loss: 32658.5449\n",
      "Epoch 1054/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 29642.7987 - val_loss: 29161.6016\n",
      "Epoch 1055/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 28894.0033 - val_loss: 29362.2441\n",
      "Epoch 1056/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33815.9342 - val_loss: 45492.4492\n",
      "Epoch 1057/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39925.2182 - val_loss: 28583.5781\n",
      "Epoch 1058/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 71476.6998 - val_loss: 234532.5938\n",
      "Epoch 1059/10000\n",
      "62/62 [==============================] - 0s 222us/step - loss: 193226.1089 - val_loss: 30479.8887\n",
      "Epoch 1060/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 55677.0633 - val_loss: 72854.3281\n",
      "Epoch 1061/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 49572.6527 - val_loss: 54755.7773\n",
      "Epoch 1062/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 66337.0106 - val_loss: 37008.0469\n",
      "Epoch 1063/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 45982.9693 - val_loss: 159777.9375\n",
      "Epoch 1064/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 109027.3938 - val_loss: 35447.1406\n",
      "Epoch 1065/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 104268.7409 - val_loss: 181820.0625\n",
      "Epoch 1066/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 142961.9745 - val_loss: 33318.6758\n",
      "Epoch 1067/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59183.4161 - val_loss: 86137.4219\n",
      "Epoch 1068/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 72707.6595 - val_loss: 29618.6270\n",
      "Epoch 1069/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34526.7591 - val_loss: 55024.3359\n",
      "Epoch 1070/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 68017.4418 - val_loss: 44277.3164\n",
      "Epoch 1071/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 40961.1808 - val_loss: 29539.0879\n",
      "Epoch 1072/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 28450.1300 - val_loss: 28323.8887\n",
      "Epoch 1073/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 28434.7539 - val_loss: 28327.3379\n",
      "Epoch 1074/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42495.2206 - val_loss: 132234.9062\n",
      "Epoch 1075/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 172512.9133 - val_loss: 121774.9219\n",
      "Epoch 1076/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 72319.0267 - val_loss: 351711.4375\n",
      "Epoch 1077/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 421785.7127 - val_loss: 112472.8438\n",
      "Epoch 1078/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 94688.2780 - val_loss: 283024.7812\n",
      "Epoch 1079/10000\n",
      "62/62 [==============================] - 0s 234us/step - loss: 180320.8503 - val_loss: 47896.7734\n",
      "Epoch 1080/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 120964.8009 - val_loss: 140242.9531\n",
      "Epoch 1081/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 85084.5581 - val_loss: 426980.7812\n",
      "Epoch 1082/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 602881.5060 - val_loss: 365320.7500\n",
      "Epoch 1083/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 196463.9793 - val_loss: 302324.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1084/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 343914.6562 - val_loss: 89140.1562\n",
      "Epoch 1085/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 83631.9428 - val_loss: 259485.6406\n",
      "Epoch 1086/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 233476.9214 - val_loss: 44920.2930\n",
      "Epoch 1087/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 67501.6399 - val_loss: 225788.6875\n",
      "Epoch 1088/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 152090.5045 - val_loss: 52469.0000\n",
      "Epoch 1089/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 173126.3821 - val_loss: 347992.1875\n",
      "Epoch 1090/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 164136.1660 - val_loss: 270187.4062\n",
      "Epoch 1091/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 407289.2137 - val_loss: 222878.2812\n",
      "Epoch 1092/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 134665.6706 - val_loss: 339827.8438\n",
      "Epoch 1093/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 367095.0141 - val_loss: 115990.2344\n",
      "Epoch 1094/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 96105.9844 - val_loss: 382568.4688\n",
      "Epoch 1095/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 423024.7450 - val_loss: 119620.9922\n",
      "Epoch 1096/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 100616.0559 - val_loss: 455524.9375\n",
      "Epoch 1097/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 411566.0948 - val_loss: 63443.7734\n",
      "Epoch 1098/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 75980.5373 - val_loss: 215750.1562\n",
      "Epoch 1099/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 135229.1074 - val_loss: 69988.6094\n",
      "Epoch 1100/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 124501.4572 - val_loss: 137375.9531\n",
      "Epoch 1101/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 84793.5822 - val_loss: 62796.3906\n",
      "Epoch 1102/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 70854.4877 - val_loss: 36087.2734\n",
      "Epoch 1103/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34775.4146 - val_loss: 30428.9551\n",
      "Epoch 1104/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 29214.7245 - val_loss: 28718.2676\n",
      "Epoch 1105/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 30703.2995 - val_loss: 50189.6797\n",
      "Epoch 1106/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40172.7883 - val_loss: 35920.7500\n",
      "Epoch 1107/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46480.3819 - val_loss: 57684.4062\n",
      "Epoch 1108/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 69013.7445 - val_loss: 74398.7734\n",
      "Epoch 1109/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 65455.9753 - val_loss: 33376.2773\n",
      "Epoch 1110/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 30976.3925 - val_loss: 41194.7227\n",
      "Epoch 1111/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 30969.6838 - val_loss: 74054.3594\n",
      "Epoch 1112/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 75135.6608 - val_loss: 44461.1016\n",
      "Epoch 1113/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37572.2937 - val_loss: 28479.1855\n",
      "Epoch 1114/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 31305.5518 - val_loss: 85349.7734\n",
      "Epoch 1115/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 71242.2437 - val_loss: 29851.1426\n",
      "Epoch 1116/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45160.7790 - val_loss: 116106.9297\n",
      "Epoch 1117/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 90926.2870 - val_loss: 30029.7324\n",
      "Epoch 1118/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 64463.3498 - val_loss: 156239.4219\n",
      "Epoch 1119/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 151823.2077 - val_loss: 66140.2266\n",
      "Epoch 1120/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 45445.2208 - val_loss: 72720.4922\n",
      "Epoch 1121/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 117477.7530 - val_loss: 134969.0938\n",
      "Epoch 1122/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 78967.8699 - val_loss: 105040.3906\n",
      "Epoch 1123/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 129017.0489 - val_loss: 90551.0000\n",
      "Epoch 1124/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 58836.1601 - val_loss: 214192.9688\n",
      "Epoch 1125/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 209956.0449 - val_loss: 52520.4023\n",
      "Epoch 1126/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 61125.8687 - val_loss: 235880.2969\n",
      "Epoch 1127/10000\n",
      "62/62 [==============================] - 0s 221us/step - loss: 258323.4325 - val_loss: 146188.2031\n",
      "Epoch 1128/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 103598.2119 - val_loss: 29825.5840\n",
      "Epoch 1129/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42282.3340 - val_loss: 56260.3281\n",
      "Epoch 1130/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39661.7828 - val_loss: 69142.0156\n",
      "Epoch 1131/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 45713.2020 - val_loss: 32613.9160\n",
      "Epoch 1132/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 93785.7213 - val_loss: 241566.8906\n",
      "Epoch 1133/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 211823.0348 - val_loss: 39514.3086\n",
      "Epoch 1134/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 38736.3235 - val_loss: 49434.5508\n",
      "Epoch 1135/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33389.7278 - val_loss: 154188.8281\n",
      "Epoch 1136/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 265652.9657 - val_loss: 315039.0625\n",
      "Epoch 1137/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 227594.9345 - val_loss: 30522.1406\n",
      "Epoch 1138/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 92940.7944 - val_loss: 291249.6875\n",
      "Epoch 1139/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 271007.0060 - val_loss: 83785.2031\n",
      "Epoch 1140/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 57919.8971 - val_loss: 146234.7812\n",
      "Epoch 1141/10000\n",
      "62/62 [==============================] - 0s 208us/step - loss: 117344.2497 - val_loss: 27974.8027\n",
      "Epoch 1142/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 72184.9751 - val_loss: 279691.3125\n",
      "Epoch 1143/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 237983.1280 - val_loss: 52874.4180\n",
      "Epoch 1144/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 64146.1966 - val_loss: 278879.5938\n",
      "Epoch 1145/10000\n",
      "62/62 [==============================] - 0s 219us/step - loss: 267941.0020 - val_loss: 74180.9531\n",
      "Epoch 1146/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 59595.0801 - val_loss: 163747.4219\n",
      "Epoch 1147/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 134597.3153 - val_loss: 38201.7109\n",
      "Epoch 1148/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 57692.9047 - val_loss: 235292.3125\n",
      "Epoch 1149/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 198981.0978 - val_loss: 35472.4766\n",
      "Epoch 1150/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46785.7098 - val_loss: 127717.6094\n",
      "Epoch 1151/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 96791.2205 - val_loss: 29758.0625\n",
      "Epoch 1152/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 31789.3717 - val_loss: 40898.5312\n",
      "Epoch 1153/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 57218.9972 - val_loss: 78915.4766\n",
      "Epoch 1154/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 49887.3905 - val_loss: 62208.8398\n",
      "Epoch 1155/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 128258.3266 - val_loss: 276920.1250\n",
      "Epoch 1156/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 188676.9123 - val_loss: 27496.0645\n",
      "Epoch 1157/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52846.4274 - val_loss: 115878.2188\n",
      "Epoch 1158/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 158309.3201 - val_loss: 160200.0156\n",
      "Epoch 1159/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 145994.3070 - val_loss: 43039.0234\n",
      "Epoch 1160/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 51517.9206 - val_loss: 60663.0352\n",
      "Epoch 1161/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 54796.4122 - val_loss: 28729.2871\n",
      "Epoch 1162/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 29159.0808 - val_loss: 37066.2969\n",
      "Epoch 1163/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37397.8082 - val_loss: 39468.2266\n",
      "Epoch 1164/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35718.5338 - val_loss: 30144.2715\n",
      "Epoch 1165/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 29777.8042 - val_loss: 59383.6562\n",
      "Epoch 1166/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 102020.4879 - val_loss: 168379.9688\n",
      "Epoch 1167/10000\n",
      "62/62 [==============================] - 0s 236us/step - loss: 106887.2253 - val_loss: 47083.2148\n",
      "Epoch 1168/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 177069.0060 - val_loss: 614436.1875\n",
      "Epoch 1169/10000\n",
      "62/62 [==============================] - 0s 263us/step - loss: 522029.3498 - val_loss: 139771.6406\n",
      "Epoch 1170/10000\n",
      "62/62 [==============================] - 0s 242us/step - loss: 85756.6365 - val_loss: 183248.6875\n",
      "Epoch 1171/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 297777.9637 - val_loss: 391562.4062\n",
      "Epoch 1172/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 319159.9068 - val_loss: 46929.6289\n",
      "Epoch 1173/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 82163.6749 - val_loss: 533717.3750\n",
      "Epoch 1174/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 567984.2762 - val_loss: 336659.5312\n",
      "Epoch 1175/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 179280.9635 - val_loss: 160666.7812\n",
      "Epoch 1176/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 246307.6714 - val_loss: 275462.6250\n",
      "Epoch 1177/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 206601.9884 - val_loss: 27970.5176\n",
      "Epoch 1178/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 36810.5305 - val_loss: 74741.1719\n",
      "Epoch 1179/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 126700.1799 - val_loss: 269897.3750\n",
      "Epoch 1180/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 258777.6401 - val_loss: 142788.3594\n",
      "Epoch 1181/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 94927.7869 - val_loss: 30953.4629\n",
      "Epoch 1182/10000\n",
      "62/62 [==============================] - 0s 306us/step - loss: 58069.5456 - val_loss: 138641.5625\n",
      "Epoch 1183/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 135979.2639 - val_loss: 65445.7188\n",
      "Epoch 1184/10000\n",
      "62/62 [==============================] - 0s 257us/step - loss: 45202.2061 - val_loss: 147735.8438\n",
      "Epoch 1185/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 191179.8564 - val_loss: 221322.7188\n",
      "Epoch 1186/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 162562.8352 - val_loss: 32183.0625\n",
      "Epoch 1187/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 51470.2694 - val_loss: 214710.5469\n",
      "Epoch 1188/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 277699.1764 - val_loss: 265592.6250\n",
      "Epoch 1189/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 187309.4491 - val_loss: 28222.5859\n",
      "Epoch 1190/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 42064.1729 - val_loss: 82528.7031\n",
      "Epoch 1191/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 73276.8967 - val_loss: 46166.9297\n",
      "Epoch 1192/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 33649.6733 - val_loss: 75246.9844\n",
      "Epoch 1193/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 194108.5151 - val_loss: 518977.6875\n",
      "Epoch 1194/10000\n",
      "62/62 [==============================] - 0s 273us/step - loss: 545588.8488 - val_loss: 303307.7188\n",
      "Epoch 1195/10000\n",
      "62/62 [==============================] - 0s 290us/step - loss: 190489.3731 - val_loss: 58020.1641\n",
      "Epoch 1196/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 133191.0781 - val_loss: 359292.9688\n",
      "Epoch 1197/10000\n",
      "62/62 [==============================] - 0s 322us/step - loss: 364328.2641 - val_loss: 273346.1875\n",
      "Epoch 1198/10000\n",
      "62/62 [==============================] - 0s 251us/step - loss: 195144.7056 - val_loss: 34256.1406\n",
      "Epoch 1199/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 45987.3880 - val_loss: 178885.2188\n",
      "Epoch 1200/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 223470.0413 - val_loss: 215377.2188\n",
      "Epoch 1201/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 247371.2601 - val_loss: 218542.5625\n",
      "Epoch 1202/10000\n",
      "62/62 [==============================] - 0s 246us/step - loss: 193911.9173 - val_loss: 67103.7734\n",
      "Epoch 1203/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52733.8312 - val_loss: 32012.4824\n",
      "Epoch 1204/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 28172.4421 - val_loss: 27786.7383\n",
      "Epoch 1205/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 26353.2206 - val_loss: 28194.2598\n",
      "Epoch 1206/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 26460.1111 - val_loss: 27752.9043\n",
      "Epoch 1207/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 26623.2025 - val_loss: 33469.9883\n",
      "Epoch 1208/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35939.5711 - val_loss: 53142.0586\n",
      "Epoch 1209/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45689.1875 - val_loss: 34715.0586\n",
      "Epoch 1210/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 32336.7184 - val_loss: 106612.2656\n",
      "Epoch 1211/10000\n",
      "62/62 [==============================] - 0s 236us/step - loss: 165263.4889 - val_loss: 277035.4062\n",
      "Epoch 1212/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 318322.1663 - val_loss: 297148.7812\n",
      "Epoch 1213/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 268452.9037 - val_loss: 115923.3281\n",
      "Epoch 1214/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 73108.3971 - val_loss: 60755.9141\n",
      "Epoch 1215/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 56318.0416 - val_loss: 48508.2344\n",
      "Epoch 1216/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35613.1299 - val_loss: 39880.2695\n",
      "Epoch 1217/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 42331.0527 - val_loss: 39190.3789\n",
      "Epoch 1218/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50493.0759 - val_loss: 82183.4453\n",
      "Epoch 1219/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 106527.1643 - val_loss: 140710.8594\n",
      "Epoch 1220/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 127629.9289 - val_loss: 59068.4414\n",
      "Epoch 1221/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46906.1345 - val_loss: 30119.3047\n",
      "Epoch 1222/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 27450.1637 - val_loss: 27839.7246\n",
      "Epoch 1223/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 27543.1414 - val_loss: 42868.2383\n",
      "Epoch 1224/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44124.3868 - val_loss: 54772.4727\n",
      "Epoch 1225/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 36695.7709 - val_loss: 35940.0234\n",
      "Epoch 1226/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 241us/step - loss: 51361.0350 - val_loss: 87056.3438\n",
      "Epoch 1227/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 80166.4514 - val_loss: 45690.0078\n",
      "Epoch 1228/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 44681.4386 - val_loss: 34565.3516\n",
      "Epoch 1229/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 68588.0592 - val_loss: 246114.9531\n",
      "Epoch 1230/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 374575.4960 - val_loss: 686655.8125\n",
      "Epoch 1231/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 772979.3851 - val_loss: 762871.3750\n",
      "Epoch 1232/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 675197.1351 - val_loss: 305937.1250\n",
      "Epoch 1233/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 192323.8695 - val_loss: 38556.9570\n",
      "Epoch 1234/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 124488.6164 - val_loss: 521780.6250\n",
      "Epoch 1235/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 602240.8649 - val_loss: 721933.4375\n",
      "Epoch 1236/10000\n",
      "62/62 [==============================] - 0s 636us/step - loss: 709914.9778 - val_loss: 553810.5000\n",
      "Epoch 1237/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 364273.6028 - val_loss: 39758.4727\n",
      "Epoch 1238/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 53697.0600 - val_loss: 244917.5469\n",
      "Epoch 1239/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 353375.4133 - val_loss: 600806.8750\n",
      "Epoch 1240/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 582212.1734 - val_loss: 419265.7812\n",
      "Epoch 1241/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 404723.7026 - val_loss: 259361.5469\n",
      "Epoch 1242/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 165881.8894 - val_loss: 29006.2520\n",
      "Epoch 1243/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 63980.7240 - val_loss: 241055.3750\n",
      "Epoch 1244/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 273357.1058 - val_loss: 360147.5312\n",
      "Epoch 1245/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 472862.6915 - val_loss: 832130.4375\n",
      "Epoch 1246/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 979677.8548 - val_loss: 1298961.0000\n",
      "Epoch 1247/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1196327.8589 - val_loss: 851770.3750\n",
      "Epoch 1248/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 617361.3155 - val_loss: 129584.8906\n",
      "Epoch 1249/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 75656.8766 - val_loss: 108419.5859\n",
      "Epoch 1250/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 202668.7419 - val_loss: 497109.5312\n",
      "Epoch 1251/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 625313.4375 - val_loss: 909576.2500\n",
      "Epoch 1252/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1120397.1694 - val_loss: 1639195.8750\n",
      "Epoch 1253/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1987758.6935 - val_loss: 2614138.5000\n",
      "Epoch 1254/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 3050651.3871 - val_loss: 3897218.7500\n",
      "Epoch 1255/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 3865361.4435 - val_loss: 3201752.5000\n",
      "Epoch 1256/10000\n",
      "62/62 [==============================] - 0s 218us/step - loss: 2650211.4032 - val_loss: 989339.1875\n",
      "Epoch 1257/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 603481.2440 - val_loss: 43843.7422\n",
      "Epoch 1258/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 211746.2702 - val_loss: 1038964.3125\n",
      "Epoch 1259/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1244819.8669 - val_loss: 1801999.1250\n",
      "Epoch 1260/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1892464.0363 - val_loss: 2124007.2500\n",
      "Epoch 1261/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 2200903.7984 - val_loss: 2409446.7500\n",
      "Epoch 1262/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 2176107.1976 - val_loss: 1451942.1250\n",
      "Epoch 1263/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1017333.9819 - val_loss: 166717.3906\n",
      "Epoch 1264/10000\n",
      "62/62 [==============================] - 0s 221us/step - loss: 94277.4541 - val_loss: 271125.7812\n",
      "Epoch 1265/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 502829.4819 - val_loss: 1259353.2500\n",
      "Epoch 1266/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1481929.4476 - val_loss: 1998851.5000\n",
      "Epoch 1267/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 2115944.2903 - val_loss: 2381981.2500\n",
      "Epoch 1268/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 2573266.5726 - val_loss: 2998881.0000\n",
      "Epoch 1269/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 2981125.7903 - val_loss: 2920946.5000\n",
      "Epoch 1270/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 2861920.2097 - val_loss: 2415138.2500\n",
      "Epoch 1271/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 1943353.6411 - val_loss: 702834.3125\n",
      "Epoch 1272/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 468210.6709 - val_loss: 29901.8105\n",
      "Epoch 1273/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 45761.3788 - val_loss: 180340.3125\n",
      "Epoch 1274/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 154091.6406 - val_loss: 105540.8438\n",
      "Epoch 1275/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 72047.7044 - val_loss: 27330.5293\n",
      "Epoch 1276/10000\n",
      "62/62 [==============================] - 0s 229us/step - loss: 26186.0435 - val_loss: 28788.9414\n",
      "Epoch 1277/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 27521.5135 - val_loss: 27676.2246\n",
      "Epoch 1278/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 26558.5512 - val_loss: 54080.7188\n",
      "Epoch 1279/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35584.1324 - val_loss: 28061.2500\n",
      "Epoch 1280/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33275.1323 - val_loss: 54889.8008\n",
      "Epoch 1281/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 46167.5202 - val_loss: 29594.2969\n",
      "Epoch 1282/10000\n",
      "62/62 [==============================] - 0s 246us/step - loss: 30590.1770 - val_loss: 105506.1719\n",
      "Epoch 1283/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 166145.7379 - val_loss: 439868.0938\n",
      "Epoch 1284/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 533899.3649 - val_loss: 911358.6250\n",
      "Epoch 1285/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1028189.4476 - val_loss: 1452881.5000\n",
      "Epoch 1286/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1446851.8185 - val_loss: 1634716.6250\n",
      "Epoch 1287/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1588843.7540 - val_loss: 1633406.8750\n",
      "Epoch 1288/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1608559.3911 - val_loss: 1697961.1250\n",
      "Epoch 1289/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 1876006.2863 - val_loss: 2739158.0000\n",
      "Epoch 1290/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 3110859.5000 - val_loss: 4687415.0000\n",
      "Epoch 1291/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 5224658.3065 - val_loss: 7065094.0000\n",
      "Epoch 1292/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 7580463.2581 - val_loss: 9703676.0000\n",
      "Epoch 1293/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 10714925.3548 - val_loss: 14420206.0000\n",
      "Epoch 1294/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 16099895.6774 - val_loss: 21687274.0000\n",
      "Epoch 1295/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 23704699.0323 - val_loss: 29378908.0000\n",
      "Epoch 1296/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 30100698.6452 - val_loss: 33149582.0000\n",
      "Epoch 1297/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 32408145.6774 - val_loss: 28481600.0000\n",
      "Epoch 1298/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 25604751.0323 - val_loss: 16041286.0000\n",
      "Epoch 1299/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 12648533.7097 - val_loss: 4096941.5000\n",
      "Epoch 1300/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 2584254.2823 - val_loss: 25943.1211\n",
      "Epoch 1301/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 461507.0585 - val_loss: 2987646.2500\n",
      "Epoch 1302/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 4197671.9355 - val_loss: 6871231.0000\n",
      "Epoch 1303/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 6887603.2742 - val_loss: 5181978.0000\n",
      "Epoch 1304/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 3831655.6290 - val_loss: 626056.3750\n",
      "Epoch 1305/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 340445.6115 - val_loss: 556558.0625\n",
      "Epoch 1306/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1143126.7903 - val_loss: 3067175.7500\n",
      "Epoch 1307/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 3159126.7500 - val_loss: 2657099.5000\n",
      "Epoch 1308/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1945377.6411 - val_loss: 291940.9688\n",
      "Epoch 1309/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 173612.3088 - val_loss: 155437.9844\n",
      "Epoch 1310/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 302046.8518 - val_loss: 718496.5000\n",
      "Epoch 1311/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 726514.7218 - val_loss: 552452.8125\n",
      "Epoch 1312/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 424868.9315 - val_loss: 76706.9844\n",
      "Epoch 1313/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 58395.4039 - val_loss: 300083.5312\n",
      "Epoch 1314/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 502409.1855 - val_loss: 1067227.7500\n",
      "Epoch 1315/10000\n",
      "62/62 [==============================] - 0s 238us/step - loss: 1176716.4556 - val_loss: 1299035.3750\n",
      "Epoch 1316/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 1077035.7883 - val_loss: 430920.1562\n",
      "Epoch 1317/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 260265.0839 - val_loss: 37865.9844\n",
      "Epoch 1318/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 116731.9627 - val_loss: 436209.2500\n",
      "Epoch 1319/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 545029.4778 - val_loss: 745227.3750\n",
      "Epoch 1320/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 642168.1573 - val_loss: 258647.6719\n",
      "Epoch 1321/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 134241.7012 - val_loss: 178185.7344\n",
      "Epoch 1322/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 263756.2429 - val_loss: 499345.6562\n",
      "Epoch 1323/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 541472.3810 - val_loss: 588659.0625\n",
      "Epoch 1324/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 535364.7712 - val_loss: 368694.3750\n",
      "Epoch 1325/10000\n",
      "62/62 [==============================] - 0s 214us/step - loss: 310644.3125 - val_loss: 146128.4375\n",
      "Epoch 1326/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 146479.9698 - val_loss: 142412.1719\n",
      "Epoch 1327/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 141541.5776 - val_loss: 143510.0156\n",
      "Epoch 1328/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 98174.0822 - val_loss: 28148.1777\n",
      "Epoch 1329/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 53764.6436 - val_loss: 313528.9375\n",
      "Epoch 1330/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 547167.7621 - val_loss: 1286178.3750\n",
      "Epoch 1331/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1726194.2661 - val_loss: 2921669.7500\n",
      "Epoch 1332/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 2830525.5242 - val_loss: 2368951.0000\n",
      "Epoch 1333/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 2009923.4758 - val_loss: 938637.5625\n",
      "Epoch 1334/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 726103.7450 - val_loss: 178650.1094\n",
      "Epoch 1335/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 127680.9173 - val_loss: 26240.0625\n",
      "Epoch 1336/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 24984.0752 - val_loss: 25803.5391\n",
      "Epoch 1337/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 24264.4083 - val_loss: 25869.1875\n",
      "Epoch 1338/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 28492.0980 - val_loss: 94446.1484\n",
      "Epoch 1339/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 133258.7681 - val_loss: 289651.4062\n",
      "Epoch 1340/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 243740.3548 - val_loss: 143610.6562\n",
      "Epoch 1341/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 119040.7356 - val_loss: 67233.3281\n",
      "Epoch 1342/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 55332.1110 - val_loss: 35648.9961\n",
      "Epoch 1343/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 28244.4549 - val_loss: 31585.3789\n",
      "Epoch 1344/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 39737.0299 - val_loss: 75462.7578\n",
      "Epoch 1345/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 73362.9670 - val_loss: 67796.4531\n",
      "Epoch 1346/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 48101.7375 - val_loss: 26705.7070\n",
      "Epoch 1347/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 24780.1253 - val_loss: 26345.9570\n",
      "Epoch 1348/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 31379.8077 - val_loss: 114167.6719\n",
      "Epoch 1349/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 137348.0786 - val_loss: 197059.7188\n",
      "Epoch 1350/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 241485.0756 - val_loss: 408569.7188\n",
      "Epoch 1351/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 449266.4335 - val_loss: 564568.3750\n",
      "Epoch 1352/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 549376.5605 - val_loss: 527069.3750\n",
      "Epoch 1353/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 571362.0565 - val_loss: 716149.1250\n",
      "Epoch 1354/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 904860.4597 - val_loss: 1591478.7500\n",
      "Epoch 1355/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1990475.8548 - val_loss: 3528190.2500\n",
      "Epoch 1356/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 3954003.1452 - val_loss: 5453086.5000\n",
      "Epoch 1357/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 6083718.2742 - val_loss: 8390391.0000\n",
      "Epoch 1358/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 9704808.3548 - val_loss: 14350809.0000\n",
      "Epoch 1359/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 16730240.8387 - val_loss: 25359758.0000\n",
      "Epoch 1360/10000\n",
      "62/62 [==============================] - 0s 234us/step - loss: 29342187.2258 - val_loss: 40926060.0000\n",
      "Epoch 1361/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44252316.3871 - val_loss: 55250308.0000\n",
      "Epoch 1362/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 58138803.0968 - val_loss: 61754776.0000\n",
      "Epoch 1363/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 57508995.3548 - val_loss: 42200568.0000\n",
      "Epoch 1364/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35242695.2903 - val_loss: 13106521.0000\n",
      "Epoch 1365/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 8512461.1935 - val_loss: 27043.4707\n",
      "Epoch 1366/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 225us/step - loss: 1244006.7016 - val_loss: 8220195.5000\n",
      "Epoch 1367/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 10624056.4194 - val_loss: 13465843.0000\n",
      "Epoch 1368/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 11416430.5484 - val_loss: 4037214.0000\n",
      "Epoch 1369/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 2280977.3700 - val_loss: 471682.0625\n",
      "Epoch 1370/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1753804.3952 - val_loss: 5566044.5000\n",
      "Epoch 1371/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 5745293.7258 - val_loss: 4169833.0000\n",
      "Epoch 1372/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 3099119.2016 - val_loss: 286228.4062\n",
      "Epoch 1373/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 249413.8256 - val_loss: 1737823.0000\n",
      "Epoch 1374/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 2286433.4677 - val_loss: 2796232.0000\n",
      "Epoch 1375/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 2225267.0484 - val_loss: 393282.0000\n",
      "Epoch 1376/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 221706.6502 - val_loss: 399518.9375\n",
      "Epoch 1377/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 630628.5181 - val_loss: 1030135.6250\n",
      "Epoch 1378/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 13233702.3548 - val_loss: 246765680.0000\n",
      "Epoch 1379/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 425391715.0968 - val_loss: 688380288.0000\n",
      "Epoch 1380/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 517687933.9355 - val_loss: 8659508.0000\n",
      "Epoch 1381/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 80026563.6129 - val_loss: 372703008.0000\n",
      "Epoch 1382/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 269339964.9032 - val_loss: 2540178.0000\n",
      "Epoch 1383/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 84076143.6129 - val_loss: 158037840.0000\n",
      "Epoch 1384/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 80326326.6673 - val_loss: 106452728.0000\n",
      "Epoch 1385/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 105083044.9032 - val_loss: 53254.8945\n",
      "Epoch 1386/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 38008516.8065 - val_loss: 55989840.0000\n",
      "Epoch 1387/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 30490501.8468 - val_loss: 65173564.0000\n",
      "Epoch 1388/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42192156.2581 - val_loss: 16614054.0000\n",
      "Epoch 1389/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 30728688.5161 - val_loss: 1153367.5000\n",
      "Epoch 1390/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 14409601.1613 - val_loss: 19575422.0000\n",
      "Epoch 1391/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 11941358.8952 - val_loss: 26321100.0000\n",
      "Epoch 1392/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 14755710.9113 - val_loss: 13732520.0000\n",
      "Epoch 1393/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 12949058.0645 - val_loss: 1647067.1250\n",
      "Epoch 1394/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 8475194.3548 - val_loss: 1312962.1250\n",
      "Epoch 1395/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 4736553.2581 - val_loss: 6819020.5000\n",
      "Epoch 1396/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 4219387.4556 - val_loss: 9317949.0000\n",
      "Epoch 1397/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 4854629.0121 - val_loss: 6139911.5000\n",
      "Epoch 1398/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 4250820.0161 - val_loss: 2159854.5000\n",
      "Epoch 1399/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 3296893.4032 - val_loss: 93468.3203\n",
      "Epoch 1400/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 2205773.2097 - val_loss: 866053.8125\n",
      "Epoch 1401/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1569143.4194 - val_loss: 2939316.7500\n",
      "Epoch 1402/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1614351.0817 - val_loss: 3379714.0000\n",
      "Epoch 1403/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 1905084.4667 - val_loss: 2013407.6250\n",
      "Epoch 1404/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1698672.6290 - val_loss: 451527.7500\n",
      "Epoch 1405/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 1376253.0242 - val_loss: 106302.7500\n",
      "Epoch 1406/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 806528.2056 - val_loss: 841754.2500\n",
      "Epoch 1407/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 657480.2258 - val_loss: 1430247.3750\n",
      "Epoch 1408/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 745554.9296 - val_loss: 1229067.1250\n",
      "Epoch 1409/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 703705.2717 - val_loss: 690823.5000\n",
      "Epoch 1410/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 629214.8387 - val_loss: 210716.6406\n",
      "Epoch 1411/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 548550.5121 - val_loss: 70251.0781\n",
      "Epoch 1412/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 366336.5766 - val_loss: 373398.0625\n",
      "Epoch 1413/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 352858.1966 - val_loss: 970672.8750\n",
      "Epoch 1414/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 512098.0493 - val_loss: 696884.7500\n",
      "Epoch 1415/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 467027.9304 - val_loss: 304195.4062\n",
      "Epoch 1416/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 311878.5897 - val_loss: 82125.5625\n",
      "Epoch 1417/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 395322.0907 - val_loss: 168535.4688\n",
      "Epoch 1418/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 211224.5161 - val_loss: 369470.1562\n",
      "Epoch 1419/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 204487.7665 - val_loss: 317673.0312\n",
      "Epoch 1420/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 211330.4695 - val_loss: 152010.8438\n",
      "Epoch 1421/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 158484.5076 - val_loss: 47323.9414\n",
      "Epoch 1422/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 97280.1154 - val_loss: 60028.8984\n",
      "Epoch 1423/10000\n",
      "62/62 [==============================] - 0s 595us/step - loss: 68353.7235 - val_loss: 107573.8281\n",
      "Epoch 1424/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 68830.7119 - val_loss: 175841.8906\n",
      "Epoch 1425/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 112764.9722 - val_loss: 100319.9609\n",
      "Epoch 1426/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 74814.6784 - val_loss: 52655.1172\n",
      "Epoch 1427/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 26938.5146 - val_loss: 77523.2422\n",
      "Epoch 1428/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 46972.9834 - val_loss: 59668.5977\n",
      "Epoch 1429/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 87562.3135 - val_loss: 51622.7617\n",
      "Epoch 1430/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 61118.7790 - val_loss: 114680.7969\n",
      "Epoch 1431/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 64224.8851 - val_loss: 123032.2188\n",
      "Epoch 1432/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 80817.8211 - val_loss: 78149.7422\n",
      "Epoch 1433/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52139.1085 - val_loss: 52541.2227\n",
      "Epoch 1434/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 81481.4524 - val_loss: 56559.3203\n",
      "Epoch 1435/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52597.8475 - val_loss: 99861.9844\n",
      "Epoch 1436/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 225us/step - loss: 58963.2033 - val_loss: 186972.9375\n",
      "Epoch 1437/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 123691.0607 - val_loss: 97781.0859\n",
      "Epoch 1438/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 71617.0171 - val_loss: 52339.0273\n",
      "Epoch 1439/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 93576.9335 - val_loss: 60024.0312\n",
      "Epoch 1440/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44307.3575 - val_loss: 43824.3906\n",
      "Epoch 1441/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 29064.7174 - val_loss: 42131.2969\n",
      "Epoch 1442/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 33576.4012 - val_loss: 53519.6992\n",
      "Epoch 1443/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 35301.8359 - val_loss: 40882.1562\n",
      "Epoch 1444/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 29100.3238 - val_loss: 40805.9766\n",
      "Epoch 1445/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 28308.4970 - val_loss: 42165.7109\n",
      "Epoch 1446/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 30891.3918 - val_loss: 75007.8984\n",
      "Epoch 1447/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 40010.9343 - val_loss: 72958.2266\n",
      "Epoch 1448/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 55314.7912 - val_loss: 47392.3672\n",
      "Epoch 1449/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33934.6458 - val_loss: 40526.2969\n",
      "Epoch 1450/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35395.9059 - val_loss: 41099.6406\n",
      "Epoch 1451/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 29772.7567 - val_loss: 45963.1836\n",
      "Epoch 1452/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 35885.6316 - val_loss: 46755.6445\n",
      "Epoch 1453/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 31402.7518 - val_loss: 39371.3672\n",
      "Epoch 1454/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 27422.8424 - val_loss: 40686.0469\n",
      "Epoch 1455/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 28436.3975 - val_loss: 39064.2930\n",
      "Epoch 1456/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 30339.1101 - val_loss: 57396.6367\n",
      "Epoch 1457/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 37933.1888 - val_loss: 39638.8359\n",
      "Epoch 1458/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 25361.8479 - val_loss: 53297.4805\n",
      "Epoch 1459/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 32996.8942 - val_loss: 42404.0898\n",
      "Epoch 1460/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 37775.5631 - val_loss: 39778.3984\n",
      "Epoch 1461/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 30598.0093 - val_loss: 52390.6680\n",
      "Epoch 1462/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 35649.4945 - val_loss: 38221.5664\n",
      "Epoch 1463/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 25158.6785 - val_loss: 41589.7695\n",
      "Epoch 1464/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33924.0159 - val_loss: 45274.4883\n",
      "Epoch 1465/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 37718.2559 - val_loss: 50151.6055\n",
      "Epoch 1466/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 32630.4161 - val_loss: 38520.9297\n",
      "Epoch 1467/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 27024.3653 - val_loss: 37829.6445\n",
      "Epoch 1468/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 27849.8592 - val_loss: 42512.8633\n",
      "Epoch 1469/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 28297.0093 - val_loss: 37491.9414\n",
      "Epoch 1470/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 25099.4793 - val_loss: 37327.9609\n",
      "Epoch 1471/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 26317.6811 - val_loss: 43355.6680\n",
      "Epoch 1472/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 28590.5038 - val_loss: 37151.4141\n",
      "Epoch 1473/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 26687.9522 - val_loss: 56794.3594\n",
      "Epoch 1474/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 31255.2267 - val_loss: 58608.3086\n",
      "Epoch 1475/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 39767.3758 - val_loss: 48809.7031\n",
      "Epoch 1476/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 50136.8741 - val_loss: 44556.0703\n",
      "Epoch 1477/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 29715.9223 - val_loss: 37075.0039\n",
      "Epoch 1478/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 29024.5366 - val_loss: 60056.2695\n",
      "Epoch 1479/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 33469.7366 - val_loss: 93519.3281\n",
      "Epoch 1480/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52277.4542 - val_loss: 94150.9297\n",
      "Epoch 1481/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 63663.0181 - val_loss: 45979.6953\n",
      "Epoch 1482/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 75702.3765 - val_loss: 52733.0430\n",
      "Epoch 1483/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 61001.2021 - val_loss: 169135.3906\n",
      "Epoch 1484/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 100752.0223 - val_loss: 75343.3125\n",
      "Epoch 1485/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 90126.7167 - val_loss: 37279.9648\n",
      "Epoch 1486/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 43246.7539 - val_loss: 66862.2109\n",
      "Epoch 1487/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 40617.0191 - val_loss: 49606.0898\n",
      "Epoch 1488/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 30003.0301 - val_loss: 63434.8164\n",
      "Epoch 1489/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 31854.0296 - val_loss: 65036.6445\n",
      "Epoch 1490/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 53009.1923 - val_loss: 40380.9648\n",
      "Epoch 1491/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50698.8425 - val_loss: 53503.6406\n",
      "Epoch 1492/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 36587.8386 - val_loss: 35353.3711\n",
      "Epoch 1493/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 25526.6992 - val_loss: 37191.5117\n",
      "Epoch 1494/10000\n",
      "62/62 [==============================] - 0s 226us/step - loss: 25500.8656 - val_loss: 39660.9219\n",
      "Epoch 1495/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 25215.0658 - val_loss: 52441.2188\n",
      "Epoch 1496/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 50471.3202 - val_loss: 35612.0547\n",
      "Epoch 1497/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 25889.7116 - val_loss: 36691.8594\n",
      "Epoch 1498/10000\n",
      "62/62 [==============================] - 0s 226us/step - loss: 25145.1299 - val_loss: 36257.7812\n",
      "Epoch 1499/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 24947.1896 - val_loss: 41923.7812\n",
      "Epoch 1500/10000\n",
      "62/62 [==============================] - 0s 241us/step - loss: 29805.4589 - val_loss: 38156.2188\n",
      "Epoch 1501/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 25657.7310 - val_loss: 34765.3555\n",
      "Epoch 1502/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 23381.6724 - val_loss: 41007.4258\n",
      "Epoch 1503/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 44362.3352 - val_loss: 57713.4961\n",
      "Epoch 1504/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 34295.2547 - val_loss: 56508.5898\n",
      "Epoch 1505/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 61703.9418 - val_loss: 37734.3945\n",
      "Epoch 1506/10000\n",
      "62/62 [==============================] - 0s 233us/step - loss: 28898.2440 - val_loss: 45037.0078\n",
      "Epoch 1507/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 27949.5800 - val_loss: 56649.3867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1508/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42569.7456 - val_loss: 39343.3750\n",
      "Epoch 1509/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 33014.1176 - val_loss: 36587.0508\n",
      "Epoch 1510/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 26284.6866 - val_loss: 37034.0000\n",
      "Epoch 1511/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 44807.4783 - val_loss: 169354.7969\n",
      "Epoch 1512/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 102853.2593 - val_loss: 63990.8555\n",
      "Epoch 1513/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 85893.3846 - val_loss: 38359.1289\n",
      "Epoch 1514/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 75137.6726 - val_loss: 204569.7812\n",
      "Epoch 1515/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 106604.0314 - val_loss: 135217.0312\n",
      "Epoch 1516/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 125299.7256 - val_loss: 36090.8789\n",
      "Epoch 1517/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 61249.6348 - val_loss: 65956.7109\n",
      "Epoch 1518/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 42973.1792 - val_loss: 65735.9141\n",
      "Epoch 1519/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 48779.7823 - val_loss: 40506.9688\n",
      "Epoch 1520/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 24822.3048 - val_loss: 37982.8398\n",
      "Epoch 1521/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 42225.5161 - val_loss: 39806.5977\n",
      "Epoch 1522/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 30722.3799 - val_loss: 62105.5703\n",
      "Epoch 1523/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 32591.7071 - val_loss: 105313.2578\n",
      "Epoch 1524/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 101706.3347 - val_loss: 33365.4219\n",
      "Epoch 1525/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 48374.4849 - val_loss: 72324.5156\n",
      "Epoch 1526/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 46857.3013 - val_loss: 147911.1719\n",
      "Epoch 1527/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 138607.6129 - val_loss: 34107.6016\n",
      "Epoch 1528/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 60729.2807 - val_loss: 87920.1484\n",
      "Epoch 1529/10000\n",
      "62/62 [==============================] - 0s 225us/step - loss: 52340.6440 - val_loss: 55099.4648\n",
      "Epoch 1530/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 33482.7993 - val_loss: 60039.7109\n",
      "Epoch 1531/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 100894.7939 - val_loss: 101218.2188\n",
      "Epoch 1532/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 57195.5730 - val_loss: 103262.1875\n",
      "Epoch 1533/10000\n",
      "62/62 [==============================] - 0s 211us/step - loss: 55638.9585 - val_loss: 106405.7891\n",
      "Epoch 1534/10000\n",
      "62/62 [==============================] - 0s 217us/step - loss: 108933.7082 - val_loss: 35526.0820\n",
      "Epoch 1535/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 64400.4904 - val_loss: 120917.4688\n",
      "Epoch 1536/10000\n",
      "62/62 [==============================] - 0s 209us/step - loss: 82481.2870 - val_loss: 63155.0078\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24757.423114678033"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=31\n",
    "numpy.random.seed(29)    \n",
    "python_random.seed(29)\n",
    "tf.random.set_random_seed(29)\n",
    "\n",
    "\n",
    "#dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_200.csv\", header=None)\n",
    "dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_\"+str(x)+\".csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "X = dataset[:,0:x*11]\n",
    "y = dataset[:,x*11]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=29)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(x*11, input_dim=x*11, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(x*11, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(x*11, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dense(x*8, kernel_initializer='normal', activation='sigmoid'))\n",
    "#model.add(Dense(x*4, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dense(x*2, kernel_initializer='normal', activation='sigmoid'))\n",
    "#model.add(Dense(x*1, kernel_initializer='normal', activation='relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dense(16, kernel_initializer='normal', activation='sigmoid'))\n",
    "#model.add(Dense(8, kernel_initializer='normal', activation='sigmoid'))\n",
    "#model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "#model.add(Dense(2, kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=200)\n",
    "mc = ModelCheckpoint('best.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)    \n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10000, verbose=1, callbacks=[es,mc])\n",
    "\n",
    "saved_model = load_model('best.h5')\n",
    "predictions = saved_model.predict(X)\n",
    "plt.plot(y,label='Actual')\n",
    "plt.plot(predictions,label='predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "metrics.mean_squared_error(y,saved_model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7642.812029032259"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=31\n",
    "numpy.random.seed(29)    \n",
    "python_random.seed(29)\n",
    "tf.random.set_random_seed(29)\n",
    "\n",
    "\n",
    "#dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_200.csv\", header=None)\n",
    "dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_\"+str(x)+\".csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "X = dataset[:,0:x*11]\n",
    "y = dataset[:,x*11]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=29)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "history = model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "plt.plot(y,label='Actual')\n",
    "plt.plot(predictions,label='predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "metrics.mean_squared_error(y,model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_model(X, y):\n",
    "    # Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'n_estimators': (10, 50, 100, 1000),\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0,                         n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],                               random_state=False, verbose=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=22\n",
    "numpy.random.seed(29)    \n",
    "python_random.seed(29)\n",
    "tf.random.set_random_seed(29)\n",
    "\n",
    "\n",
    "#dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_200.csv\", header=None)\n",
    "dataframe = pandas.read_csv(\"C:\\\\Work\\\\Orca\\\\MonthsHistory_\"+str(x)+\".csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "X = dataset[:,0:x*23]\n",
    "y = dataset[:,x*23]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=29)\n",
    "\n",
    "gsc = GridSearchCV(estimator=RandomForestRegressor(),param_grid={'max_depth': range(7,21),'n_estimators': (431,432,433),},cv=5,scoring='neg_mean_squared_error',verbose=0,n_jobs=-1)\n",
    "\n",
    "    \n",
    "grid_result = gsc.fit(X, y)\n",
    "best_params = grid_result.best_params_\n",
    "    \n",
    "rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],random_state=False, verbose=False)\n",
    "\n",
    "history = rfr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#rfr_model(X_train,y_train)\n",
    "\n",
    "#scores = cross_val_score(rfr, X_train, y_train, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6076.7620347772045"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = rfr.predict(X)\n",
    "plt.plot(y,label='Actual')\n",
    "plt.plot(predictions,label='predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "metrics.mean_squared_error(y,rfr.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12, 'n_estimators': 432}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24943.02642187865"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = load_model('best.h5')\n",
    "predictions = saved_model.predict(X)\n",
    "plt.plot(y,label='Actual')\n",
    "plt.plot(predictions,label='predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "metrics.mean_squared_error(y,saved_model.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10789.017262090289"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(y,saved_model.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 947.44,  940.44,  833.4 ,  701.32,  577.86,  677.99,  757.28,\n",
       "        788.27,  837.73,  933.18,  923.84,  889.67,  879.52,  893.44,\n",
       "        888.99,  855.18,  634.61,  706.55,  735.16,  792.11,  885.37,\n",
       "        866.82, 1031.65,  839.33, 1071.38,  894.2 ,  887.91,  927.07,\n",
       "        740.39,  716.61,  772.22,  786.63,  749.8 , 1009.57, 1095.53,\n",
       "        984.2 ,  998.25,  859.66,  865.18,  676.28,  693.24,  702.67,\n",
       "        845.52,  851.27,  958.05, 1005.23, 1042.04, 1046.05, 1014.42,\n",
       "        918.66,  937.13,  736.37,  679.7 ,  659.43,  805.34,  864.12,\n",
       "       1013.52,  953.94,  935.67,  949.11,  960.46,  919.88,  893.56,\n",
       "        848.96,  844.86,  856.51,  869.66,  963.39, 1066.97, 1020.56,\n",
       "       1138.15, 1038.95, 1059.69,  937.99,  904.16,  835.01,  775.34,\n",
       "        770.62,  807.95,  852.99,  935.04, 1004.79, 1033.64,  997.48,\n",
       "        982.38,  927.33,  807.  ,  796.93,  662.48,  814.95,  806.9 ,\n",
       "        850.42,  902.06,  983.89, 1008.07,  880.03, 1213.86,  877.5 ,\n",
       "        828.29,  797.22,  706.65,  711.64,  775.22,  809.81,  882.2 ,\n",
       "       1075.09, 1047.51,  980.05, 1035.33,  891.74,  894.59,  830.96,\n",
       "        764.89,  783.53,  764.98])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115,)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
